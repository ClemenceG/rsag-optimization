{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../rsag_convex.png\" alt=\"algoconvex\" />\n",
    "<img src=\"../x_update.png\" alt=\"x_update\" />\n",
    "<img src=\"../mean.png\" alt=\"mean\" />\n",
    "<img src=\"../rsag_composite.png\" alt=\"algo\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Parameters :__\n",
    "- $\\alpha$: (1-$\\alpha$) weight of aggregated x on current state, i.e. momentum\n",
    "- $\\lambda$: learning rate\n",
    "- $\\beta$: change for aggregated x\n",
    "- $p_k$ termination probability\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  torch.optim import Adam, SGD, RMSprop\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import warnings\n",
    "import copy\n",
    "import torch.utils.data as data_utils\n",
    "# import EarlyStopping\n",
    "# from pytorchtools import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch version: 2.1.2+cu121\n",
      "Using GPU, device name: NVIDIA GeForce GTX 1660 Ti\n"
     ]
    }
   ],
   "source": [
    "print('Using PyTorch version:', torch.__version__)\n",
    "if torch.cuda.is_available():\n",
    "    print('Using GPU, device name:', torch.cuda.get_device_name(0))\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    print('No GPU found, using CPU instead.') \n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'models'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[0;32m      3\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MLP\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01moptimizers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RSAG, AccSGD\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoader\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'models'"
     ]
    }
   ],
   "source": [
    "import path\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from models import MLP\n",
    "from optimizers import RSAG, AccSGD\n",
    "from util import DataLoader\n",
    "from util import calc_accuracy, train_model, HPScheduler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run MLP:\n",
    "__TUNE DIFFERENT OPTIMIZERS__:\n",
    "- Nesterov w/ weight decay w/ Scheduled LR (SGD)\n",
    "- Momentum w/ weight decay w/ Scheduled LR (SGD)\n",
    "- Basic SGD\n",
    "- Adagrad?\n",
    "- Adam?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataLoader()\n",
    "loaders = data_loader.get_loaders()\n",
    "# loss_function = torch.nn.CrossEntropyLoss()\n",
    "# model = MLP().to(device)\n",
    "# print(model)\n",
    "\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=1e-4, nesterov=True, momentum=0.9)\n",
    "# optimizer = RSAG(model.parameters(), lr=1e-4, alpha=.9, beta=9e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module): \n",
    "    \"\"\"\n",
    "    Very simple\n",
    "    2 hidden layer MLP with 512 and 512 hidden units respectively\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=28*28, output_dim=10, h=512):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(input_dim, h),\n",
    "            nn.Linear(h, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = x.view(-1,28*28)\n",
    "        # x = F.relu(self.layers[0](x))\n",
    "        return self.layers(x)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RSAG Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RSAG(torch.optim.Optimizer):\n",
    "    r\"\"\"\n",
    "    Args:\n",
    "        params (iterable): iterable of parameters to optimize or dicts defining\n",
    "            parameter groups\n",
    "        lr (float): learning rate (lambda) (required)\n",
    "        kappa (float): lambda  (default: 1000)\n",
    "        xi (float, optional): statistical advantage parameter (default: 10)\n",
    "        smallConst (float, optional): any value <=1 (default: 0.7)\n",
    "    Example:\n",
    "        >>> from RSAG import *\n",
    "        >>> optimizer = RSAG(model.parameters(), lr=0.1, kappa = 1000.0, xi = 10.0)\n",
    "        >>> optimizer.zero_grad()\n",
    "        >>> loss_fn(model(input), target).backward()\n",
    "        >>> optimizer.step()\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 params, \n",
    "                 lr=0.01, \n",
    "                 alpha = 0.1, \n",
    "                 beta = 0.1): #, smallConst = 0.7, weight_decay=0):\n",
    "        #defaults = dict(lr=lr, kappa=kappa, xi, smallConst=smallConst,\n",
    "                        # weight_decay=weight_decay)\n",
    "        \n",
    "        if lr < 0.0:\n",
    "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
    "        if alpha < 0.0 or alpha > 1.0:\n",
    "            raise ValueError(\"Invalid alpha: {}\".format(alpha))\n",
    "        if beta < 0.0:\n",
    "            raise ValueError(\"Invalid beta: {}\".format(beta))\n",
    "        \n",
    "        defaults = dict(lr=[lr], alpha=[alpha], beta=[beta], step=0)\n",
    "\n",
    "        \n",
    "        # ASSUME PARAMS ARE ALREADY PROJECTED\n",
    "        super(RSAG, self).__init__(params, defaults)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super(RSAG, self).__setstate__(state)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        \"\"\" Performs a single optimization step.\n",
    "        Arguments:\n",
    "            closure (callable, optional): A closure that reevaluates the model\n",
    "                and returns the loss.\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "            print('hereee')\n",
    "        # print('param_group',self.param_groups)\n",
    "        for group in self.param_groups:\n",
    "            # weight_decay = group['weight_decay']\n",
    "            lr = group['lr']\n",
    "            alpha, beta = group['alpha'], group['beta']\n",
    "            alpha_bar = 1.0-alpha\n",
    "            momentum_buffer_list = []\n",
    "            # step = group['step']\n",
    "            # print('group', group)\n",
    "\n",
    "            # INITIALIZE GROUPS\n",
    "            # params_with_grad, d_p_list, momentum_buffer_list = [], [], []\n",
    "            # for p in group['params']:\n",
    "            #     if p.grad is not None:\n",
    "            #         params_with_grad.append(p)\n",
    "            #         d_p_list.append(p.grad)\n",
    "            #         # if p.grad.is_sparse:\n",
    "            #         #     has_sparse_grad = True\n",
    "\n",
    "            #         state = self.state[p]\n",
    "            #         if 'momentum_aggr' not in state:\n",
    "            #             momentum_buffer_list.append(None)\n",
    "            #         else:\n",
    "            #             momentum_buffer_list.append(state['momentum_buffer'])\n",
    "            \n",
    "            # UPDATE GROUPS\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "\n",
    "                d_w = p.grad.data\n",
    "                param_state = self.state[p]\n",
    "                w = p.data\n",
    "\n",
    "                \n",
    "                momentum_aggr = torch.tensor(w - beta[-1]*d_w)\n",
    "                \n",
    "                if group['step'] == 0:\n",
    "                    alpha.append(alpha[-1])\n",
    "                    beta.append(beta[-1])\n",
    "                    lr.append(lr[-1])\n",
    "\n",
    "                    if param_state['momentum_aggr'] is None:\n",
    "                        param_state['momentum_aggr'] = {}\n",
    "\n",
    "                    param_state['momentum_aggr'][p] = momentum_aggr\n",
    "\n",
    "                    alpha_val = (1-alpha[-1])*beta[-2]+alpha[-1]*lr[-1]\n",
    "                    p.data.add_(d_w, alpha=-alpha_val)\n",
    "                    \n",
    "                else:\n",
    "                    alpha.append(alpha[-1])\n",
    "                    beta.append(beta[-1])\n",
    "                    lr.append(lr[-1])\n",
    "\n",
    "                    mu = 1-alpha[-1] + alpha[-1]/alpha[-2]\n",
    "                    xi = (1-alpha[-1])*beta[-2] + alpha[-1]*lr[-1]\n",
    "                    eta = (1-alpha[-2])*alpha[-1]/alpha[-2]\n",
    "\n",
    "                    p.data.mul_(mu)\n",
    "                    p.data.add_(w, alpha=xi)\n",
    "                    p.data.add_(param_state['momentum_aggr'][p], alpha=-eta)\n",
    "                    param_state['momentum_aggr'][p] = momentum_aggr\n",
    "\n",
    "                    # param_state['momentum_aggr'] = momentum_aggr\n",
    "\n",
    "                # if weight_decay != 0:\n",
    "                #     grad_d.add_(weight_decay, p.data)\n",
    "                \n",
    "                # if 'momentum_aggr' not in param_state:\n",
    "                #     param_state['momentum_aggr'] = copy.deepcopy(p.data)\n",
    "                #     param_state['prev_momentum_aggr'] = copy.deepcopy(p.data)\n",
    "                # buf = param_state['momentum_aggr']\n",
    "                # aggr_grad = (buf-param_state['prev_momentum_aggr'])\n",
    "                # aggr_grad.mul_(alpha_bar)\n",
    "                # aggr_grad.add_(d_w, alpha=alpha)\n",
    "                \n",
    "                # param_state['prev_momentum_aggr'] = copy.deepcopy(buf)\n",
    "                \n",
    "                # # Update momentum buffer:'\n",
    "                # buf.mul_(alpha_bar)\n",
    "                # buf.add_(p.data, alpha=alpha)\n",
    "                # buf.add_(aggr_grad, alpha=-beta)\n",
    "                \n",
    "                # p.data.add_(aggr_grad, alpha=-lr)\n",
    "                \n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "                model,\n",
    "                loss_function,\n",
    "                optimizer,\n",
    "                loaders,\n",
    "                device='cpu',\n",
    "                verbose=True,\n",
    "                save_path=None,\n",
    "                log_path=None,\n",
    "                n_epochs=200,\n",
    "                print_every=1\n",
    "                ):\n",
    "    log = {}\n",
    "    log['loss'], log['accuracy'] = [], []\n",
    "    log['v_loss'], log['v_accuracy'] = [], []\n",
    "    \n",
    "    log['v_loss_std'], log['v_accuracy_std'] = [], []\n",
    "    log['loss_std'], log['accuracy_std'] = [], []\n",
    "\n",
    "    best_acc = 0.0\n",
    "    \n",
    "\n",
    "    # initialize the early_stopping object\n",
    "    # early_stopping = EarlyStopping(patience=20, verbose=True)\n",
    "\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    for epoch in range(0,n_epochs):\n",
    "        print(f'Starting Epoch {epoch+1}')\n",
    "\n",
    "        current_loss, total_acc = [], []\n",
    "        v_loss, v_acc = [], []\n",
    "\n",
    "        for data, targets in loaders['train']:\n",
    "            # inputs, targets = data\n",
    "            # inputs, targets = inputs.float(), targets.float()\n",
    "            # targets = targets.reshape((targets.shape[0], 1))\n",
    "            \n",
    "            # Copy data and targets to GPU\n",
    "            data = data.to(device)\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(data)\n",
    "\n",
    "            # Calculate the loss\n",
    "            loss = loss_function(outputs, targets)\n",
    "            # current_loss += loss\n",
    "\n",
    "            # Backpropagation\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            current_loss.append(loss.item())\n",
    "            total_acc.append(calc_accuracy(outputs, targets))\n",
    "            \n",
    "        # Validation\n",
    "        model.eval()\n",
    "\n",
    "        for data, targets in loaders['valid']:\n",
    "            data = data.to(device)\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            outputs = model(data)\n",
    "\n",
    "            loss = loss_function(outputs, targets)\n",
    "            v_loss.append(loss.item())\n",
    "            v_acc.append(calc_accuracy(outputs, targets))        \n",
    "\n",
    "        print(f'Epoch {epoch+1} finished')\n",
    "        # current_loss /= len(loaders['train'])\n",
    "        # total_acc /= len(loaders['train'])\n",
    "        # print('loss {:.4f}'.format(current_loss))\n",
    "        # print('Accuracy:  {:.4f}'.format(total_acc))\n",
    "\n",
    "        update_log(log, current_loss, total_acc, v_loss, v_acc, len(loaders['train']), len(loaders['valid']))\n",
    "\n",
    "        if verbose:\n",
    "            if epoch%print_every == 0:\n",
    "                print('Epoch {}/{}'.format(epoch+1, n_epochs))\n",
    "                print('-' * 10)\n",
    "                print('Loss {:.4f}'.format(log['loss'][-1]))\n",
    "                print('Accuracy:  {:.4f}'.format(log['accuracy'][-1]))\n",
    "                print('Validation Loss {:.4f}'.format(log['v_loss'][-1]))\n",
    "                print('Validation Accuracy:  {:.4f}'.format(log['v_accuracy'][-1]))\n",
    "        \n",
    "        if len(log['v_accuracy']) > 1 and (np.abs(log['v_accuracy'][-1]-log['v_accuracy'][-2])<0.1):\n",
    "            print('Early stopping at epoch %d'%epoch)\n",
    "            break\n",
    "        # early_stopping(log['v_loss'][-1], model)\n",
    "        \n",
    "        # if early_stopping.early_stop:\n",
    "        #     print(\"Early stopping\")\n",
    "        #     break\n",
    "\n",
    "        if log['v_accuracy'][-1] > best_acc:\n",
    "            best_acc = log['v_accuracy'][-1]\n",
    "    \n",
    "            # load the last checkpoint with the best model\n",
    "            model.load_state_dict(torch.load('checkpoint.pt'))\n",
    "    \n",
    "    if save_path is not None:\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "        print('Model saved to %s'%save_path)\n",
    "\n",
    "    if log_path is not None:\n",
    "        df = pd.DataFrame.from_dict(log)\n",
    "        df.to_csv(log_path)\n",
    "        print('Log saved to %s'%log_path)\n",
    "\n",
    "    print(\"Training has completed\")\n",
    "    return log, best_acc\n",
    "\n",
    "def update_log(log, current_loss, total_acc, v_loss, v_acc, train_len, valid_len):\n",
    "        log['loss_std'].append(np.std(current_loss))\n",
    "        log['accuracy_std'].append(np.std(total_acc))\n",
    "        current_loss = sum(current_loss)/train_len\n",
    "        total_acc = sum(total_acc)/train_len\n",
    "        log['loss'].append(current_loss)\n",
    "        log['accuracy'].append(total_acc)\n",
    "\n",
    "        \n",
    "        log['v_loss_std'].append(np.std(v_loss))\n",
    "        log['v_accuracy_std'].append(np.std(v_acc))\n",
    "        v_loss = sum(v_loss)/valid_len\n",
    "        v_acc = sum(v_acc)/valid_len\n",
    "        log['v_loss'].append(v_loss)\n",
    "        log['v_accuracy'].append(v_acc)\n",
    "\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (layers): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (1): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (2): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "Starting Epoch 1\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for -: 'float' and 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m loss_function \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[0;32m      6\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m RSAG(model\u001b[38;5;241m.\u001b[39mparameters(),  lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-4\u001b[39m, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m.9\u001b[39m, beta\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m9e-5\u001b[39m)\n\u001b[1;32m----> 7\u001b[0m log, best_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[11], line 53\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, loss_function, optimizer, loaders, device, verbose, save_path, log_path, n_epochs, print_every)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# current_loss += loss\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# Backpropagation\u001b[39;00m\n\u001b[0;32m     52\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m---> 53\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m current_loss\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[0;32m     56\u001b[0m total_acc\u001b[38;5;241m.\u001b[39mappend(calc_accuracy(outputs, targets))\n",
      "File \u001b[1;32mc:\\Users\\camgr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\optim\\optimizer.py:373\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    368\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    369\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    370\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    371\u001b[0m             )\n\u001b[1;32m--> 373\u001b[0m out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    374\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    376\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[30], line 57\u001b[0m, in \u001b[0;36mRSAG.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m     55\u001b[0m lr \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     56\u001b[0m alpha, beta \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124malpha\u001b[39m\u001b[38;5;124m'\u001b[39m], group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbeta\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m---> 57\u001b[0m alpha_bar \u001b[38;5;241m=\u001b[39m \u001b[38;5;241;43m1.0\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43malpha\u001b[49m\n\u001b[0;32m     58\u001b[0m momentum_buffer_list \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# step = group['step']\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;66;03m# print('group', group)\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     76\u001b[0m \n\u001b[0;32m     77\u001b[0m \u001b[38;5;66;03m# UPDATE GROUPS\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'float' and 'list'"
     ]
    }
   ],
   "source": [
    "model = MLP().to(device)\n",
    "print(model)\n",
    "\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = RSAG(model.parameters(),  lr=1e-4, alpha=.9, beta=9e-5)\n",
    "log, best_acc = train_model(model, loss_function, optimizer, loaders, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 58)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m<tokenize>:58\u001b[1;36m\u001b[0m\n\u001b[1;33m    loss_function = torch.nn.CrossEntropyLoss()\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "def train_with_hyperparameters(alpha_values, lr_values, save_log=False):\n",
    "    loss_function = torch.nn.CrossEntropyLoss()\n",
    "    best_alpha, best_lr = 0.0, 0.0\n",
    "    best_accuracy = 0.0\n",
    "    v_accs, acc_std, v_loss, loss_std = [], [], [], []\n",
    "    acc, loss = [], []\n",
    "    \n",
    "    for alpha in alpha_values:\n",
    "        for lr in lr_values:\n",
    "            beta = lr * alpha\n",
    "            \n",
    "            print(f\"----------- Training with alpha={alpha}, lr={lr} -----------------\")\n",
    "            \n",
    "            model = MLP().to(device)\n",
    "            optimizer = RSAG(model.parameters(), lr=lr, alpha=alpha, beta=beta)\n",
    "            log = train_model(model, loaders, optimizer, loss_function, device, epochs=20)\n",
    "            \n",
    "            if log['v_accuracy'][-1] > best_accuracy:\n",
    "                print(f\"Found a new best accuracy: {log['v_accuracy'][-1]}\")\n",
    "                print(f\"best alpha: {alpha}, best lr: {lr}\")\n",
    "                best_accuracy = log['v_accuracy'][-1]\n",
    "                best_alpha = alpha\n",
    "                best_lr = lr\n",
    "            \n",
    "            v_accs.append(log['v_accuracy'])\n",
    "            acc_std.append(log['v_accuracy_std'])\n",
    "            v_loss.append(log['v_loss'])\n",
    "            loss_std.append(log['v_loss_std'])\n",
    "            acc.append(log['accuracy'])\n",
    "            loss.append(log['loss'])\n",
    "            \n",
    "\n",
    "    \n",
    "    return best_alpha, best_lr, v_accs, acc_std, v_loss, loss_std, acc, loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Epoch 1\n",
      "Epoch 1 finished\n",
      "loss 4.6072\n",
      "Accuracy:  7.4900\n",
      "Starting Epoch 2\n",
      "Epoch 2 finished\n",
      "loss 4.6027\n",
      "Accuracy:  8.8633\n",
      "Starting Epoch 3\n",
      "Epoch 3 finished\n",
      "loss 4.5982\n",
      "Accuracy:  10.3017\n",
      "Starting Epoch 4\n",
      "Epoch 4 finished\n",
      "loss 4.5937\n",
      "Accuracy:  11.6917\n",
      "Starting Epoch 5\n",
      "Epoch 5 finished\n",
      "loss 4.5893\n",
      "Accuracy:  13.1383\n",
      "Training has completed\n"
     ]
    }
   ],
   "source": [
    "\n",
    "optimizer = RSAG(model.parameters(), lr=1e-4, alpha=.9, beta=9e-5)\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=1e-4, nesterov=True, momentum=0.9)\n",
    "log, best_acc = train_model(model, loaders, optimizer, loss_function, device, epochs=5)\n",
    "valid_loss = log['v_loss']\n",
    "train_loss = log['loss']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Loss and Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the loss as the network trained\n",
    "fig = plt.figure(figsize=(10,8))\n",
    "plt.plot(range(1,len(train_loss)+1),train_loss, label='Training Loss')\n",
    "plt.plot(range(1,len(valid_loss)+1),valid_loss,label='Validation Loss')\n",
    "\n",
    "# find position of lowest validation loss\n",
    "minposs = valid_loss.index(min(valid_loss))+1 \n",
    "plt.axvline(minposs, linestyle='--', color='r',label='Early Stopping Checkpoint')\n",
    "\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.ylim(0, 0.5) # consistent scale\n",
    "plt.xlim(0, len(train_loss)+1) # consistent scale\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "fig.savefig('loss_plot.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize lists to monitor test loss and accuracy\n",
    "test_loss = 0.0\n",
    "class_correct = list(0. for i in range(10))\n",
    "class_total = list(0. for i in range(10))\n",
    "\n",
    "model.eval() # prep model for evaluation\n",
    "\n",
    "for data, target in test_loader:\n",
    "    if len(target.data) != batch_size:\n",
    "        break\n",
    "    # forward pass: compute predicted outputs by passing inputs to the model\n",
    "    output = model(data)\n",
    "    # calculate the loss\n",
    "    loss = loss_function(output, target)\n",
    "    # update test loss \n",
    "    test_loss += loss.item()*data.size(0)\n",
    "    # convert output probabilities to predicted class\n",
    "    _, pred = torch.max(output, 1)\n",
    "    # compare predictions to true label\n",
    "    correct = np.squeeze(pred.eq(target.data.view_as(pred)))\n",
    "    # calculate test accuracy for each object class\n",
    "    for i in range(batch_size):\n",
    "        label = target.data[i]\n",
    "        class_correct[label] += correct[i].item()\n",
    "        class_total[label] += 1\n",
    "\n",
    "# calculate and print avg test loss\n",
    "test_loss = test_loss/len(test_loader.dataset)\n",
    "print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
    "\n",
    "for i in range(10):\n",
    "    if class_total[i] > 0:\n",
    "        print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (\n",
    "            str(i), 100 * class_correct[i] / class_total[i],\n",
    "            np.sum(class_correct[i]), np.sum(class_total[i])))\n",
    "    else:\n",
    "        print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))\n",
    "\n",
    "print('\\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (\n",
    "    100. * np.sum(class_correct) / np.sum(class_total),\n",
    "    np.sum(class_correct), np.sum(class_total)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
