{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../rsag_convex.png\" alt=\"algoconvex\" />\n",
    "<img src=\"../x_update.png\" alt=\"x_update\" />\n",
    "<img src=\"../mean.png\" alt=\"mean\" />\n",
    "<img src=\"../rsag_composite.png\" alt=\"algo\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Parameters :__\n",
    "- $\\alpha$: (1-$\\alpha$) weight of aggregated x on current state, i.e. momentum\n",
    "- $\\lambda$: learning rate\n",
    "- $\\beta$: change for aggregated x\n",
    "- $p_k$ termination probability\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  torch.optim import Adam, SGD, RMSprop\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch import nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as data_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch version: 2.1.2+cu121\n",
      "Using GPU, device name: NVIDIA GeForce GTX 1660 Ti\n"
     ]
    }
   ],
   "source": [
    "print('Using PyTorch version:', torch.__version__)\n",
    "if torch.cuda.is_available():\n",
    "    print('Using GPU, device name:', torch.cuda.get_device_name(0))\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    print('No GPU found, using CPU instead.') \n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import path\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from models import MLP\n",
    "from optimizers import RSAG, AccSGD\n",
    "from util import DataLoader\n",
    "from util import calc_accuracy, train_model, HPScheduler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run MLP:\n",
    "__TUNE DIFFERENT OPTIMIZERS__:\n",
    "- Nesterov w/ weight decay w/ Scheduled LR (SGD)\n",
    "- Momentum w/ weight decay w/ Scheduled LR (SGD)\n",
    "- Basic SGD\n",
    "- Adagrad?\n",
    "- Adam?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataLoader()\n",
    "loaders = data_loader.get_loaders()\n",
    "# loss_function = torch.nn.CrossEntropyLoss()\n",
    "# model = MLP().to(device)\n",
    "# print(model)\n",
    "\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=1e-4, nesterov=True, momentum=0.9)\n",
    "# optimizer = RSAG(model.parameters(), lr=1e-4, alpha=.9, beta=9e-5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nesterov w/ weight decay w/ Scheduled LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "model = MLP().to(device)\n",
    "\n",
    "optimizer = SGD(model.parameters(), lr=0.9, nesterov=True, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = HPScheduler.lambda_scheduler(optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Epoch 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 finished\n",
      "Epoch 1/100\n",
      "----------\n",
      "Loss 0.3942\n",
      "Accuracy:  88.6958\n",
      "Validation Loss 0.3559\n",
      "Validation Accuracy:  89.7500\n",
      "Starting Epoch 2\n",
      "Epoch 2 finished\n",
      "Starting Epoch 3\n",
      "Epoch 3 finished\n",
      "Epoch 3/100\n",
      "----------\n",
      "Loss 0.3158\n",
      "Accuracy:  91.0771\n",
      "Validation Loss 0.3245\n",
      "Validation Accuracy:  90.6083\n",
      "Starting Epoch 4\n",
      "Epoch 4 finished\n",
      "Starting Epoch 5\n",
      "Epoch 5 finished\n",
      "Epoch 5/100\n",
      "----------\n",
      "Loss 0.3063\n",
      "Accuracy:  91.4208\n",
      "Validation Loss 0.3275\n",
      "Validation Accuracy:  90.8250\n",
      "Early stopping at epoch 4\n",
      "Training has completed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'loss': [0.3941815403290093,\n",
       "   0.3248839430200557,\n",
       "   0.31583735146559777,\n",
       "   0.3109839874474953,\n",
       "   0.30630107706723114],\n",
       "  'accuracy': [88.69583333333334,\n",
       "   90.82708333333333,\n",
       "   91.07708333333333,\n",
       "   91.0875,\n",
       "   91.42083333333333],\n",
       "  'v_loss': [0.3559361208230257,\n",
       "   0.32003985326737167,\n",
       "   0.32445109051962695,\n",
       "   0.3205532369514306,\n",
       "   0.3275000611320138],\n",
       "  'v_accuracy': [89.75,\n",
       "   90.91666666666667,\n",
       "   90.60833333333333,\n",
       "   90.75833333333334,\n",
       "   90.825],\n",
       "  'v_loss_std': [0.11666602211074947,\n",
       "   0.09884205020363171,\n",
       "   0.09338897123260617,\n",
       "   0.11243451601954904,\n",
       "   0.1067623683076376],\n",
       "  'v_accuracy_std': [3.3322915038553673,\n",
       "   2.9988423692410966,\n",
       "   2.7757756673685923,\n",
       "   2.807715065473861,\n",
       "   2.7828717182076503],\n",
       "  'loss_std': [0.2305265310742368,\n",
       "   0.10814583791131628,\n",
       "   0.10320107471420607,\n",
       "   0.1004741430196087,\n",
       "   0.09932773567516279],\n",
       "  'accuracy_std': [7.244767029073851,\n",
       "   2.7608301577102172,\n",
       "   2.951746628645864,\n",
       "   2.8585853873317597,\n",
       "   2.766236788892488]},\n",
       " 90.91666666666667)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model(model, loss_function, optimizer, loaders, device, n_epochs=100, print_every=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_nesterov(alpha_values, lr_values, save_log=False):\n",
    "    loss_function = torch.nn.CrossEntropyLoss()\n",
    "    best_alpha, best_lr = 0.0, 0.0\n",
    "    best_accuracy = 0.0\n",
    "    v_accs, acc_std, v_loss, loss_std = [], [], [], []\n",
    "    acc, loss = [], []\n",
    "    \n",
    "    for alpha in alpha_values:\n",
    "        for lr in lr_values:\n",
    "            beta = lr * alpha\n",
    "            \n",
    "            print(f\"----------- Training with alpha={alpha}, lr={lr} -----------------\")\n",
    "            \n",
    "            model = MLP().to(device)\n",
    "            optimizer = SGD(model.parameters(), lr=lr, nesterov=True, momentum=0.9)\n",
    "\n",
    "\n",
    "            log, best_accuracy = train_model(model,loss_function,optimizer,loaders,print_every=5):\n",
    "            if log['v_accuracy'][-1] > best_accuracy:\n",
    "                print(f\"Found a new best accuracy: {log['v_accuracy'][-1]}\")\n",
    "                print(f\"best alpha: {alpha}, best lr: {lr}\")\n",
    "                best_accuracy = log['v_accuracy'][-1]\n",
    "                best_alpha = alpha\n",
    "                best_lr = lr\n",
    "            \n",
    "            v_accs.append(log['v_accuracy'])\n",
    "            acc_std.append(log['v_accuracy_std'])\n",
    "            v_loss.append(log['v_loss'])\n",
    "            loss_std.append(log['v_loss_std'])\n",
    "            acc.append(log['accuracy'])\n",
    "            loss.append(log['loss'])\n",
    "            \n",
    "\n",
    "    \n",
    "    return best_alpha, best_lr, v_accs, acc_std, v_loss, loss_std, acc, loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (layers): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (1): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (2): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "Starting Epoch 1\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'copy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# optimizer = torch.optim.SGD(model.parameters(), lr=1e-4, nesterov=True, momentum=0.9)\u001b[39;00m\n\u001b[0;32m      6\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m RSAG(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-4\u001b[39m, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m.9\u001b[39m, beta\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m9e-5\u001b[39m)\n\u001b[1;32m----> 8\u001b[0m log \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\camgr\\Documents\\stoch_prog\\notebooks\\..\\util\\train_pipeline.py:54\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, loss_function, optimizer, loaders, device, verbose, save_path, log_path, n_epochs)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# current_loss += loss\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# Backpropagation\u001b[39;00m\n\u001b[0;32m     53\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m---> 54\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m current_loss\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[0;32m     57\u001b[0m total_acc\u001b[38;5;241m.\u001b[39mappend(calc_accuracy(outputs, targets))\n",
      "File \u001b[1;32mc:\\Users\\camgr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\optim\\optimizer.py:373\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    368\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    369\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    370\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    371\u001b[0m             )\n\u001b[1;32m--> 373\u001b[0m out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    374\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    376\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\camgr\\Documents\\stoch_prog\\notebooks\\..\\optimizers\\rsag.py:87\u001b[0m, in \u001b[0;36mRSAG.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;66;03m# if weight_decay != 0:\u001b[39;00m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;66;03m#     grad_d.add_(weight_decay, p.data)\u001b[39;00m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmomentum_aggr\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m param_state:\n\u001b[1;32m---> 87\u001b[0m     param_state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmomentum_aggr\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mcopy\u001b[49m\u001b[38;5;241m.\u001b[39mdeepcopy(p\u001b[38;5;241m.\u001b[39mdata)\n\u001b[0;32m     88\u001b[0m     param_state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprev_momentum_aggr\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(p\u001b[38;5;241m.\u001b[39mdata)\n\u001b[0;32m     89\u001b[0m buf \u001b[38;5;241m=\u001b[39m param_state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmomentum_aggr\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'copy' is not defined"
     ]
    }
   ],
   "source": [
    "model = MLP().to(device)\n",
    "print(model)\n",
    "\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "log = train_model(model, loss_function, optimizer, loaders, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 58)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m<tokenize>:58\u001b[1;36m\u001b[0m\n\u001b[1;33m    loss_function = torch.nn.CrossEntropyLoss()\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "def train_with_hyperparameters(alpha_values, lr_values, save_log=False):\n",
    "    loss_function = torch.nn.CrossEntropyLoss()\n",
    "    best_alpha, best_lr = 0.0, 0.0\n",
    "    best_accuracy = 0.0\n",
    "    v_accs, acc_std, v_loss, loss_std = [], [], [], []\n",
    "    acc, loss = [], []\n",
    "    \n",
    "    for alpha in alpha_values:\n",
    "        for lr in lr_values:\n",
    "            beta = lr * alpha\n",
    "            \n",
    "            print(f\"----------- Training with alpha={alpha}, lr={lr} -----------------\")\n",
    "            \n",
    "            model = MLP().to(device)\n",
    "            optimizer = RSAG(model.parameters(), lr=lr, alpha=alpha, beta=beta)\n",
    "            log = train_model(model, loaders, optimizer, loss_function, device, epochs=20)\n",
    "            \n",
    "            if log['v_accuracy'][-1] > best_accuracy:\n",
    "                print(f\"Found a new best accuracy: {log['v_accuracy'][-1]}\")\n",
    "                print(f\"best alpha: {alpha}, best lr: {lr}\")\n",
    "                best_accuracy = log['v_accuracy'][-1]\n",
    "                best_alpha = alpha\n",
    "                best_lr = lr\n",
    "            \n",
    "            v_accs.append(log['v_accuracy'])\n",
    "            acc_std.append(log['v_accuracy_std'])\n",
    "            v_loss.append(log['v_loss'])\n",
    "            loss_std.append(log['v_loss_std'])\n",
    "            acc.append(log['accuracy'])\n",
    "            loss.append(log['loss'])\n",
    "            \n",
    "\n",
    "    \n",
    "    return best_alpha, best_lr, v_accs, acc_std, v_loss, loss_std, acc, loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Epoch 1\n",
      "Epoch 1 finished\n",
      "loss 4.6072\n",
      "Accuracy:  7.4900\n",
      "Starting Epoch 2\n",
      "Epoch 2 finished\n",
      "loss 4.6027\n",
      "Accuracy:  8.8633\n",
      "Starting Epoch 3\n",
      "Epoch 3 finished\n",
      "loss 4.5982\n",
      "Accuracy:  10.3017\n",
      "Starting Epoch 4\n",
      "Epoch 4 finished\n",
      "loss 4.5937\n",
      "Accuracy:  11.6917\n",
      "Starting Epoch 5\n",
      "Epoch 5 finished\n",
      "loss 4.5893\n",
      "Accuracy:  13.1383\n",
      "Training has completed\n"
     ]
    }
   ],
   "source": [
    "\n",
    "optimizer = RSAG(model.parameters(), lr=1e-4, alpha=.9, beta=9e-5)\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=1e-4, nesterov=True, momentum=0.9)\n",
    "train_model(model, loaders, optimizer, loss_function, device, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
