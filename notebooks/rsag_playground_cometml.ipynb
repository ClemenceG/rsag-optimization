{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../rsag_convex.png\" alt=\"algoconvex\" />\n",
    "<img src=\"../x_update.png\" alt=\"x_update\" />\n",
    "<img src=\"../mean.png\" alt=\"mean\" />\n",
    "<img src=\"../rsag_composite.png\" alt=\"algo\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Parameters :__\n",
    "- $\\alpha$: (1-$\\alpha$) weight of aggregated x on current state, i.e. momentum\n",
    "- $\\lambda$: learning rate\n",
    "- $\\beta$: change for aggregated x\n",
    "- $p_k$ termination probability\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.optimizer import Optimizer, required\n",
    "import torch\n",
    "import copy\n",
    "from torch.nn import functional as F\n",
    "from torch import nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x23aab3208f0>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchvision.datasets as dsets\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "import torch.utils.data as data_utils\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch version: 2.1.2+cu121\n",
      "Using GPU, device name: NVIDIA GeForce GTX 1660 Ti\n"
     ]
    }
   ],
   "source": [
    "print('Using PyTorch version:', torch.__version__)\n",
    "if torch.cuda.is_available():\n",
    "    print('Using GPU, device name:', torch.cuda.get_device_name(0))\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    print('No GPU found, using CPU instead.') \n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import path\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from models import MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RSAG(Optimizer):\n",
    "    r\"\"\"\n",
    "    Args:\n",
    "        params (iterable): iterable of parameters to optimize or dicts defining\n",
    "            parameter groups\n",
    "        lr (float): learning rate (lambda) (required)\n",
    "        kappa (float): lambda  (default: 1000)\n",
    "        xi (float, optional): statistical advantage parameter (default: 10)\n",
    "        smallConst (float, optional): any value <=1 (default: 0.7)\n",
    "    Example:\n",
    "        >>> from RSAG import *\n",
    "        >>> optimizer = RSAG(model.parameters(), lr=0.1, kappa = 1000.0, xi = 10.0)\n",
    "        >>> optimizer.zero_grad()\n",
    "        >>> loss_fn(model(input), target).backward()\n",
    "        >>> optimizer.step()\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, params, lr=0.01, alpha = 0.1, beta = 0.1): #, smallConst = 0.7, weight_decay=0):\n",
    "        #defaults = dict(lr=lr, kappa=kappa, xi, smallConst=smallConst,\n",
    "                        # weight_decay=weight_decay)\n",
    "        defaults = dict(lr=lr, alpha=alpha, beta=beta)\n",
    "        super(RSAG, self).__init__(params, defaults)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super(RSAG, self).__setstate__(state)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        \"\"\" Performs a single optimization step.\n",
    "        Arguments:\n",
    "            closure (callable, optional): A closure that reevaluates the model\n",
    "                and returns the loss.\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            # weight_decay = group['weight_decay']\n",
    "            lr = group['lr']\n",
    "            alpha, beta = group['alpha'], group['beta']\n",
    "            alpha_bar = 1.0-alpha\n",
    "            momentum_buffer_list = []\n",
    "\n",
    "            # INITIALIZE GROUPS\n",
    "            # params_with_grad, d_p_list, momentum_buffer_list = [], [], []\n",
    "            # for p in group['params']:\n",
    "            #     if p.grad is not None:\n",
    "            #         params_with_grad.append(p)b\n",
    "            #         d_p_list.append(p.grad)\n",
    "            #         # if p.grad.is_sparse:\n",
    "            #         #     has_sparse_grad = True\n",
    "\n",
    "            #         state = self.state[p]\n",
    "            #         if 'momentum_aggr' not in state:\n",
    "            #             momentum_buffer_list.append(None)\n",
    "            #         else:\n",
    "            #             momentum_buffer_list.append(state['momentum_buffer'])\n",
    "            \n",
    "            # UPDATE GROUPS\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "\n",
    "                d_w = p.grad.data\n",
    "                # w = p.data\n",
    "                param_state = self.state[p]\n",
    "\n",
    "                # if weight_decay != 0:\n",
    "                #     grad_d.add_(weight_decay, p.data)\n",
    "                \n",
    "                if 'momentum_aggr' not in param_state:\n",
    "                    param_state['momentum_aggr'] = copy.deepcopy(p.data)\n",
    "                    param_state['prev_momentum_aggr'] = copy.deepcopy(p.data)\n",
    "                buf = param_state['momentum_aggr']\n",
    "                aggr_grad = (buf-param_state['prev_momentum_aggr'])\n",
    "                aggr_grad.mul_(alpha_bar)\n",
    "                aggr_grad.add_(d_w, alpha=alpha)\n",
    "                \n",
    "                param_state['prev_momentum_aggr'] = copy.deepcopy(buf)\n",
    "                \n",
    "                # Update momentum buffer:'\n",
    "                buf.mul_(alpha_bar)\n",
    "                buf.add_(p.data, alpha=alpha)\n",
    "                buf.add_(aggr_grad, alpha=-beta)\n",
    "                \n",
    "                p.data.add_(aggr_grad, alpha=-lr)\n",
    "                # print('aggr_grad', aggr_grad)\n",
    "            \n",
    "            # UPDATE MOMENTUM BUFFER\n",
    "            # for p, momentum_buffer in zip(params_with_grad, momentum_buffer_list):\n",
    "            #     state = self.state[p]\n",
    "                \n",
    "            #     state['momentum_buffer'] = momentum_buffer\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = dsets.MNIST(root = './data', train = True, transform = ToTensor(), download = True)\n",
    "test_data = dsets.MNIST(root = './data', train = False, transform = ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data.targets = F.one_hot(train_data.targets)\n",
    "# indices = torch.arange(10000)\n",
    "# train_data = data_utils.Subset(train_data, indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sampler, valid_sampler = get_train_valid_sampler(train_data)\n",
    "loaders = {}\n",
    "loaders['train'] = torch.utils.data.DataLoader(train_data, batch_size=100, num_workers=1, sampler=train_sampler)\n",
    "loaders['valid'] = torch.utils.data.DataLoader(train_data, batch_size=100, num_workers=1, sampler=valid_sampler)\n",
    "loaders['test'] = torch.utils.data.DataLoader(test_data, batch_size=100, shuffle=True, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (layers): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = MLP().to(device)\n",
    "print(model)\n",
    "\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "optimizer = RSAG(model.parameters(), lr=1e-4, alpha=1e-2, beta=.1)\n",
    "# optimizer = torch.optim.Adagrad(mlp.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_accuracy(y_pred, labels):\n",
    "    predicted_digits = y_pred.argmax(1)                            # pick digit with largest network output\n",
    "    correct_ones = (predicted_digits == labels).type(torch.float)  # 1.0 for correct, 0.0 for incorrect\n",
    "    return correct_ones.sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(model, loaders, optimizer, loss_function, device, epochs=5, experiment=None):\n",
    "    log = {}\n",
    "    log['loss'], log['accuracy'] = [], []\n",
    "    log['v_loss'], log['v_accuracy'] = [], []\n",
    "    \n",
    "    log['v_loss_std'] = []\n",
    "    log['v_accuracy_std'] = []\n",
    "\n",
    "    \n",
    "    log['loss_std'] = []\n",
    "    log['accuracy_std'] = []\n",
    "\n",
    "    for epoch in range(0,epochs):\n",
    "        print(f'Starting Epoch {epoch+1}')\n",
    "\n",
    "        current_loss, total_acc = [], []\n",
    "        v_loss, v_acc = [], []\n",
    "\n",
    "        for data, targets in loaders['train']:\n",
    "            # inputs, targets = data\n",
    "            # inputs, targets = inputs.float(), targets.float()\n",
    "            # targets = targets.reshape((targets.shape[0], 1))\n",
    "            \n",
    "            # Copy data and targets to GPU\n",
    "            data = data.to(device)\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(data)\n",
    "\n",
    "            # Calculate the loss\n",
    "            loss = loss_function(outputs, targets)\n",
    "            # current_loss += loss\n",
    "\n",
    "            # Backpropagation\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            current_loss.append(loss.item())\n",
    "            total_acc.append(calc_accuracy(outputs, targets))\n",
    "            \n",
    "        # Validation\n",
    "        model.eval()\n",
    "\n",
    "        for data, targets in loaders['valid']:\n",
    "            data = data.to(device)\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            outputs = model(data)\n",
    "\n",
    "            loss = loss_function(outputs, targets)\n",
    "            v_loss.append(loss.item())\n",
    "            v_acc.append(calc_accuracy(outputs, targets))\n",
    "            \n",
    "        experiment.log_metric('v;loss', np.mean(current_loss), epoch)\n",
    "        \n",
    "\n",
    "            # if i%10 == 0:\n",
    "            #     print(f'Loss after mini-batch %5d: %.3f'%(i+1, current_loss/500))\n",
    "            #     current_loss = 0.0\n",
    "\n",
    "        print(f'Epoch {epoch+1} finished')\n",
    "        # current_loss /= len(loaders['train'])\n",
    "        # total_acc /= len(loaders['train'])\n",
    "        # print('loss {:.4f}'.format(current_loss))\n",
    "        # print('Accuracy:  {:.4f}'.format(total_acc))\n",
    "        \n",
    "        log['loss_std'].append(np.std(current_loss))\n",
    "        log['accuracy_std'].append(np.std(total_acc))\n",
    "\n",
    "        current_loss = sum(current_loss)/len(loaders['train'])\n",
    "        total_acc = sum(total_acc)/len(loaders['train'])\n",
    "        log['loss'].append(current_loss)\n",
    "        log['accuracy'].append(total_acc)\n",
    "\n",
    "        \n",
    "        log['v_loss_std'].append(np.std(v_loss))\n",
    "        log['v_accuracy_std'].append(np.std(v_acc))\n",
    "        v_loss = sum(v_loss)/len(loaders['valid'])\n",
    "        v_acc = sum(v_acc)/len(loaders['valid'])\n",
    "        log['v_loss'].append(v_loss)\n",
    "        log['v_accuracy'].append(v_acc)\n",
    "\n",
    "\n",
    "    experiment.log_metrics({\"accuracy\": log['v_accuracy'], \"loss\": log['v_loss']}, epoch=epoch)\n",
    "\n",
    "    print(\"Training has completed\")\n",
    "    return log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from comet_ml import Experiment\n",
    "from comet_ml.integration.pytorch import log_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m To get all data logged automatically, import comet_ml before the following modules: torch.\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Comet.ml Experiment Summary\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Data:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     display_summary_level : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     url                   : https://www.comet.com/clemenceg/general/6ec8447465bb4341a06910af29ae6cd4\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Metrics [count] (min, max):\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     accuracy [3] : (96.00833333333334, 96.83333333333333)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     loss [3]     : (0.10292138303630054, 0.1331247233785689)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     v;loss [3]   : (0.07695864576962777, 0.42371859843066584)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Parameters:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     alpha : 0.9\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     beta  : 0.9\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     lr    : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Uploads:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     environment details      : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     filename                 : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     git metadata             : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     git-patch (uncompressed) : 1 (567 bytes)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     installed packages       : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     notebook                 : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     source_code              : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m \n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m To get all data logged automatically, import comet_ml before the following modules: torch.\n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Experiment is live on comet.com https://www.comet.com/clemenceg/general/ef5750f6c81e4ec68710696cfbcf5a78\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- Training with alpha=0.9, lr=1 -----------------\n",
      "Starting Epoch 1\n",
      "Epoch 1 finished\n",
      "Starting Epoch 2\n",
      "Epoch 2 finished\n",
      "Starting Epoch 3\n",
      "Epoch 3 finished\n",
      "Starting Epoch 4\n",
      "Epoch 4 finished\n",
      "Starting Epoch 5\n",
      "Epoch 5 finished\n",
      "Training has completed\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DataParallel' object has no attribute 'id'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[78], line 39\u001b[0m\n\u001b[0;32m     37\u001b[0m log \u001b[38;5;241m=\u001b[39m run_training(model, loaders, optimizer, loss_function, device, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, experiment\u001b[38;5;241m=\u001b[39mexperiment)\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# Seamlessly log your Pytorch model\u001b[39;00m\n\u001b[1;32m---> 39\u001b[0m \u001b[43mlog_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTheModel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m experiment\u001b[38;5;241m.\u001b[39mend()\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m log[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mv_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m best_accuracy:\n",
      "File \u001b[1;32mc:\\Users\\camgr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\comet_ml\\integration\\pytorch\\public_api.py:79\u001b[0m, in \u001b[0;36mlog_model\u001b[1;34m(experiment, model, model_name, metadata, pickle_module, **torch_save_args)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlog_model\u001b[39m(\n\u001b[0;32m     23\u001b[0m     experiment, model, model_name, metadata\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, pickle_module\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtorch_save_args\n\u001b[0;32m     24\u001b[0m ):\n\u001b[0;32m     25\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;124;03m    Logs a Pytorch model to an experiment. This will save the model using\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;124;03m    [torch.save](https://pytorch.org/docs/stable/generated/torch.save.html) and save it as an\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;124;03m    Returns: None\u001b[39;00m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 79\u001b[0m     \u001b[43m_track_usage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexperiment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     81\u001b[0m     state_dict \u001b[38;5;241m=\u001b[39m model_logging\u001b[38;5;241m.\u001b[39mget_state_dict(model)\n\u001b[0;32m     83\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pickle_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\camgr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\comet_ml\\integration\\pytorch\\public_api.py:94\u001b[0m, in \u001b[0;36m_track_usage\u001b[1;34m(experiment, model)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_track_usage\u001b[39m(experiment, model):\n\u001b[0;32m     93\u001b[0m     comet_ml\u001b[38;5;241m.\u001b[39mevent_tracker\u001b[38;5;241m.\u001b[39mregister(\n\u001b[1;32m---> 94\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcomet_ml.integration.pytorch.log_model-called\u001b[39m\u001b[38;5;124m\"\u001b[39m, experiment_key\u001b[38;5;241m=\u001b[39m\u001b[43mexperiment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mid\u001b[49m\n\u001b[0;32m     95\u001b[0m     )\n\u001b[0;32m     96\u001b[0m     experiment\u001b[38;5;241m.\u001b[39m_report(\n\u001b[0;32m     97\u001b[0m         event_name\u001b[38;5;241m=\u001b[39mcomet_ml\u001b[38;5;241m.\u001b[39m_reporting\u001b[38;5;241m.\u001b[39mPYTORCH_MODEL_SAVING_EXPLICIT_CALL,\n\u001b[0;32m     98\u001b[0m         err_msg\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mtype\u001b[39m(model)),\n\u001b[0;32m     99\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\camgr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1695\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1693\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[0;32m   1694\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1695\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataParallel' object has no attribute 'id'"
     ]
    }
   ],
   "source": [
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "lr_values = [1, .01, .001, .0001 , .00001]\n",
    "alpha_values = [.9, .75, .5, .25,.1]\n",
    "best_alpha, best_lr = 0.0, 0.0\n",
    "best_accuracy = 0.0\n",
    "for alpha in alpha_values:\n",
    "        for lr in lr_values:\n",
    "            experiment = Experiment(\n",
    "                api_key=\"tP7gJL6gsW7R9Mu1CsUKqc022\",\n",
    "                project_name=\"general\",\n",
    "                workspace=\"clemenceg\"\n",
    "            )\n",
    "\n",
    "\n",
    "            beta = lr*alpha\n",
    "            \n",
    "            print(f\"----------- Training with alpha={alpha}, lr={lr} -----------------\")\n",
    "            # Update optimizer with new hyperparameters\n",
    "            # model = MLP().to(device)\n",
    "            model = MLP()\n",
    "            # model = nn.DataParallel(model, device_ids=[0])\n",
    "            model = model.to(device)\n",
    "\n",
    "            # Report multiple hyperparameters using a dictionary:\n",
    "            hyper_params = {\n",
    "            \"alpha\": alpha,\n",
    "            \"lr\": lr,\n",
    "            \"beta\": beta,\n",
    "            }   \n",
    "            experiment.log_parameters(hyper_params)\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "            optimizer = RSAG(model.parameters(), lr=lr, alpha=alpha, beta=beta)\n",
    "            log = run_training(model, loaders, optimizer, loss_function, device, epochs=5, experiment=experiment)\n",
    "            # Seamlessly log your Pytorch model\n",
    "            log_model(model, \"TheModel\", model)\n",
    "            experiment.end()\n",
    "\n",
    "\n",
    "            if log['v_accuracy'][-1] > best_accuracy:\n",
    "                print(f\"Found a new best accuracy: {log['v_accuracy'][-1]}\")\n",
    "                print(f\"best alpha: {alpha}, best lr: {lr}\")\n",
    "                best_accuracy = log['v_accuracy'][-1]\n",
    "                best_alpha = alpha\n",
    "                best_lr = lr\n",
    "\n",
    "            # Create a filename based on hyperparameters\n",
    "            filename = f\"../logs/results_lr_{lr}_alpha_{alpha}.csv\"\n",
    "            with open(filename, 'w') as f:\n",
    "                f.write(f\"loss,{log['loss']}\\n\" ) \n",
    "                f.write(f\"accuracy,{log['accuracy']}\\n\" )\n",
    "                f.write(f\"loss,{log['v_loss']}\\n\" ) \n",
    "                f.write(f\"accuracy,{log['v_accuracy']}\\n\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Epoch 1\n",
      "Epoch 1 finished\n",
      "loss 4.6072\n",
      "Accuracy:  7.4900\n",
      "Starting Epoch 2\n",
      "Epoch 2 finished\n",
      "loss 4.6027\n",
      "Accuracy:  8.8633\n",
      "Starting Epoch 3\n",
      "Epoch 3 finished\n",
      "loss 4.5982\n",
      "Accuracy:  10.3017\n",
      "Starting Epoch 4\n",
      "Epoch 4 finished\n",
      "loss 4.5937\n",
      "Accuracy:  11.6917\n",
      "Starting Epoch 5\n",
      "Epoch 5 finished\n",
      "loss 4.5893\n",
      "Accuracy:  13.1383\n",
      "Training has completed\n"
     ]
    }
   ],
   "source": [
    "\n",
    "optimizer = RSAG(model.parameters(), lr=1e-4, alpha=.9, beta=9e-5)\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=1e-4, nesterov=True, momentum=0.9)\n",
    "run_training(model, loaders, optimizer, loss_function, device, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
