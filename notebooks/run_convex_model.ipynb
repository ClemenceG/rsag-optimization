{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../rsag_convex.png\" alt=\"algoconvex\" />\n",
    "<img src=\"../x_update.png\" alt=\"x_update\" />\n",
    "<img src=\"../mean.png\" alt=\"mean\" />\n",
    "<img src=\"../rsag_composite.png\" alt=\"algo\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Parameters :__\n",
    "- $\\alpha$: (1-$\\alpha$) weight of aggregated x on current state, i.e. momentum\n",
    "- $\\lambda$: learning rate\n",
    "- $\\beta$: change for aggregated x\n",
    "- $p_k$ termination probability\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\camgr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_acc(truth, pred):\n",
    "  truth = [truth]\n",
    "  dif = np.abs([1-pred[i]+truth[i] for i in range(len(pred))])\n",
    "  return np.sum(1-dif) / float(len(pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Line():\n",
    "    \"\"\"\n",
    "        Linear Model with two weights w0 (intercept) and w1 (slope)\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=10, rsag=False):\n",
    "        self.params = None\n",
    "        self.aggr_params = None\n",
    "        self.rsag = rsag\n",
    "        \n",
    "    def predict(self,x):\n",
    "        return self.params[0] + self.params[1]*x\n",
    "\n",
    "    def fit(self, x, y, optimizer):\n",
    "        N = len(x)\n",
    "\n",
    "        def gradient(x, y, params):\n",
    "\n",
    "            total = [0,0]\n",
    "            for x_temp,y_temp in zip(x,y):\n",
    "\n",
    "                w = params[1]\n",
    "                b = params[0]\n",
    "\n",
    "                yh = b + w*x_temp\n",
    "\n",
    "                db = (yh - y_temp)\n",
    "                dw = x_temp*(yh - y_temp)\n",
    "                train_acc = evaluate_acc(y_temp, yh)\n",
    "                # print(train_acc)\n",
    "                \n",
    "                total = [total[0] + db, total[1] + dw]\n",
    "\n",
    "            gradient = [t/N for t in total]\n",
    "            return gradient, train_acc\n",
    "\n",
    "        if self.params is None:\n",
    "            self.params = [np.random.uniform(0,1,1) for _ in range(2)]\n",
    "\n",
    "            if self.rsag:\n",
    "                self.aggr_params = [np.copy(w) for w in self.params]\n",
    "\n",
    "        if self.rsag:\n",
    "            self.params, self.aggr_params, train_acc = optimizer.mini_batch_step(gradient, x, y, self.params, self.aggr_params)\n",
    "        else:\n",
    "            self.params, train_acc = optimizer.mini_batch_step(gradient, x, y, self.params)\n",
    "\n",
    "        return train_acc\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"y = {self.params[0]} + {self.params[1]}*x\"\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RSAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_lamda = lambda lr, t: lr/(1+t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RSAG:\n",
    "\n",
    "    def __init__(self, \n",
    "                 learning_rate=.001, \n",
    "                 alpha=0.009, \n",
    "                 beta=.000009, \n",
    "                 max_iters=200, \n",
    "                 epsilon=1e-8, \n",
    "                 lr_fn = None,\n",
    "                alpha_fn = None,\n",
    "                beta_fn = None,\n",
    "                 ):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iters = max_iters\n",
    "        self.epsilon = epsilon\n",
    "        self.alpha = alpha  # momentum param\n",
    "        self.beta = beta \n",
    "\n",
    "        self.lr_fn = lr_fn\n",
    "        self.alpha_fn = alpha_fn\n",
    "        self.beta_fn = beta_fn\n",
    "\n",
    "        self.update_params = False\n",
    "\n",
    "        self.t = 0\n",
    "\n",
    "    \n",
    "    def set_update_params(self, update_params):\n",
    "        self.update_params = update_params\n",
    "\n",
    "    def mini_batch_step(self, \n",
    "                       gradient_fn,\n",
    "                       x, \n",
    "                       y,\n",
    "                       params, \n",
    "                       agg_params,\n",
    "                       ):\n",
    "        \n",
    "\n",
    "\n",
    "        # if self.update_params:\n",
    "        #     print('Update params')\n",
    "        #     if self.lr_fn is not None:\n",
    "        #         self.learning_rate = self.lr_fn(self.learning_rate, self.t)\n",
    "        #         # print('New learning rate:', self.learning_rate)\n",
    "        #     if self.alpha_fn is not None:\n",
    "        #         self.alpha = self.alpha_fn(self.alpha, self.t)\n",
    "        #         # print('New alpha:', self.alpha)\n",
    "        #     if self.beta_fn is not None:\n",
    "        #         self.beta = self.beta_fn(self.beta, self.t)\n",
    "        #         # print('New beta:', self.beta)\n",
    "        #     self.update_params = False\n",
    "\n",
    "        grad = None\n",
    "\n",
    "\n",
    "        proj_params = [(1-self.alpha) * a_p + self.alpha * p for p, a_p in zip(params, agg_params)]\n",
    "\n",
    "        grad, temp_acc = gradient_fn(x, y, proj_params)\n",
    "\n",
    "        train_acc = ( self.t, temp_acc ) \n",
    "\n",
    "        for p in range(len(params)):\n",
    "            agg_params[p] -= self.beta * (grad[p])[0]\n",
    "            params[p] -= self.learning_rate * (grad[p])[0]\n",
    "\n",
    "            \n",
    "        self.t += 1\n",
    "            \n",
    "        return params, agg_params, train_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "c_clf = MLPClassifier(hidden_layer_sizes=(), activation='identity', \n",
    "                    solver='sgd', \n",
    "                    alpha=0.0001, momentum=0.8, nesterovs_momentum=True,\n",
    "                    batch_size=64, \n",
    "                    learning_rate='constant', learning_rate_init=0.09, \n",
    "                    verbose=True)\n",
    "%time c_clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GD:\n",
    "    \"\"\"\n",
    "        gd: will estimate the parameters w1 and w2 (here it uses least square cost function)\n",
    "        model: the model we are trying to optimize using gradient descent\n",
    "        xs: all point on the plane\n",
    "        ys: all response on the plane\n",
    "        learning_rate: the learning rate for the step that weights update will take\n",
    "        max_num_iteration: the number of iteration before we stop updating\n",
    "    \"\"\"    \n",
    "    def __init__(self, learning_rate=.001):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.t = 0\n",
    "\n",
    "    def mini_batch_step(self, \n",
    "                        gradient_fn,\n",
    "                        x, \n",
    "                        y,\n",
    "                        params\n",
    "                        ):\n",
    "        \n",
    "        # Updating the model parameters\n",
    "        grad, temp_acc = gradient_fn(x, y, params)\n",
    "        # chunk.append(temp_acc)\n",
    "        train_acc =  ( self.t, temp_acc )\n",
    "\n",
    "        for p in range(len(params)):\n",
    "            params[p] -= self.learning_rate * (grad[p])[0]\n",
    "        self.t += 1\n",
    "        return params, train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.00021853 2.00016323 3.00010794 4.00005264 4.99999734 5.99994205\n",
      " 6.99988675]\n",
      "y = [0.00027383] + [0.9999447]*x\n"
     ]
    }
   ],
   "source": [
    "x_train = [1,2,3,4,5,6,7]\n",
    "y_train = [1,2,3,4,5,6,7]\n",
    "\n",
    "model = Line(num_classes=10, rsag=False)\n",
    "optimizer = GD(learning_rate=.02)\n",
    "\n",
    "for i in range(2000):\n",
    "    model.fit(x_train, y_train, optimizer)\n",
    "print(model.predict(x_train))\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.97197172 2.01008872 3.04820571 4.08632271 5.1244397  6.16255669\n",
      " 7.20067369]\n",
      "y = [-0.06614527] + [1.03811699]*x\n"
     ]
    }
   ],
   "source": [
    "x_train = [1,2,3,4,5,6,7]\n",
    "y_train = [1,2,3,4,5,6,7]\n",
    "\n",
    "model = Line(num_classes=10, rsag=True)\n",
    "optimizer = RSAG(learning_rate=.02, alpha=0.9, beta=0.001)\n",
    "\n",
    "for i in range(2000):\n",
    "    model.fit(x_train, y_train, optimizer)\n",
    "print(model.predict(x_train))\n",
    "print(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
