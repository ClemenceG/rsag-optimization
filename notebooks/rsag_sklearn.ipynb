{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../rsag_convex.png\" alt=\"algoconvex\" />\n",
    "<img src=\"../x_update.png\" alt=\"x_update\" />\n",
    "<img src=\"../mean.png\" alt=\"mean\" />\n",
    "<img src=\"../rsag_composite.png\" alt=\"algo\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Parameters :__\n",
    "- $\\alpha$: (1-$\\alpha$) weight of aggregated x on current state, i.e. momentum\n",
    "- $\\lambda$: learning rate\n",
    "- $\\beta$: change for aggregated x\n",
    "- $p_k$ termination probability\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import time\n",
    "import numpy as np\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import path\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from util import DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
      "29515/29515 [==============================] - 0s 1us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
      "26421880/26421880 [==============================] - 2s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
      "5148/5148 [==============================] - 0s 0s/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
      "4422102/4422102 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# packaging it all into a function\n",
    "def preprocess_fashion_mnist():\n",
    "  import random as rand\n",
    "\n",
    "\n",
    "  (x_train, y_train), (x_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "  mean_mat = np.mean(x_train, axis=0)\n",
    "\n",
    "  # centering the data by removing the pixel wise mean from every pixel in every image\n",
    "  x_train_centered = x_train - mean_mat\n",
    "  x_test_centered = x_test - mean_mat\n",
    "\n",
    "  # normalizing the grayscale values to values in interval [0,1]\n",
    "  x_train_normalized = x_train_centered/255.0\n",
    "  x_test_normalized = x_test_centered/255.0\n",
    "\n",
    "  #finally, flattening the data\n",
    "  x_train = np.reshape(x_train_normalized, (60000,784))\n",
    "  x_test = np.reshape(x_test_normalized, (10000, 784))\n",
    "  \n",
    "  #converting the test data to one hot encodings\n",
    "  y_train = keras.utils.to_categorical(y_train, num_classes=10)\n",
    "  y_test = keras.utils.to_categorical(y_test, num_classes=10)\n",
    "\n",
    "  return x_train, y_train, x_test, y_test\n",
    "x_train, y_train, x_test, y_test = preprocess_fashion_mnist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Implementation - Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_acc(pred, truth):\n",
    "  counter =0\n",
    "\n",
    "  for i in range(len(pred)):\n",
    "    maxVal = np.where(pred[i] == np.amax(pred[i]))\n",
    "    counter += 1 if maxVal == np.where(truth[i]==1) else 0\n",
    "  return counter * 100.0 / float(len(pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#activation functions\n",
    "softmax1D = lambda z: np.exp(z)/float(sum(np.exp(z)))\n",
    "softmax2D = lambda z: np.array([np.exp(i)/float(sum(np.exp(i))) for i in z])\n",
    "# relu = lambda y: y[y <= 0]=0\n",
    "def relu(x):\n",
    "  alpha = 0.1\n",
    "  x=np.array(x).astype(float)\n",
    "  # x[x<=0]=0.1*x\n",
    "  np.putmask(x, x<0, alpha*x)\n",
    "  return x\n",
    "def relu_grad(x):\n",
    "  alpha = 0.1\n",
    "  x=np.array(x).astype(float)\n",
    "  x[x>0]=1\n",
    "  x[x<=0]=alpha\n",
    "  return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn.neural_network.MLPClassifier(hidden_layer_sizes=(100,), activation='relu', solver='adam', alpha=0.0001, batch_size=64, learning_rate='constant', learning_rate_init=0.001, verbose=True)\n",
    "logistic = lambda z: 1./ (1 + np.exp(-z))\n",
    "\n",
    "class MLP:\n",
    "\n",
    "    def __init__(self, M = 128, num_classes = 10, rsag=False):\n",
    "        self.M = M\n",
    "        self.num_classes = num_classes\n",
    "        self.rsag = rsag\n",
    "\n",
    "    def fit(self, x, y, optimizer, x_valid=None, y_valid=None):\n",
    "        N,D = x.shape\n",
    "        def gradient(x, y, params):\n",
    "            w = params[0] # v.shape = (D, M), w.shape = (M)\n",
    "            z = np.dot(x, w)\n",
    "            yh = softmax2D(z)#N\n",
    "            dy = yh - y #N\n",
    "            train_acc = evaluate_acc(yh, y)\n",
    "            dw = np.dot(x.T, dy)/N #M\n",
    "            dparams = [dw]\n",
    "            return dparams ,train_acc\n",
    "\n",
    "        initializer = keras.initializers.GlorotNormal()\n",
    "        w = initializer(shape=(D, self.num_classes))\n",
    "        params0 = [w]\n",
    "\n",
    "        if self.rsag:\n",
    "            a_w = initializer(shape=(D, self.num_classes))\n",
    "            aggr_params = [a_w]\n",
    "            self.params, self.aggr_params, train_accs, batch_train_accs = optimizer.mini_batch_step(gradient, x, y, params0, aggr_params, x_val=x_valid, y_val=y_valid)\n",
    "        else:\n",
    "            self.params, train_accs, batch_train_accs = optimizer.run_mini_batch(gradient, x, y, params0)\n",
    "\n",
    "        return self, train_accs, batch_train_accs\n",
    "\n",
    "    def predict(self, x):\n",
    "        # print('self:',self)\n",
    "        # print('self==None:',self==None)\n",
    "        w = self.params[0]\n",
    "        # print(w.shape)\n",
    "        # z = relu(np.dot(x, w)) #N x M\n",
    "        yh = softmax2D(np.dot(x, w))#N\n",
    "        return yh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini batcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mini_batcher(x, y, mini_batch_size):\n",
    "  zipped = np.hstack( (x, y ) )\n",
    "  np.random.shuffle(zipped)\n",
    "  x_batches, y_batches = [], []\n",
    "  mini_batches = []\n",
    "  batch_num = x.shape[0] // mini_batch_size\n",
    "  for i in range(batch_num):\n",
    "    x_batch = zipped[ i * mini_batch_size : (i+1) * mini_batch_size, :-10]\n",
    "    y_batch = zipped[ i * mini_batch_size : (i+1) * mini_batch_size, -10:]\n",
    "    mini_batches.append( ( x_batch, y_batch) )\n",
    "    # mini_batches.append( ( x_batch, np.argmax(y_batch,axis=1)[:,None] ) )\n",
    "  if x.shape[0] % mini_batch_size != 0:\n",
    "    x_batch = zipped[ batch_num * mini_batch_size :, :-10]\n",
    "    y_batch = zipped[ batch_num * mini_batch_size :, -10:]\n",
    "    # print(\"Length of last mini-batch =\", y_batch.shape[0])\n",
    "    mini_batches.append( ( x_batch, y_batch ) )\n",
    "    # mini_batches.append( ( x_batch, np.argmax(y_batch,axis=1) ) )\n",
    "  # print(mini_batches[0])\n",
    "  # print(\"yShape = \",y.shape)\n",
    "  return mini_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientDescent:\n",
    "\n",
    "    def __init__(self, learning_rate=.001, max_iters=2e4, epsilon=1e-8, batch_size=32):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iters = max_iters\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def run(self, gradient_fn, x, y, params):\n",
    "        norms = np.array([np.inf])\n",
    "        t = 1\n",
    "        while np.any(norms > self.epsilon) and t < self.max_iters:\n",
    "            grad = gradient_fn(x, y, params)\n",
    "            # print(grad[0].shape)\n",
    "            # print(params[0].shape)\n",
    "            for p in range(len(params)):\n",
    "                params[p] -= self.learning_rate * grad[p]\n",
    "            t += 1\n",
    "            norms = np.array([np.linalg.norm(g) for g in grad])\n",
    "        print(t)\n",
    "        return params\n",
    "\n",
    "    def run_mini_batch(self, gradient_fn, x, y, params, batch_size=32):\n",
    "        train_acc, batch_train_acc, chunk = [], [], []\n",
    "        norms = np.array([np.inf])\n",
    "        t=1\n",
    "        mini_batches = mini_batcher(x, y, batch_size)\n",
    "        while np.any(norms > self.epsilon) and t < self.max_iters * len(mini_batches):\n",
    "            x_temp, y_temp = mini_batches[t % ( len(mini_batches)-1 ) ][0], mini_batches[t % ( len(mini_batches)-1 ) ][1]\n",
    "            grad, temp_acc = gradient_fn(x_temp, y_temp, params)\n",
    "            for p in range(len(params)):\n",
    "                params[p] -= self.learning_rate * grad[p]\n",
    "            chunk.append(temp_acc)\n",
    "            print(f\"Epoch{t}:{temp_acc}%\")\n",
    "            train_acc.append( ( t, temp_acc ) )\n",
    "            t += 1\n",
    "            if t%len(mini_batches) == 2:\n",
    "              batch_train_acc.append(np.mean(chunk))\n",
    "              chunk = []\n",
    "            norms = np.array([np.linalg.norm(g) for g in grad])\n",
    "        return params, train_acc, batch_train_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RSAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RSAG:\n",
    "\n",
    "    def __init__(self, learning_rate=.001, alpha=0.009, beta=.000009, max_iters=2e4, epsilon=1e-8, batch_size=32):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iters = max_iters\n",
    "        self.epsilon = epsilon\n",
    "        self.alpha = alpha  # momentum\n",
    "        self.beta = beta \n",
    "\n",
    "\n",
    "    def run(self, gradient_fn, x, y, params, agg_params):\n",
    "        norms = np.array([np.inf])\n",
    "        t = 1\n",
    "        while np.any(norms > self.epsilon) and t < self.max_iters:\n",
    "            \n",
    "\n",
    "            proj_params = [(1-self.alpha) * a_p + self.alpha * p for p, a_p in zip(params, agg_params)]\n",
    "            grad = gradient_fn(x, y, proj_params)\n",
    "            \n",
    "            for p in range(len(params)):\n",
    "                agg_params[p] -= self.beta * grad[p]\n",
    "                params[p] -= self.learning_rate * grad[p]\n",
    "            t += 1\n",
    "            norms = np.array([np.linalg.norm(g) for g in grad])\n",
    "        print(t)\n",
    "        return params, agg_params\n",
    "\n",
    "    def mini_batch_step(self, \n",
    "                       gradient_fn,\n",
    "                       x, \n",
    "                       y,\n",
    "                       params, \n",
    "                       agg_params, \n",
    "                       batch_size=32,\n",
    "                       ):\n",
    "        \n",
    "        train_acc, batch_train_acc, chunk = [], [], []\n",
    "        v_acc, v_mean_acc, v_chunk = [],  [], []\n",
    "        norms = np.array([np.inf])\n",
    "        t=0\n",
    "        stable_cnt, base = 0, 0.0\n",
    "\n",
    "        mini_batches = mini_batcher(x, y, batch_size)\n",
    "        grad = None\n",
    "        while np.any(norms > self.epsilon) and t < self.max_iters * len(mini_batches):\n",
    "\n",
    "            x_temp, y_temp = mini_batches[t %  len(mini_batches) ][0], mini_batches[t % len(mini_batches) ][1]\n",
    "            # x_val, y_val = mini_batches[t %  len(mini_batches)]\n",
    "\n",
    "            proj_params = [(1-self.alpha) * a_p + self.alpha * p for p, a_p in zip(params, agg_params)]\n",
    "\n",
    "            temp_grad, temp_acc = gradient_fn(x_temp, y_temp, proj_params)\n",
    "            if grad == None: grad = temp_grad \n",
    "            else:\n",
    "                for p in range(len(params)):\n",
    "                    grad[p] += temp_grad[p]\n",
    "                                \n",
    " \n",
    "            # v_chunk.append(evaluate_acc())\n",
    "            chunk.append(temp_acc)\n",
    "            train_acc.append( ( t, temp_acc ) )\n",
    "\n",
    "            if t%batch_size ==0:\n",
    "                for p in range(len(params)):\n",
    "                    agg_params[p] -= self.beta * (grad[p]/batch_size)\n",
    "                    params[p] -= self.learning_rate * (grad[p]/batch_size)\n",
    "                \n",
    "            t += 1\n",
    "            \n",
    "            if t % (len(mini_batches)*batch_size) == 0: break # After an Epoch\n",
    "           \n",
    "            norms = np.array([np.linalg.norm(g) for g in grad])\n",
    "        return params, agg_params, train_acc, batch_train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch1:15.625%\n",
      "Epoch2:6.25%\n",
      "Epoch3:18.75%\n",
      "Epoch4:21.875%\n",
      "Epoch5:6.25%\n",
      "Epoch6:21.875%\n",
      "Epoch7:15.625%\n",
      "Epoch8:6.25%\n",
      "Epoch9:12.5%\n",
      "Epoch10:3.125%\n",
      "Epoch11:12.5%\n",
      "Epoch12:18.75%\n",
      "Epoch13:3.125%\n",
      "Epoch14:9.375%\n",
      "Epoch15:9.375%\n",
      "Epoch16:9.375%\n",
      "Epoch17:21.875%\n",
      "Epoch18:3.125%\n",
      "Epoch19:9.375%\n",
      "Epoch20:9.375%\n",
      "Epoch21:21.875%\n",
      "Epoch22:12.5%\n",
      "Epoch23:12.5%\n",
      "Epoch24:18.75%\n",
      "Epoch25:9.375%\n",
      "Epoch26:9.375%\n",
      "Epoch27:9.375%\n",
      "Epoch28:6.25%\n",
      "Epoch29:9.375%\n",
      "Epoch30:12.5%\n",
      "Epoch31:9.375%\n",
      "Epoch32:6.25%\n",
      "Epoch33:6.25%\n",
      "Epoch34:12.5%\n",
      "Epoch35:9.375%\n",
      "Epoch36:15.625%\n",
      "Epoch37:6.25%\n",
      "Epoch38:6.25%\n",
      "Epoch39:9.375%\n",
      "Epoch40:12.5%\n",
      "Epoch41:9.375%\n",
      "Epoch42:18.75%\n",
      "Epoch43:12.5%\n",
      "Epoch44:6.25%\n",
      "Epoch45:12.5%\n",
      "Epoch46:12.5%\n",
      "Epoch47:6.25%\n",
      "Epoch48:9.375%\n",
      "Epoch49:18.75%\n",
      "Epoch50:15.625%\n",
      "Epoch51:15.625%\n",
      "Epoch52:21.875%\n",
      "Epoch53:3.125%\n",
      "Epoch54:9.375%\n",
      "Epoch55:6.25%\n",
      "Epoch56:18.75%\n",
      "Epoch57:3.125%\n",
      "Epoch58:6.25%\n",
      "Epoch59:3.125%\n",
      "Epoch60:21.875%\n",
      "Epoch61:6.25%\n",
      "Epoch62:12.5%\n",
      "Epoch63:9.375%\n",
      "Epoch64:9.375%\n",
      "Epoch65:18.75%\n",
      "Epoch66:9.375%\n",
      "Epoch67:21.875%\n",
      "Epoch68:15.625%\n",
      "Epoch69:6.25%\n",
      "Epoch70:9.375%\n",
      "Epoch71:0.0%\n",
      "Epoch72:12.5%\n",
      "Epoch73:3.125%\n",
      "Epoch74:6.25%\n",
      "Epoch75:25.0%\n",
      "Epoch76:18.75%\n",
      "Epoch77:9.375%\n",
      "Epoch78:18.75%\n",
      "Epoch79:6.25%\n",
      "Epoch80:6.25%\n",
      "Epoch81:9.375%\n",
      "Epoch82:18.75%\n",
      "Epoch83:6.25%\n",
      "Epoch84:21.875%\n",
      "Epoch85:3.125%\n",
      "Epoch86:9.375%\n",
      "Epoch87:12.5%\n",
      "Epoch88:9.375%\n",
      "Epoch89:9.375%\n",
      "Epoch90:9.375%\n",
      "Epoch91:18.75%\n",
      "Epoch92:15.625%\n",
      "Epoch93:18.75%\n",
      "Epoch94:9.375%\n",
      "Epoch95:6.25%\n",
      "Epoch96:3.125%\n",
      "Epoch97:15.625%\n",
      "Epoch98:3.125%\n",
      "Epoch99:15.625%\n",
      "Epoch100:15.625%\n",
      "Epoch101:15.625%\n",
      "Epoch102:9.375%\n",
      "Epoch103:12.5%\n",
      "Epoch104:15.625%\n",
      "Epoch105:6.25%\n",
      "Epoch106:9.375%\n",
      "Epoch107:15.625%\n",
      "Epoch108:15.625%\n",
      "Epoch109:18.75%\n",
      "Epoch110:12.5%\n",
      "Epoch111:15.625%\n",
      "Epoch112:9.375%\n",
      "Epoch113:12.5%\n",
      "Epoch114:3.125%\n",
      "Epoch115:9.375%\n",
      "Epoch116:12.5%\n",
      "Epoch117:6.25%\n",
      "Epoch118:15.625%\n",
      "Epoch119:15.625%\n",
      "Epoch120:12.5%\n",
      "Epoch121:6.25%\n",
      "Epoch122:28.125%\n",
      "Epoch123:15.625%\n",
      "Epoch124:6.25%\n",
      "Epoch125:21.875%\n",
      "Epoch126:6.25%\n",
      "Epoch127:3.125%\n",
      "Epoch128:9.375%\n",
      "Epoch129:15.625%\n",
      "Epoch130:18.75%\n",
      "Epoch131:9.375%\n",
      "Epoch132:9.375%\n",
      "Epoch133:25.0%\n",
      "Epoch134:12.5%\n",
      "Epoch135:15.625%\n",
      "Epoch136:9.375%\n",
      "Epoch137:9.375%\n",
      "Epoch138:9.375%\n",
      "Epoch139:9.375%\n",
      "Epoch140:18.75%\n",
      "Epoch141:12.5%\n",
      "Epoch142:12.5%\n",
      "Epoch143:6.25%\n",
      "Epoch144:15.625%\n",
      "Epoch145:18.75%\n",
      "Epoch146:6.25%\n",
      "Epoch147:3.125%\n",
      "Epoch148:25.0%\n",
      "Epoch149:12.5%\n",
      "Epoch150:12.5%\n",
      "Epoch151:18.75%\n",
      "Epoch152:15.625%\n",
      "Epoch153:12.5%\n",
      "Epoch154:25.0%\n",
      "Epoch155:6.25%\n",
      "Epoch156:21.875%\n",
      "Epoch157:25.0%\n",
      "Epoch158:18.75%\n",
      "Epoch159:12.5%\n",
      "Epoch160:9.375%\n",
      "Epoch161:6.25%\n",
      "Epoch162:28.125%\n",
      "Epoch163:21.875%\n",
      "Epoch164:12.5%\n",
      "Epoch165:9.375%\n",
      "Epoch166:3.125%\n",
      "Epoch167:18.75%\n",
      "Epoch168:15.625%\n",
      "Epoch169:28.125%\n",
      "Epoch170:21.875%\n",
      "Epoch171:21.875%\n",
      "Epoch172:6.25%\n",
      "Epoch173:18.75%\n",
      "Epoch174:9.375%\n",
      "Epoch175:18.75%\n",
      "Epoch176:9.375%\n",
      "Epoch177:15.625%\n",
      "Epoch178:9.375%\n",
      "Epoch179:6.25%\n",
      "Epoch180:12.5%\n",
      "Epoch181:25.0%\n",
      "Epoch182:15.625%\n",
      "Epoch183:9.375%\n",
      "Epoch184:18.75%\n",
      "Epoch185:12.5%\n",
      "Epoch186:9.375%\n",
      "Epoch187:3.125%\n",
      "Epoch188:9.375%\n",
      "Epoch189:6.25%\n",
      "Epoch190:9.375%\n",
      "Epoch191:3.125%\n",
      "Epoch192:6.25%\n",
      "Epoch193:12.5%\n",
      "Epoch194:15.625%\n",
      "Epoch195:18.75%\n",
      "Epoch196:9.375%\n",
      "Epoch197:3.125%\n",
      "Epoch198:12.5%\n",
      "Epoch199:9.375%\n",
      "Epoch200:6.25%\n",
      "Epoch201:9.375%\n",
      "Epoch202:18.75%\n",
      "Epoch203:12.5%\n",
      "Epoch204:21.875%\n",
      "Epoch205:18.75%\n",
      "Epoch206:12.5%\n",
      "Epoch207:9.375%\n",
      "Epoch208:40.625%\n",
      "Epoch209:18.75%\n",
      "Epoch210:18.75%\n",
      "Epoch211:15.625%\n",
      "Epoch212:21.875%\n",
      "Epoch213:6.25%\n",
      "Epoch214:15.625%\n",
      "Epoch215:25.0%\n",
      "Epoch216:9.375%\n",
      "Epoch217:12.5%\n",
      "Epoch218:12.5%\n",
      "Epoch219:9.375%\n",
      "Epoch220:9.375%\n",
      "Epoch221:18.75%\n",
      "Epoch222:9.375%\n",
      "Epoch223:3.125%\n",
      "Epoch224:9.375%\n",
      "Epoch225:9.375%\n",
      "Epoch226:6.25%\n",
      "Epoch227:9.375%\n",
      "Epoch228:6.25%\n",
      "Epoch229:31.25%\n",
      "Epoch230:15.625%\n",
      "Epoch231:21.875%\n",
      "Epoch232:15.625%\n",
      "Epoch233:6.25%\n",
      "Epoch234:6.25%\n",
      "Epoch235:31.25%\n",
      "Epoch236:6.25%\n",
      "Epoch237:21.875%\n",
      "Epoch238:18.75%\n",
      "Epoch239:9.375%\n",
      "Epoch240:9.375%\n",
      "Epoch241:12.5%\n",
      "Epoch242:9.375%\n",
      "Epoch243:6.25%\n",
      "Epoch244:9.375%\n",
      "Epoch245:9.375%\n",
      "Epoch246:15.625%\n",
      "Epoch247:9.375%\n",
      "Epoch248:12.5%\n",
      "Epoch249:12.5%\n",
      "Epoch250:12.5%\n",
      "Epoch251:12.5%\n",
      "Epoch252:18.75%\n",
      "Epoch253:21.875%\n",
      "Epoch254:15.625%\n",
      "Epoch255:6.25%\n",
      "Epoch256:25.0%\n",
      "Epoch257:9.375%\n",
      "Epoch258:9.375%\n",
      "Epoch259:12.5%\n",
      "Epoch260:15.625%\n",
      "Epoch261:21.875%\n",
      "Epoch262:12.5%\n",
      "Epoch263:6.25%\n",
      "Epoch264:12.5%\n",
      "Epoch265:6.25%\n",
      "Epoch266:31.25%\n",
      "Epoch267:3.125%\n",
      "Epoch268:15.625%\n",
      "Epoch269:3.125%\n",
      "Epoch270:9.375%\n",
      "Epoch271:12.5%\n",
      "Epoch272:18.75%\n",
      "Epoch273:28.125%\n",
      "Epoch274:18.75%\n",
      "Epoch275:21.875%\n",
      "Epoch276:15.625%\n",
      "Epoch277:9.375%\n",
      "Epoch278:9.375%\n",
      "Epoch279:18.75%\n",
      "Epoch280:12.5%\n",
      "Epoch281:12.5%\n",
      "Epoch282:25.0%\n",
      "Epoch283:18.75%\n",
      "Epoch284:15.625%\n",
      "Epoch285:15.625%\n",
      "Epoch286:12.5%\n",
      "Epoch287:12.5%\n",
      "Epoch288:21.875%\n",
      "Epoch289:6.25%\n",
      "Epoch290:25.0%\n",
      "Epoch291:6.25%\n",
      "Epoch292:6.25%\n",
      "Epoch293:9.375%\n",
      "Epoch294:21.875%\n",
      "Epoch295:15.625%\n",
      "Epoch296:15.625%\n",
      "Epoch297:12.5%\n",
      "Epoch298:6.25%\n",
      "Epoch299:6.25%\n",
      "Epoch300:12.5%\n",
      "Epoch301:15.625%\n",
      "Epoch302:6.25%\n",
      "Epoch303:9.375%\n",
      "Epoch304:9.375%\n",
      "Epoch305:15.625%\n",
      "Epoch306:6.25%\n",
      "Epoch307:18.75%\n",
      "Epoch308:9.375%\n",
      "Epoch309:3.125%\n",
      "Epoch310:12.5%\n",
      "Epoch311:18.75%\n",
      "Epoch312:9.375%\n",
      "Epoch313:15.625%\n",
      "Epoch314:15.625%\n",
      "Epoch315:15.625%\n",
      "Epoch316:9.375%\n",
      "Epoch317:12.5%\n",
      "Epoch318:12.5%\n",
      "Epoch319:18.75%\n",
      "Epoch320:18.75%\n",
      "Epoch321:12.5%\n",
      "Epoch322:15.625%\n",
      "Epoch323:6.25%\n",
      "Epoch324:9.375%\n",
      "Epoch325:9.375%\n",
      "Epoch326:18.75%\n",
      "Epoch327:21.875%\n",
      "Epoch328:12.5%\n",
      "Epoch329:6.25%\n",
      "Epoch330:12.5%\n",
      "Epoch331:21.875%\n",
      "Epoch332:12.5%\n",
      "Epoch333:12.5%\n",
      "Epoch334:12.5%\n",
      "Epoch335:12.5%\n",
      "Epoch336:6.25%\n",
      "Epoch337:6.25%\n",
      "Epoch338:3.125%\n",
      "Epoch339:9.375%\n",
      "Epoch340:15.625%\n",
      "Epoch341:12.5%\n",
      "Epoch342:15.625%\n",
      "Epoch343:15.625%\n",
      "Epoch344:25.0%\n",
      "Epoch345:9.375%\n",
      "Epoch346:15.625%\n",
      "Epoch347:12.5%\n",
      "Epoch348:12.5%\n",
      "Epoch349:9.375%\n",
      "Epoch350:3.125%\n",
      "Epoch351:18.75%\n",
      "Epoch352:6.25%\n",
      "Epoch353:15.625%\n",
      "Epoch354:12.5%\n",
      "Epoch355:6.25%\n",
      "Epoch356:12.5%\n",
      "Epoch357:25.0%\n",
      "Epoch358:9.375%\n",
      "Epoch359:9.375%\n",
      "Epoch360:6.25%\n",
      "Epoch361:12.5%\n",
      "Epoch362:12.5%\n",
      "Epoch363:18.75%\n",
      "Epoch364:3.125%\n",
      "Epoch365:15.625%\n",
      "Epoch366:18.75%\n",
      "Epoch367:6.25%\n",
      "Epoch368:9.375%\n",
      "Epoch369:6.25%\n",
      "Epoch370:9.375%\n",
      "Epoch371:21.875%\n",
      "Epoch372:18.75%\n",
      "Epoch373:12.5%\n",
      "Epoch374:9.375%\n",
      "Epoch375:9.375%\n",
      "Epoch376:6.25%\n",
      "Epoch377:21.875%\n",
      "Epoch378:18.75%\n",
      "Epoch379:18.75%\n",
      "Epoch380:15.625%\n",
      "Epoch381:9.375%\n",
      "Epoch382:31.25%\n",
      "Epoch383:18.75%\n",
      "Epoch384:9.375%\n",
      "Epoch385:15.625%\n",
      "Epoch386:15.625%\n",
      "Epoch387:6.25%\n",
      "Epoch388:6.25%\n",
      "Epoch389:18.75%\n",
      "Epoch390:15.625%\n",
      "Epoch391:9.375%\n",
      "Epoch392:15.625%\n",
      "Epoch393:12.5%\n",
      "Epoch394:18.75%\n",
      "Epoch395:6.25%\n",
      "Epoch396:25.0%\n",
      "Epoch397:21.875%\n",
      "Epoch398:9.375%\n",
      "Epoch399:15.625%\n",
      "Epoch400:6.25%\n",
      "Epoch401:12.5%\n",
      "Epoch402:9.375%\n",
      "Epoch403:12.5%\n",
      "Epoch404:6.25%\n",
      "Epoch405:12.5%\n",
      "Epoch406:3.125%\n",
      "Epoch407:9.375%\n",
      "Epoch408:0.0%\n",
      "Epoch409:9.375%\n",
      "Epoch410:21.875%\n",
      "Epoch411:9.375%\n",
      "Epoch412:6.25%\n",
      "Epoch413:21.875%\n",
      "Epoch414:12.5%\n",
      "Epoch415:15.625%\n",
      "Epoch416:28.125%\n",
      "Epoch417:9.375%\n",
      "Epoch418:15.625%\n",
      "Epoch419:21.875%\n",
      "Epoch420:9.375%\n",
      "Epoch421:3.125%\n",
      "Epoch422:12.5%\n",
      "Epoch423:18.75%\n",
      "Epoch424:15.625%\n",
      "Epoch425:18.75%\n",
      "Epoch426:6.25%\n",
      "Epoch427:9.375%\n",
      "Epoch428:12.5%\n",
      "Epoch429:6.25%\n",
      "Epoch430:15.625%\n",
      "Epoch431:9.375%\n",
      "Epoch432:9.375%\n",
      "Epoch433:15.625%\n",
      "Epoch434:21.875%\n",
      "Epoch435:9.375%\n",
      "Epoch436:12.5%\n",
      "Epoch437:9.375%\n",
      "Epoch438:12.5%\n",
      "Epoch439:3.125%\n",
      "Epoch440:12.5%\n",
      "Epoch441:12.5%\n",
      "Epoch442:9.375%\n",
      "Epoch443:6.25%\n",
      "Epoch444:25.0%\n",
      "Epoch445:12.5%\n",
      "Epoch446:3.125%\n",
      "Epoch447:15.625%\n",
      "Epoch448:15.625%\n",
      "Epoch449:9.375%\n",
      "Epoch450:12.5%\n",
      "Epoch451:3.125%\n",
      "Epoch452:18.75%\n",
      "Epoch453:15.625%\n",
      "Epoch454:15.625%\n",
      "Epoch455:34.375%\n",
      "Epoch456:6.25%\n",
      "Epoch457:18.75%\n",
      "Epoch458:9.375%\n",
      "Epoch459:15.625%\n",
      "Epoch460:12.5%\n",
      "Epoch461:6.25%\n",
      "Epoch462:21.875%\n",
      "Epoch463:6.25%\n",
      "Epoch464:15.625%\n",
      "Epoch465:6.25%\n",
      "Epoch466:9.375%\n",
      "Epoch467:9.375%\n",
      "Epoch468:9.375%\n",
      "Epoch469:25.0%\n",
      "Epoch470:9.375%\n",
      "Epoch471:9.375%\n",
      "Epoch472:12.5%\n",
      "Epoch473:28.125%\n",
      "Epoch474:12.5%\n",
      "Epoch475:12.5%\n",
      "Epoch476:6.25%\n",
      "Epoch477:12.5%\n",
      "Epoch478:12.5%\n",
      "Epoch479:18.75%\n",
      "Epoch480:6.25%\n",
      "Epoch481:9.375%\n",
      "Epoch482:3.125%\n",
      "Epoch483:6.25%\n",
      "Epoch484:21.875%\n",
      "Epoch485:9.375%\n",
      "Epoch486:9.375%\n",
      "Epoch487:12.5%\n",
      "Epoch488:6.25%\n",
      "Epoch489:6.25%\n",
      "Epoch490:3.125%\n",
      "Epoch491:12.5%\n",
      "Epoch492:12.5%\n",
      "Epoch493:15.625%\n",
      "Epoch494:21.875%\n",
      "Epoch495:15.625%\n",
      "Epoch496:12.5%\n",
      "Epoch497:6.25%\n",
      "Epoch498:3.125%\n",
      "Epoch499:6.25%\n",
      "Epoch500:6.25%\n",
      "Epoch501:9.375%\n",
      "Epoch502:9.375%\n",
      "Epoch503:3.125%\n",
      "Epoch504:12.5%\n",
      "Epoch505:12.5%\n",
      "Epoch506:21.875%\n",
      "Epoch507:12.5%\n",
      "Epoch508:9.375%\n",
      "Epoch509:12.5%\n",
      "Epoch510:9.375%\n",
      "Epoch511:21.875%\n",
      "Epoch512:12.5%\n",
      "Epoch513:9.375%\n",
      "Epoch514:15.625%\n",
      "Epoch515:6.25%\n",
      "Epoch516:9.375%\n",
      "Epoch517:9.375%\n",
      "Epoch518:28.125%\n",
      "Epoch519:15.625%\n",
      "Epoch520:9.375%\n",
      "Epoch521:21.875%\n",
      "Epoch522:12.5%\n",
      "Epoch523:6.25%\n",
      "Epoch524:3.125%\n",
      "Epoch525:28.125%\n",
      "Epoch526:18.75%\n",
      "Epoch527:15.625%\n",
      "Epoch528:6.25%\n",
      "Epoch529:21.875%\n",
      "Epoch530:12.5%\n",
      "Epoch531:12.5%\n",
      "Epoch532:0.0%\n",
      "Epoch533:9.375%\n",
      "Epoch534:12.5%\n",
      "Epoch535:12.5%\n",
      "Epoch536:12.5%\n",
      "Epoch537:12.5%\n",
      "Epoch538:18.75%\n",
      "Epoch539:6.25%\n",
      "Epoch540:15.625%\n",
      "Epoch541:9.375%\n",
      "Epoch542:9.375%\n",
      "Epoch543:9.375%\n",
      "Epoch544:18.75%\n",
      "Epoch545:9.375%\n",
      "Epoch546:21.875%\n",
      "Epoch547:6.25%\n",
      "Epoch548:15.625%\n",
      "Epoch549:9.375%\n",
      "Epoch550:3.125%\n",
      "Epoch551:12.5%\n",
      "Epoch552:12.5%\n",
      "Epoch553:12.5%\n",
      "Epoch554:9.375%\n",
      "Epoch555:15.625%\n",
      "Epoch556:12.5%\n",
      "Epoch557:9.375%\n",
      "Epoch558:15.625%\n",
      "Epoch559:12.5%\n",
      "Epoch560:15.625%\n",
      "Epoch561:12.5%\n",
      "Epoch562:9.375%\n",
      "Epoch563:9.375%\n",
      "Epoch564:15.625%\n",
      "Epoch565:3.125%\n",
      "Epoch566:18.75%\n",
      "Epoch567:28.125%\n",
      "Epoch568:6.25%\n",
      "Epoch569:15.625%\n",
      "Epoch570:0.0%\n",
      "Epoch571:9.375%\n",
      "Epoch572:12.5%\n",
      "Epoch573:12.5%\n",
      "Epoch574:9.375%\n",
      "Epoch575:15.625%\n",
      "Epoch576:9.375%\n",
      "Epoch577:9.375%\n",
      "Epoch578:9.375%\n",
      "Epoch579:12.5%\n",
      "Epoch580:28.125%\n",
      "Epoch581:12.5%\n",
      "Epoch582:9.375%\n",
      "Epoch583:6.25%\n",
      "Epoch584:12.5%\n",
      "Epoch585:6.25%\n",
      "Epoch586:9.375%\n",
      "Epoch587:9.375%\n",
      "Epoch588:12.5%\n",
      "Epoch589:3.125%\n",
      "Epoch590:18.75%\n",
      "Epoch591:18.75%\n",
      "Epoch592:21.875%\n",
      "Epoch593:6.25%\n",
      "Epoch594:0.0%\n",
      "Epoch595:9.375%\n",
      "Epoch596:15.625%\n",
      "Epoch597:18.75%\n",
      "Epoch598:3.125%\n",
      "Epoch599:12.5%\n",
      "Epoch600:15.625%\n",
      "Epoch601:15.625%\n",
      "Epoch602:6.25%\n",
      "Epoch603:9.375%\n",
      "Epoch604:9.375%\n",
      "Epoch605:15.625%\n",
      "Epoch606:12.5%\n",
      "Epoch607:15.625%\n",
      "Epoch608:18.75%\n",
      "Epoch609:3.125%\n",
      "Epoch610:12.5%\n",
      "Epoch611:18.75%\n",
      "Epoch612:9.375%\n",
      "Epoch613:25.0%\n",
      "Epoch614:12.5%\n",
      "Epoch615:9.375%\n",
      "Epoch616:6.25%\n",
      "Epoch617:21.875%\n",
      "Epoch618:12.5%\n",
      "Epoch619:12.5%\n",
      "Epoch620:12.5%\n",
      "Epoch621:9.375%\n",
      "Epoch622:15.625%\n",
      "Epoch623:9.375%\n",
      "Epoch624:15.625%\n",
      "Epoch625:12.5%\n",
      "Epoch626:9.375%\n",
      "Epoch627:6.25%\n",
      "Epoch628:9.375%\n",
      "Epoch629:6.25%\n",
      "Epoch630:3.125%\n",
      "Epoch631:15.625%\n",
      "Epoch632:15.625%\n",
      "Epoch633:6.25%\n",
      "Epoch634:12.5%\n",
      "Epoch635:9.375%\n",
      "Epoch636:9.375%\n",
      "Epoch637:28.125%\n",
      "Epoch638:9.375%\n",
      "Epoch639:21.875%\n",
      "Epoch640:3.125%\n",
      "Epoch641:18.75%\n",
      "Epoch642:9.375%\n",
      "Epoch643:15.625%\n",
      "Epoch644:6.25%\n",
      "Epoch645:9.375%\n",
      "Epoch646:15.625%\n",
      "Epoch647:15.625%\n",
      "Epoch648:9.375%\n",
      "Epoch649:6.25%\n",
      "Epoch650:9.375%\n",
      "Epoch651:12.5%\n",
      "Epoch652:9.375%\n",
      "Epoch653:15.625%\n",
      "Epoch654:12.5%\n",
      "Epoch655:12.5%\n",
      "Epoch656:9.375%\n",
      "Epoch657:15.625%\n",
      "Epoch658:6.25%\n",
      "Epoch659:25.0%\n",
      "Epoch660:18.75%\n",
      "Epoch661:12.5%\n",
      "Epoch662:6.25%\n",
      "Epoch663:18.75%\n",
      "Epoch664:12.5%\n",
      "Epoch665:15.625%\n",
      "Epoch666:12.5%\n",
      "Epoch667:9.375%\n",
      "Epoch668:9.375%\n",
      "Epoch669:15.625%\n",
      "Epoch670:12.5%\n",
      "Epoch671:18.75%\n",
      "Epoch672:6.25%\n",
      "Epoch673:6.25%\n",
      "Epoch674:12.5%\n",
      "Epoch675:9.375%\n",
      "Epoch676:25.0%\n",
      "Epoch677:15.625%\n",
      "Epoch678:25.0%\n",
      "Epoch679:12.5%\n",
      "Epoch680:15.625%\n",
      "Epoch681:9.375%\n",
      "Epoch682:6.25%\n",
      "Epoch683:9.375%\n",
      "Epoch684:9.375%\n",
      "Epoch685:15.625%\n",
      "Epoch686:25.0%\n",
      "Epoch687:12.5%\n",
      "Epoch688:9.375%\n",
      "Epoch689:28.125%\n",
      "Epoch690:12.5%\n",
      "Epoch691:12.5%\n",
      "Epoch692:9.375%\n",
      "Epoch693:9.375%\n",
      "Epoch694:3.125%\n",
      "Epoch695:6.25%\n",
      "Epoch696:15.625%\n",
      "Epoch697:15.625%\n",
      "Epoch698:9.375%\n",
      "Epoch699:9.375%\n",
      "Epoch700:6.25%\n",
      "Epoch701:3.125%\n",
      "Epoch702:18.75%\n",
      "Epoch703:9.375%\n",
      "Epoch704:18.75%\n",
      "Epoch705:9.375%\n",
      "Epoch706:9.375%\n",
      "Epoch707:6.25%\n",
      "Epoch708:9.375%\n",
      "Epoch709:15.625%\n",
      "Epoch710:12.5%\n",
      "Epoch711:3.125%\n",
      "Epoch712:9.375%\n",
      "Epoch713:15.625%\n",
      "Epoch714:25.0%\n",
      "Epoch715:12.5%\n",
      "Epoch716:12.5%\n",
      "Epoch717:15.625%\n",
      "Epoch718:6.25%\n",
      "Epoch719:12.5%\n",
      "Epoch720:12.5%\n",
      "Epoch721:25.0%\n",
      "Epoch722:12.5%\n",
      "Epoch723:12.5%\n",
      "Epoch724:6.25%\n",
      "Epoch725:12.5%\n",
      "Epoch726:18.75%\n",
      "Epoch727:15.625%\n",
      "Epoch728:9.375%\n",
      "Epoch729:9.375%\n",
      "Epoch730:15.625%\n",
      "Epoch731:3.125%\n",
      "Epoch732:9.375%\n",
      "Epoch733:18.75%\n",
      "Epoch734:21.875%\n",
      "Epoch735:15.625%\n",
      "Epoch736:15.625%\n",
      "Epoch737:12.5%\n",
      "Epoch738:9.375%\n",
      "Epoch739:15.625%\n",
      "Epoch740:9.375%\n",
      "Epoch741:0.0%\n",
      "Epoch742:18.75%\n",
      "Epoch743:9.375%\n",
      "Epoch744:12.5%\n",
      "Epoch745:15.625%\n",
      "Epoch746:0.0%\n",
      "Epoch747:12.5%\n",
      "Epoch748:15.625%\n",
      "Epoch749:21.875%\n",
      "Epoch750:15.625%\n",
      "Epoch751:12.5%\n",
      "Epoch752:18.75%\n",
      "Epoch753:12.5%\n",
      "Epoch754:9.375%\n",
      "Epoch755:12.5%\n",
      "Epoch756:0.0%\n",
      "Epoch757:18.75%\n",
      "Epoch758:18.75%\n",
      "Epoch759:6.25%\n",
      "Epoch760:28.125%\n",
      "Epoch761:12.5%\n",
      "Epoch762:3.125%\n",
      "Epoch763:12.5%\n",
      "Epoch764:6.25%\n",
      "Epoch765:9.375%\n",
      "Epoch766:0.0%\n",
      "Epoch767:6.25%\n",
      "Epoch768:12.5%\n",
      "Epoch769:25.0%\n",
      "Epoch770:9.375%\n",
      "Epoch771:6.25%\n",
      "Epoch772:12.5%\n",
      "Epoch773:12.5%\n",
      "Epoch774:15.625%\n",
      "Epoch775:15.625%\n",
      "Epoch776:15.625%\n",
      "Epoch777:3.125%\n",
      "Epoch778:9.375%\n",
      "Epoch779:18.75%\n",
      "Epoch780:12.5%\n",
      "Epoch781:18.75%\n",
      "Epoch782:6.25%\n",
      "Epoch783:6.25%\n",
      "Epoch784:9.375%\n",
      "Epoch785:18.75%\n",
      "Epoch786:15.625%\n",
      "Epoch787:9.375%\n",
      "Epoch788:9.375%\n",
      "Epoch789:18.75%\n",
      "Epoch790:3.125%\n",
      "Epoch791:9.375%\n",
      "Epoch792:12.5%\n",
      "Epoch793:9.375%\n",
      "Epoch794:12.5%\n",
      "Epoch795:12.5%\n",
      "Epoch796:9.375%\n",
      "Epoch797:34.375%\n",
      "Epoch798:15.625%\n",
      "Epoch799:18.75%\n",
      "Epoch800:18.75%\n",
      "Epoch801:6.25%\n",
      "Epoch802:15.625%\n",
      "Epoch803:9.375%\n",
      "Epoch804:18.75%\n",
      "Epoch805:12.5%\n",
      "Epoch806:18.75%\n",
      "Epoch807:6.25%\n",
      "Epoch808:18.75%\n",
      "Epoch809:15.625%\n",
      "Epoch810:18.75%\n",
      "Epoch811:9.375%\n",
      "Epoch812:3.125%\n",
      "Epoch813:9.375%\n",
      "Epoch814:12.5%\n",
      "Epoch815:9.375%\n",
      "Epoch816:0.0%\n",
      "Epoch817:6.25%\n",
      "Epoch818:9.375%\n",
      "Epoch819:18.75%\n",
      "Epoch820:3.125%\n",
      "Epoch821:18.75%\n",
      "Epoch822:15.625%\n",
      "Epoch823:12.5%\n",
      "Epoch824:9.375%\n",
      "Epoch825:15.625%\n",
      "Epoch826:6.25%\n",
      "Epoch827:12.5%\n",
      "Epoch828:3.125%\n",
      "Epoch829:25.0%\n",
      "Epoch830:18.75%\n",
      "Epoch831:15.625%\n",
      "Epoch832:9.375%\n",
      "Epoch833:6.25%\n",
      "Epoch834:9.375%\n",
      "Epoch835:12.5%\n",
      "Epoch836:6.25%\n",
      "Epoch837:15.625%\n",
      "Epoch838:15.625%\n",
      "Epoch839:3.125%\n",
      "Epoch840:21.875%\n",
      "Epoch841:9.375%\n",
      "Epoch842:9.375%\n",
      "Epoch843:12.5%\n",
      "Epoch844:15.625%\n",
      "Epoch845:18.75%\n",
      "Epoch846:18.75%\n",
      "Epoch847:12.5%\n",
      "Epoch848:21.875%\n",
      "Epoch849:15.625%\n",
      "Epoch850:18.75%\n",
      "Epoch851:12.5%\n",
      "Epoch852:12.5%\n",
      "Epoch853:18.75%\n",
      "Epoch854:12.5%\n",
      "Epoch855:9.375%\n",
      "Epoch856:15.625%\n",
      "Epoch857:15.625%\n",
      "Epoch858:3.125%\n",
      "Epoch859:6.25%\n",
      "Epoch860:18.75%\n",
      "Epoch861:9.375%\n",
      "Epoch862:15.625%\n",
      "Epoch863:12.5%\n",
      "Epoch864:6.25%\n",
      "Epoch865:6.25%\n",
      "Epoch866:6.25%\n",
      "Epoch867:6.25%\n",
      "Epoch868:12.5%\n",
      "Epoch869:18.75%\n",
      "Epoch870:12.5%\n",
      "Epoch871:15.625%\n",
      "Epoch872:18.75%\n",
      "Epoch873:3.125%\n",
      "Epoch874:21.875%\n",
      "Epoch875:12.5%\n",
      "Epoch876:15.625%\n",
      "Epoch877:15.625%\n",
      "Epoch878:6.25%\n",
      "Epoch879:9.375%\n",
      "Epoch880:18.75%\n",
      "Epoch881:15.625%\n",
      "Epoch882:18.75%\n",
      "Epoch883:25.0%\n",
      "Epoch884:15.625%\n",
      "Epoch885:18.75%\n",
      "Epoch886:12.5%\n",
      "Epoch887:3.125%\n",
      "Epoch888:12.5%\n",
      "Epoch889:12.5%\n",
      "Epoch890:3.125%\n",
      "Epoch891:18.75%\n",
      "Epoch892:21.875%\n",
      "Epoch893:15.625%\n",
      "Epoch894:12.5%\n",
      "Epoch895:21.875%\n",
      "Epoch896:9.375%\n",
      "Epoch897:6.25%\n",
      "Epoch898:6.25%\n",
      "Epoch899:18.75%\n",
      "Epoch900:9.375%\n",
      "Epoch901:12.5%\n",
      "Epoch902:9.375%\n",
      "Epoch903:15.625%\n",
      "Epoch904:15.625%\n",
      "Epoch905:9.375%\n",
      "Epoch906:6.25%\n",
      "Epoch907:12.5%\n",
      "Epoch908:12.5%\n",
      "Epoch909:6.25%\n",
      "Epoch910:15.625%\n",
      "Epoch911:6.25%\n",
      "Epoch912:9.375%\n",
      "Epoch913:3.125%\n",
      "Epoch914:15.625%\n",
      "Epoch915:25.0%\n",
      "Epoch916:18.75%\n",
      "Epoch917:6.25%\n",
      "Epoch918:12.5%\n",
      "Epoch919:9.375%\n",
      "Epoch920:12.5%\n",
      "Epoch921:9.375%\n",
      "Epoch922:15.625%\n",
      "Epoch923:18.75%\n",
      "Epoch924:15.625%\n",
      "Epoch925:18.75%\n",
      "Epoch926:15.625%\n",
      "Epoch927:12.5%\n",
      "Epoch928:21.875%\n",
      "Epoch929:9.375%\n",
      "Epoch930:18.75%\n",
      "Epoch931:12.5%\n",
      "Epoch932:12.5%\n",
      "Epoch933:18.75%\n",
      "Epoch934:21.875%\n",
      "Epoch935:6.25%\n",
      "Epoch936:0.0%\n",
      "Epoch937:18.75%\n",
      "Epoch938:9.375%\n",
      "Epoch939:21.875%\n",
      "Epoch940:15.625%\n",
      "Epoch941:18.75%\n",
      "Epoch942:15.625%\n",
      "Epoch943:28.125%\n",
      "Epoch944:15.625%\n",
      "Epoch945:18.75%\n",
      "Epoch946:6.25%\n",
      "Epoch947:18.75%\n",
      "Epoch948:12.5%\n",
      "Epoch949:15.625%\n",
      "Epoch950:9.375%\n",
      "Epoch951:6.25%\n",
      "Epoch952:3.125%\n",
      "Epoch953:15.625%\n",
      "Epoch954:31.25%\n",
      "Epoch955:3.125%\n",
      "Epoch956:6.25%\n",
      "Epoch957:15.625%\n",
      "Epoch958:6.25%\n",
      "Epoch959:9.375%\n",
      "Epoch960:12.5%\n",
      "Epoch961:15.625%\n",
      "Epoch962:21.875%\n",
      "Epoch963:18.75%\n",
      "Epoch964:21.875%\n",
      "Epoch965:6.25%\n",
      "Epoch966:9.375%\n",
      "Epoch967:21.875%\n",
      "Epoch968:18.75%\n",
      "Epoch969:12.5%\n",
      "Epoch970:15.625%\n",
      "Epoch971:18.75%\n",
      "Epoch972:18.75%\n",
      "Epoch973:15.625%\n",
      "Epoch974:21.875%\n",
      "Epoch975:28.125%\n",
      "Epoch976:15.625%\n",
      "Epoch977:6.25%\n",
      "Epoch978:12.5%\n",
      "Epoch979:9.375%\n",
      "Epoch980:15.625%\n",
      "Epoch981:15.625%\n",
      "Epoch982:12.5%\n",
      "Epoch983:9.375%\n",
      "Epoch984:21.875%\n",
      "Epoch985:15.625%\n",
      "Epoch986:9.375%\n",
      "Epoch987:12.5%\n",
      "Epoch988:9.375%\n",
      "Epoch989:12.5%\n",
      "Epoch990:9.375%\n",
      "Epoch991:9.375%\n",
      "Epoch992:3.125%\n",
      "Epoch993:3.125%\n",
      "Epoch994:12.5%\n",
      "Epoch995:6.25%\n",
      "Epoch996:25.0%\n",
      "Epoch997:6.25%\n",
      "Epoch998:25.0%\n",
      "Epoch999:12.5%\n",
      "Epoch1000:15.625%\n",
      "Epoch1001:15.625%\n",
      "Epoch1002:18.75%\n",
      "Epoch1003:9.375%\n",
      "Epoch1004:12.5%\n",
      "Epoch1005:12.5%\n",
      "Epoch1006:21.875%\n",
      "Epoch1007:12.5%\n",
      "Epoch1008:12.5%\n",
      "Epoch1009:15.625%\n",
      "Epoch1010:6.25%\n",
      "Epoch1011:31.25%\n",
      "Epoch1012:15.625%\n",
      "Epoch1013:9.375%\n",
      "Epoch1014:12.5%\n",
      "Epoch1015:9.375%\n",
      "Epoch1016:12.5%\n",
      "Epoch1017:15.625%\n",
      "Epoch1018:21.875%\n",
      "Epoch1019:21.875%\n",
      "Epoch1020:6.25%\n",
      "Epoch1021:15.625%\n",
      "Epoch1022:0.0%\n",
      "Epoch1023:3.125%\n",
      "Epoch1024:9.375%\n",
      "Epoch1025:12.5%\n",
      "Epoch1026:9.375%\n",
      "Epoch1027:6.25%\n",
      "Epoch1028:3.125%\n",
      "Epoch1029:6.25%\n",
      "Epoch1030:6.25%\n",
      "Epoch1031:12.5%\n",
      "Epoch1032:18.75%\n",
      "Epoch1033:6.25%\n",
      "Epoch1034:12.5%\n",
      "Epoch1035:6.25%\n",
      "Epoch1036:25.0%\n",
      "Epoch1037:12.5%\n",
      "Epoch1038:9.375%\n",
      "Epoch1039:9.375%\n",
      "Epoch1040:21.875%\n",
      "Epoch1041:12.5%\n",
      "Epoch1042:9.375%\n",
      "Epoch1043:12.5%\n",
      "Epoch1044:9.375%\n",
      "Epoch1045:12.5%\n",
      "Epoch1046:9.375%\n",
      "Epoch1047:21.875%\n",
      "Epoch1048:15.625%\n",
      "Epoch1049:18.75%\n",
      "Epoch1050:21.875%\n",
      "Epoch1051:12.5%\n",
      "Epoch1052:12.5%\n",
      "Epoch1053:15.625%\n",
      "Epoch1054:9.375%\n",
      "Epoch1055:18.75%\n",
      "Epoch1056:18.75%\n",
      "Epoch1057:6.25%\n",
      "Epoch1058:21.875%\n",
      "Epoch1059:18.75%\n",
      "Epoch1060:6.25%\n",
      "Epoch1061:15.625%\n",
      "Epoch1062:9.375%\n",
      "Epoch1063:9.375%\n",
      "Epoch1064:12.5%\n",
      "Epoch1065:9.375%\n",
      "Epoch1066:25.0%\n",
      "Epoch1067:21.875%\n",
      "Epoch1068:15.625%\n",
      "Epoch1069:18.75%\n",
      "Epoch1070:9.375%\n",
      "Epoch1071:15.625%\n",
      "Epoch1072:6.25%\n",
      "Epoch1073:25.0%\n",
      "Epoch1074:12.5%\n",
      "Epoch1075:6.25%\n",
      "Epoch1076:18.75%\n",
      "Epoch1077:6.25%\n",
      "Epoch1078:28.125%\n",
      "Epoch1079:15.625%\n",
      "Epoch1080:9.375%\n",
      "Epoch1081:28.125%\n",
      "Epoch1082:12.5%\n",
      "Epoch1083:15.625%\n",
      "Epoch1084:12.5%\n",
      "Epoch1085:18.75%\n",
      "Epoch1086:12.5%\n",
      "Epoch1087:21.875%\n",
      "Epoch1088:18.75%\n",
      "Epoch1089:21.875%\n",
      "Epoch1090:9.375%\n",
      "Epoch1091:18.75%\n",
      "Epoch1092:12.5%\n",
      "Epoch1093:18.75%\n",
      "Epoch1094:18.75%\n",
      "Epoch1095:15.625%\n",
      "Epoch1096:9.375%\n",
      "Epoch1097:9.375%\n",
      "Epoch1098:3.125%\n",
      "Epoch1099:12.5%\n",
      "Epoch1100:9.375%\n",
      "Epoch1101:18.75%\n",
      "Epoch1102:12.5%\n",
      "Epoch1103:6.25%\n",
      "Epoch1104:15.625%\n",
      "Epoch1105:12.5%\n",
      "Epoch1106:18.75%\n",
      "Epoch1107:12.5%\n",
      "Epoch1108:18.75%\n",
      "Epoch1109:25.0%\n",
      "Epoch1110:9.375%\n",
      "Epoch1111:15.625%\n",
      "Epoch1112:9.375%\n",
      "Epoch1113:6.25%\n",
      "Epoch1114:15.625%\n",
      "Epoch1115:3.125%\n",
      "Epoch1116:15.625%\n",
      "Epoch1117:15.625%\n",
      "Epoch1118:12.5%\n",
      "Epoch1119:6.25%\n",
      "Epoch1120:9.375%\n",
      "Epoch1121:9.375%\n",
      "Epoch1122:9.375%\n",
      "Epoch1123:18.75%\n",
      "Epoch1124:0.0%\n",
      "Epoch1125:18.75%\n",
      "Epoch1126:12.5%\n",
      "Epoch1127:21.875%\n",
      "Epoch1128:3.125%\n",
      "Epoch1129:9.375%\n",
      "Epoch1130:21.875%\n",
      "Epoch1131:12.5%\n",
      "Epoch1132:21.875%\n",
      "Epoch1133:12.5%\n",
      "Epoch1134:18.75%\n",
      "Epoch1135:12.5%\n",
      "Epoch1136:28.125%\n",
      "Epoch1137:21.875%\n",
      "Epoch1138:9.375%\n",
      "Epoch1139:9.375%\n",
      "Epoch1140:18.75%\n",
      "Epoch1141:9.375%\n",
      "Epoch1142:9.375%\n",
      "Epoch1143:9.375%\n",
      "Epoch1144:9.375%\n",
      "Epoch1145:9.375%\n",
      "Epoch1146:15.625%\n",
      "Epoch1147:12.5%\n",
      "Epoch1148:12.5%\n",
      "Epoch1149:15.625%\n",
      "Epoch1150:12.5%\n",
      "Epoch1151:3.125%\n",
      "Epoch1152:12.5%\n",
      "Epoch1153:9.375%\n",
      "Epoch1154:9.375%\n",
      "Epoch1155:21.875%\n",
      "Epoch1156:12.5%\n",
      "Epoch1157:6.25%\n",
      "Epoch1158:9.375%\n",
      "Epoch1159:9.375%\n",
      "Epoch1160:15.625%\n",
      "Epoch1161:12.5%\n",
      "Epoch1162:15.625%\n",
      "Epoch1163:6.25%\n",
      "Epoch1164:12.5%\n",
      "Epoch1165:6.25%\n",
      "Epoch1166:9.375%\n",
      "Epoch1167:21.875%\n",
      "Epoch1168:12.5%\n",
      "Epoch1169:15.625%\n",
      "Epoch1170:18.75%\n",
      "Epoch1171:9.375%\n",
      "Epoch1172:12.5%\n",
      "Epoch1173:12.5%\n",
      "Epoch1174:18.75%\n",
      "Epoch1175:3.125%\n",
      "Epoch1176:12.5%\n",
      "Epoch1177:12.5%\n",
      "Epoch1178:12.5%\n",
      "Epoch1179:18.75%\n",
      "Epoch1180:12.5%\n",
      "Epoch1181:12.5%\n",
      "Epoch1182:18.75%\n",
      "Epoch1183:12.5%\n",
      "Epoch1184:6.25%\n",
      "Epoch1185:9.375%\n",
      "Epoch1186:12.5%\n",
      "Epoch1187:28.125%\n",
      "Epoch1188:21.875%\n",
      "Epoch1189:3.125%\n",
      "Epoch1190:12.5%\n",
      "Epoch1191:12.5%\n",
      "Epoch1192:15.625%\n",
      "Epoch1193:9.375%\n",
      "Epoch1194:3.125%\n",
      "Epoch1195:25.0%\n",
      "Epoch1196:15.625%\n",
      "Epoch1197:3.125%\n",
      "Epoch1198:12.5%\n",
      "Epoch1199:15.625%\n",
      "Epoch1200:21.875%\n",
      "Epoch1201:15.625%\n",
      "Epoch1202:15.625%\n",
      "Epoch1203:21.875%\n",
      "Epoch1204:12.5%\n",
      "Epoch1205:9.375%\n",
      "Epoch1206:12.5%\n",
      "Epoch1207:18.75%\n",
      "Epoch1208:15.625%\n",
      "Epoch1209:21.875%\n",
      "Epoch1210:9.375%\n",
      "Epoch1211:18.75%\n",
      "Epoch1212:9.375%\n",
      "Epoch1213:9.375%\n",
      "Epoch1214:21.875%\n",
      "Epoch1215:6.25%\n",
      "Epoch1216:15.625%\n",
      "Epoch1217:9.375%\n",
      "Epoch1218:15.625%\n",
      "Epoch1219:12.5%\n",
      "Epoch1220:15.625%\n",
      "Epoch1221:15.625%\n",
      "Epoch1222:18.75%\n",
      "Epoch1223:15.625%\n",
      "Epoch1224:21.875%\n",
      "Epoch1225:6.25%\n",
      "Epoch1226:6.25%\n",
      "Epoch1227:6.25%\n",
      "Epoch1228:3.125%\n",
      "Epoch1229:25.0%\n",
      "Epoch1230:6.25%\n",
      "Epoch1231:12.5%\n",
      "Epoch1232:6.25%\n",
      "Epoch1233:15.625%\n",
      "Epoch1234:25.0%\n",
      "Epoch1235:15.625%\n",
      "Epoch1236:12.5%\n",
      "Epoch1237:0.0%\n",
      "Epoch1238:6.25%\n",
      "Epoch1239:6.25%\n",
      "Epoch1240:15.625%\n",
      "Epoch1241:9.375%\n",
      "Epoch1242:9.375%\n",
      "Epoch1243:15.625%\n",
      "Epoch1244:3.125%\n",
      "Epoch1245:9.375%\n",
      "Epoch1246:15.625%\n",
      "Epoch1247:6.25%\n",
      "Epoch1248:6.25%\n",
      "Epoch1249:25.0%\n",
      "Epoch1250:12.5%\n",
      "Epoch1251:9.375%\n",
      "Epoch1252:9.375%\n",
      "Epoch1253:3.125%\n",
      "Epoch1254:18.75%\n",
      "Epoch1255:18.75%\n",
      "Epoch1256:6.25%\n",
      "Epoch1257:18.75%\n",
      "Epoch1258:12.5%\n",
      "Epoch1259:15.625%\n",
      "Epoch1260:15.625%\n",
      "Epoch1261:9.375%\n",
      "Epoch1262:12.5%\n",
      "Epoch1263:18.75%\n",
      "Epoch1264:18.75%\n",
      "Epoch1265:12.5%\n",
      "Epoch1266:12.5%\n",
      "Epoch1267:9.375%\n",
      "Epoch1268:15.625%\n",
      "Epoch1269:6.25%\n",
      "Epoch1270:3.125%\n",
      "Epoch1271:6.25%\n",
      "Epoch1272:12.5%\n",
      "Epoch1273:6.25%\n",
      "Epoch1274:9.375%\n",
      "Epoch1275:6.25%\n",
      "Epoch1276:9.375%\n",
      "Epoch1277:15.625%\n",
      "Epoch1278:9.375%\n",
      "Epoch1279:15.625%\n",
      "Epoch1280:9.375%\n",
      "Epoch1281:18.75%\n",
      "Epoch1282:15.625%\n",
      "Epoch1283:12.5%\n",
      "Epoch1284:18.75%\n",
      "Epoch1285:21.875%\n",
      "Epoch1286:9.375%\n",
      "Epoch1287:15.625%\n",
      "Epoch1288:15.625%\n",
      "Epoch1289:12.5%\n",
      "Epoch1290:6.25%\n",
      "Epoch1291:9.375%\n",
      "Epoch1292:21.875%\n",
      "Epoch1293:21.875%\n",
      "Epoch1294:6.25%\n",
      "Epoch1295:28.125%\n",
      "Epoch1296:12.5%\n",
      "Epoch1297:3.125%\n",
      "Epoch1298:6.25%\n",
      "Epoch1299:6.25%\n",
      "Epoch1300:3.125%\n",
      "Epoch1301:9.375%\n",
      "Epoch1302:15.625%\n",
      "Epoch1303:15.625%\n",
      "Epoch1304:21.875%\n",
      "Epoch1305:18.75%\n",
      "Epoch1306:15.625%\n",
      "Epoch1307:9.375%\n",
      "Epoch1308:21.875%\n",
      "Epoch1309:3.125%\n",
      "Epoch1310:6.25%\n",
      "Epoch1311:18.75%\n",
      "Epoch1312:12.5%\n",
      "Epoch1313:15.625%\n",
      "Epoch1314:9.375%\n",
      "Epoch1315:9.375%\n",
      "Epoch1316:12.5%\n",
      "Epoch1317:9.375%\n",
      "Epoch1318:18.75%\n",
      "Epoch1319:18.75%\n",
      "Epoch1320:12.5%\n",
      "Epoch1321:15.625%\n",
      "Epoch1322:12.5%\n",
      "Epoch1323:9.375%\n",
      "Epoch1324:25.0%\n",
      "Epoch1325:9.375%\n",
      "Epoch1326:9.375%\n",
      "Epoch1327:12.5%\n",
      "Epoch1328:21.875%\n",
      "Epoch1329:25.0%\n",
      "Epoch1330:6.25%\n",
      "Epoch1331:21.875%\n",
      "Epoch1332:12.5%\n",
      "Epoch1333:15.625%\n",
      "Epoch1334:0.0%\n",
      "Epoch1335:9.375%\n",
      "Epoch1336:12.5%\n",
      "Epoch1337:21.875%\n",
      "Epoch1338:9.375%\n",
      "Epoch1339:6.25%\n",
      "Epoch1340:9.375%\n",
      "Epoch1341:15.625%\n",
      "Epoch1342:12.5%\n",
      "Epoch1343:25.0%\n",
      "Epoch1344:12.5%\n",
      "Epoch1345:15.625%\n",
      "Epoch1346:9.375%\n",
      "Epoch1347:6.25%\n",
      "Epoch1348:25.0%\n",
      "Epoch1349:6.25%\n",
      "Epoch1350:15.625%\n",
      "Epoch1351:12.5%\n",
      "Epoch1352:25.0%\n",
      "Epoch1353:0.0%\n",
      "Epoch1354:3.125%\n",
      "Epoch1355:9.375%\n",
      "Epoch1356:25.0%\n",
      "Epoch1357:9.375%\n",
      "Epoch1358:9.375%\n",
      "Epoch1359:15.625%\n",
      "Epoch1360:6.25%\n",
      "Epoch1361:9.375%\n",
      "Epoch1362:6.25%\n",
      "Epoch1363:12.5%\n",
      "Epoch1364:15.625%\n",
      "Epoch1365:12.5%\n",
      "Epoch1366:3.125%\n",
      "Epoch1367:9.375%\n",
      "Epoch1368:6.25%\n",
      "Epoch1369:9.375%\n",
      "Epoch1370:9.375%\n",
      "Epoch1371:9.375%\n",
      "Epoch1372:12.5%\n",
      "Epoch1373:6.25%\n",
      "Epoch1374:28.125%\n",
      "Epoch1375:15.625%\n",
      "Epoch1376:12.5%\n",
      "Epoch1377:9.375%\n",
      "Epoch1378:18.75%\n",
      "Epoch1379:3.125%\n",
      "Epoch1380:12.5%\n",
      "Epoch1381:9.375%\n",
      "Epoch1382:6.25%\n",
      "Epoch1383:12.5%\n",
      "Epoch1384:15.625%\n",
      "Epoch1385:6.25%\n",
      "Epoch1386:6.25%\n",
      "Epoch1387:12.5%\n",
      "Epoch1388:3.125%\n",
      "Epoch1389:15.625%\n",
      "Epoch1390:18.75%\n",
      "Epoch1391:12.5%\n",
      "Epoch1392:12.5%\n",
      "Epoch1393:6.25%\n",
      "Epoch1394:3.125%\n",
      "Epoch1395:6.25%\n",
      "Epoch1396:9.375%\n",
      "Epoch1397:12.5%\n",
      "Epoch1398:15.625%\n",
      "Epoch1399:21.875%\n",
      "Epoch1400:18.75%\n",
      "Epoch1401:18.75%\n",
      "Epoch1402:9.375%\n",
      "Epoch1403:12.5%\n",
      "Epoch1404:9.375%\n",
      "Epoch1405:12.5%\n",
      "Epoch1406:15.625%\n",
      "Epoch1407:12.5%\n",
      "Epoch1408:21.875%\n",
      "Epoch1409:9.375%\n",
      "Epoch1410:12.5%\n",
      "Epoch1411:25.0%\n",
      "Epoch1412:9.375%\n",
      "Epoch1413:15.625%\n",
      "Epoch1414:15.625%\n",
      "Epoch1415:6.25%\n",
      "Epoch1416:12.5%\n",
      "Epoch1417:12.5%\n",
      "Epoch1418:21.875%\n",
      "Epoch1419:12.5%\n",
      "Epoch1420:12.5%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[105], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m MLP(M\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m, num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m      2\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m GradientDescent(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m.004\u001b[39m, max_iters\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m y_pred, train_accs, batch_train_accs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[100], line 32\u001b[0m, in \u001b[0;36mMLP.fit\u001b[1;34m(self, x, y, optimizer, x_valid, y_valid)\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams, train_accs, batch_train_accs \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mrun_mini_batch(gradient, \u001b[38;5;28mself\u001b[39m, x, y, params0, aggr_params, x_valid, y_valid)\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 32\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams, train_accs, batch_train_accs \u001b[38;5;241m=\u001b[39m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_mini_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams0\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m, train_accs, batch_train_accs\n",
      "Cell \u001b[1;32mIn[25], line 29\u001b[0m, in \u001b[0;36mGradientDescent.run_mini_batch\u001b[1;34m(self, gradient_fn, x, y, params, batch_size)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m np\u001b[38;5;241m.\u001b[39many(norms \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepsilon) \u001b[38;5;129;01mand\u001b[39;00m t \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_iters \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(mini_batches):\n\u001b[0;32m     28\u001b[0m     x_temp, y_temp \u001b[38;5;241m=\u001b[39m mini_batches[t \u001b[38;5;241m%\u001b[39m ( \u001b[38;5;28mlen\u001b[39m(mini_batches)\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m ) ][\u001b[38;5;241m0\u001b[39m], mini_batches[t \u001b[38;5;241m%\u001b[39m ( \u001b[38;5;28mlen\u001b[39m(mini_batches)\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m ) ][\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m---> 29\u001b[0m     grad, temp_acc \u001b[38;5;241m=\u001b[39m \u001b[43mgradient_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_temp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_temp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(params)):\n\u001b[0;32m     31\u001b[0m         params[p] \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlearning_rate \u001b[38;5;241m*\u001b[39m grad[p]\n",
      "Cell \u001b[1;32mIn[100], line 19\u001b[0m, in \u001b[0;36mMLP.fit.<locals>.gradient\u001b[1;34m(x, y, params)\u001b[0m\n\u001b[0;32m     17\u001b[0m dy \u001b[38;5;241m=\u001b[39m yh \u001b[38;5;241m-\u001b[39m y \u001b[38;5;66;03m#N\u001b[39;00m\n\u001b[0;32m     18\u001b[0m train_acc \u001b[38;5;241m=\u001b[39m evaluate_acc(yh, y)\n\u001b[1;32m---> 19\u001b[0m dw \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdy\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m/\u001b[39mN \u001b[38;5;66;03m#M\u001b[39;00m\n\u001b[0;32m     20\u001b[0m dparams \u001b[38;5;241m=\u001b[39m [dw]\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dparams ,train_acc\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = MLP(M=128, num_classes=10)\n",
    "optimizer = GradientDescent(learning_rate=.004, max_iters=2000, batch_size=64)\n",
    "y_pred, train_accs, batch_train_accs = model.fit(x_train, y_train, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyper_tuning(x_train, y_train):\n",
    "  from sklearn.model_selection import KFold\n",
    "  import pandas as pd\n",
    "  kf = KFold(5)\n",
    "  acc_vals = []\n",
    "  # hidden_units = [64, 128, 256, 512]\n",
    "  # activations = [relu] #,leaky_relu, tanh ]\n",
    "  learning_rate = [0.001, 0.002, 0.004]\n",
    "  batch_size = [16, 32, 64]\n",
    "  for btch in batch_size:\n",
    "    print('batchsize:',btch)\n",
    "    for lr in learning_rate:\n",
    "      print('learningrate:',lr)\n",
    "      optimizer = GradientDescent(learning_rate = lr, batch_size=btch)\n",
    "      # for activ in activations:\n",
    "      # for hu in hidden_units:   \n",
    "      avg_acc = 0;       \n",
    "      # print(f\"for M=128, nonlinearity={activ}, lr={lr}, batch size={btch}.\")\n",
    "      start = time.time()\n",
    "      for k, (train, test) in enumerate(kf.split(x_train, y_train)):\n",
    "          print('k:',k)\n",
    "          temp_model = MLP(M=128)\n",
    "          temp_model.fit(x_train[train], y_train[train], optimizer)\n",
    "          y_test_pred = temp_model.predict(x_train[test])\n",
    "          temp_acc = evaluate_acc(y_test_pred, y_train[test])\n",
    "          avg_acc += temp_acc\n",
    "      avg_acc = avg_acc/5\n",
    "      acc_vals.append(avg_acc)\n",
    "      end = time.time()\n",
    "      print('time elapsed:',(end-start)/60/60,\"hrs\")\n",
    "      print('acc:',avg_acc)\n",
    "      \n",
    "  data = {'learningRate' : [0.001, 0.002, 0.004, 0.001, 0.002, 0.004, 0.001, 0.002, 0.004], \n",
    "          'batchSize':[16, 16, 16, 32, 32, 32, 64, 64, 64],\n",
    "          'accuracies': acc_vals\n",
    "          }\n",
    "  acc = pd.DataFrame(data)\n",
    "  print(acc)\n",
    "  return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rsag():\n",
    "     \n",
    "     \n",
    "\n",
    "        batch_train_acc.append(np.mean(chunk))\n",
    "\n",
    "        print(f\"Epoch {t/(len(mini_batches)*batch_size)}: {batch_train_acc[-1]}%\")\n",
    "\n",
    "        if len(batch_train_acc)>1 and (np.abs(batch_train_acc[-1]-base) < .01):\n",
    "            if stable_cnt>=2:\n",
    "                print('STABLE TRAIN ACCURACY')\n",
    "                break\n",
    "            else: stable_cnt+=1\n",
    "        else: \n",
    "            base = batch_train_acc[-1]\n",
    "            stable_cnt = 0 \n",
    "        \n",
    "        # # Validation\n",
    "        # if x_val is not None:\n",
    "        #     val_acc = []\n",
    "        #     for (x_v, y_v)  in zip(x_val, y_val):\n",
    "        #         y_pred = predict_fn(x_v)\n",
    "        #         val_acc.append(evaluate_acc(y_pred, y_v))\n",
    "        #     v_mean_acc.append(np.mean(val_acc))\n",
    "        #     v_acc.append(val_acc)\n",
    "        #     print(f\"MEAN VAL ACC: {v_mean_acc[-1]}\")\n",
    "\n",
    "\n",
    "\n",
    "        chunk = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyper_tuning_rsag(x_train, \n",
    "                      y_train ,\n",
    "                      x_valid=None,\n",
    "                      y_valid=None):\n",
    "  from sklearn.model_selection import KFold\n",
    "  import pandas as pd\n",
    "  kf = KFold(5)\n",
    "  acc_vals = []\n",
    "  # hidden_units = [64, 128, 256, 512]\n",
    "  # activations = [relu] #,leaky_relu, tanh ]\n",
    "  learning_rate = [0.001, 0.002, 0.004]\n",
    "  alphas = [.9, .75, .7, .5]\n",
    "  betas = [.001, .002, 0.004]\n",
    "#   batch_size = [16, 32, 64]\n",
    "  for alpha in alphas:\n",
    "    print('alpha:',alpha)\n",
    "    for beta in betas:\n",
    "        for lr in learning_rate:\n",
    "            print('--------New Model----------')\n",
    "            print(f\"learning rate: {lr}\\t alpha: {alpha}\\t beta:{beta}\")\n",
    "            optimizer = RSAG(learning_rate = lr, alpha=alpha, beta=beta, batch_size=64)\n",
    "            # for activ in activations:\n",
    "            # for hu in hidden_units:   \n",
    "            avg_acc = 0;       \n",
    "            # print(f\"for M=128, nonlinearity={activ}, lr={lr}, batch size={btch}.\")\n",
    "            start = time.time()\n",
    "            for k, (train, test) in enumerate(kf.split(x_train, y_train)):\n",
    "                print('k:',k)\n",
    "                temp_model = MLP(M=128, rsag=True)\n",
    "                temp_model.fit(x_train[train], y_train[train], optimizer)\n",
    "\n",
    "                temp_model = train(temp_model, optimizer, x_train[train], y_train[train])\n",
    "\n",
    "                y_test_pred = temp_model.predict(x_train[test])\n",
    "                temp_acc = evaluate_acc(y_test_pred, y_train[test])\n",
    "                avg_acc += temp_acc\n",
    "            avg_acc = avg_acc/5\n",
    "            acc_vals.append(avg_acc)\n",
    "            end = time.time()\n",
    "            print('time elapsed:',(end-start)/60/60,\"hrs\")\n",
    "            print('acc:',avg_acc)\n",
    "  data = {'learningRate' : [0.001, 0.002, 0.004, 0.001, 0.002, 0.004, 0.001, 0.002, 0.004], \n",
    "          'batchSize':[16, 16, 16, 32, 32, 32, 64, 64, 64],\n",
    "          'accuracies': acc_vals\n",
    "          }\n",
    "  acc = pd.DataFrame(data)\n",
    "  print(acc)\n",
    "  return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha: 0.9\n",
      "--------New Model----------\n",
      "learning rate: 0.001\t alpha: 0.9\t beta:0.001\n",
      "k: 0\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "600\n",
      "601\n",
      "602\n",
      "603\n",
      "604\n",
      "605\n",
      "606\n",
      "607\n",
      "608\n",
      "609\n",
      "610\n",
      "611\n",
      "612\n",
      "613\n",
      "614\n",
      "615\n",
      "616\n",
      "617\n",
      "618\n",
      "619\n",
      "620\n",
      "621\n",
      "622\n",
      "623\n",
      "624\n",
      "625\n",
      "626\n",
      "627\n",
      "628\n",
      "629\n",
      "630\n",
      "631\n",
      "632\n",
      "633\n",
      "634\n",
      "635\n",
      "636\n",
      "637\n",
      "638\n",
      "639\n",
      "640\n",
      "641\n",
      "642\n",
      "643\n",
      "644\n",
      "645\n",
      "646\n",
      "647\n",
      "648\n",
      "649\n",
      "650\n",
      "651\n",
      "652\n",
      "653\n",
      "654\n",
      "655\n",
      "656\n",
      "657\n",
      "658\n",
      "659\n",
      "660\n",
      "661\n",
      "662\n",
      "663\n",
      "664\n",
      "665\n",
      "666\n",
      "667\n",
      "668\n",
      "669\n",
      "670\n",
      "671\n",
      "672\n",
      "673\n",
      "674\n",
      "675\n",
      "676\n",
      "677\n",
      "678\n",
      "679\n",
      "680\n",
      "681\n",
      "682\n",
      "683\n",
      "684\n",
      "685\n",
      "686\n",
      "687\n",
      "688\n",
      "689\n",
      "690\n",
      "691\n",
      "692\n",
      "693\n",
      "694\n",
      "695\n",
      "696\n",
      "697\n",
      "698\n",
      "699\n",
      "700\n",
      "701\n",
      "702\n",
      "703\n",
      "704\n",
      "705\n",
      "706\n",
      "707\n",
      "708\n",
      "709\n",
      "710\n",
      "711\n",
      "712\n",
      "713\n",
      "714\n",
      "715\n",
      "716\n",
      "717\n",
      "718\n",
      "719\n",
      "720\n",
      "721\n",
      "722\n",
      "723\n",
      "724\n",
      "725\n",
      "726\n",
      "727\n",
      "728\n",
      "729\n",
      "730\n",
      "731\n",
      "732\n",
      "733\n",
      "734\n",
      "735\n",
      "736\n",
      "737\n",
      "738\n",
      "739\n",
      "740\n",
      "741\n",
      "742\n",
      "743\n",
      "744\n",
      "745\n",
      "746\n",
      "747\n",
      "748\n",
      "749\n",
      "750\n",
      "751\n",
      "752\n",
      "753\n",
      "754\n",
      "755\n",
      "756\n",
      "757\n",
      "758\n",
      "759\n",
      "760\n",
      "761\n",
      "762\n",
      "763\n",
      "764\n",
      "765\n",
      "766\n",
      "767\n",
      "768\n",
      "769\n",
      "770\n",
      "771\n",
      "772\n",
      "773\n",
      "774\n",
      "775\n",
      "776\n",
      "777\n",
      "778\n",
      "779\n",
      "780\n",
      "781\n",
      "782\n",
      "783\n",
      "784\n",
      "785\n",
      "786\n",
      "787\n",
      "788\n",
      "789\n",
      "790\n",
      "791\n",
      "792\n",
      "793\n",
      "794\n",
      "795\n",
      "796\n",
      "797\n",
      "798\n",
      "799\n",
      "800\n",
      "801\n",
      "802\n",
      "803\n",
      "804\n",
      "805\n",
      "806\n",
      "807\n",
      "808\n",
      "809\n",
      "810\n",
      "811\n",
      "812\n",
      "813\n",
      "814\n",
      "815\n",
      "816\n",
      "817\n",
      "818\n",
      "819\n",
      "820\n",
      "821\n",
      "822\n",
      "823\n",
      "824\n",
      "825\n",
      "826\n",
      "827\n",
      "828\n",
      "829\n",
      "830\n",
      "831\n",
      "832\n",
      "833\n",
      "834\n",
      "835\n",
      "836\n",
      "837\n",
      "838\n",
      "839\n",
      "840\n",
      "841\n",
      "842\n",
      "843\n",
      "844\n",
      "845\n",
      "846\n",
      "847\n",
      "848\n",
      "849\n",
      "850\n",
      "851\n",
      "852\n",
      "853\n",
      "854\n",
      "855\n",
      "856\n",
      "857\n",
      "858\n",
      "859\n",
      "860\n",
      "861\n",
      "862\n",
      "863\n",
      "864\n",
      "865\n",
      "866\n",
      "867\n",
      "868\n",
      "869\n",
      "870\n",
      "871\n",
      "872\n",
      "873\n",
      "874\n",
      "875\n",
      "876\n",
      "877\n",
      "878\n",
      "879\n",
      "880\n",
      "881\n",
      "882\n",
      "883\n",
      "884\n",
      "885\n",
      "886\n",
      "887\n",
      "888\n",
      "889\n",
      "890\n",
      "891\n",
      "892\n",
      "893\n",
      "894\n",
      "895\n",
      "896\n",
      "897\n",
      "898\n",
      "899\n",
      "900\n",
      "901\n",
      "902\n",
      "903\n",
      "904\n",
      "905\n",
      "906\n",
      "907\n",
      "908\n",
      "909\n",
      "910\n",
      "911\n",
      "912\n",
      "913\n",
      "914\n",
      "915\n",
      "916\n",
      "917\n",
      "918\n",
      "919\n",
      "920\n",
      "921\n",
      "922\n",
      "923\n",
      "924\n",
      "925\n",
      "926\n",
      "927\n",
      "928\n",
      "929\n",
      "930\n",
      "931\n",
      "932\n",
      "933\n",
      "934\n",
      "935\n",
      "936\n",
      "937\n",
      "938\n",
      "939\n",
      "940\n",
      "941\n",
      "942\n",
      "943\n",
      "944\n",
      "945\n",
      "946\n",
      "947\n",
      "948\n",
      "949\n",
      "950\n",
      "951\n",
      "952\n",
      "953\n",
      "954\n",
      "955\n",
      "956\n",
      "957\n",
      "958\n",
      "959\n",
      "960\n",
      "961\n",
      "962\n",
      "963\n",
      "964\n",
      "965\n",
      "966\n",
      "967\n",
      "968\n",
      "969\n",
      "970\n",
      "971\n",
      "972\n",
      "973\n",
      "974\n",
      "975\n",
      "976\n",
      "977\n",
      "978\n",
      "979\n",
      "980\n",
      "981\n",
      "982\n",
      "983\n",
      "984\n",
      "985\n",
      "986\n",
      "987\n",
      "988\n",
      "989\n",
      "990\n",
      "991\n",
      "992\n",
      "993\n",
      "994\n",
      "995\n",
      "996\n",
      "997\n",
      "998\n",
      "999\n",
      "1000\n",
      "1001\n",
      "1002\n",
      "1003\n",
      "1004\n",
      "1005\n",
      "1006\n",
      "1007\n",
      "1008\n",
      "1009\n",
      "1010\n",
      "1011\n",
      "1012\n",
      "1013\n",
      "1014\n",
      "1015\n",
      "1016\n",
      "1017\n",
      "1018\n",
      "1019\n",
      "1020\n",
      "1021\n",
      "1022\n",
      "1023\n",
      "1024\n",
      "1025\n",
      "1026\n",
      "1027\n",
      "1028\n",
      "1029\n",
      "1030\n",
      "1031\n",
      "1032\n",
      "1033\n",
      "1034\n",
      "1035\n",
      "1036\n",
      "1037\n",
      "1038\n",
      "1039\n",
      "1040\n",
      "1041\n",
      "1042\n",
      "1043\n",
      "1044\n",
      "1045\n",
      "1046\n",
      "1047\n",
      "1048\n",
      "1049\n",
      "1050\n",
      "1051\n",
      "1052\n",
      "1053\n",
      "1054\n",
      "1055\n",
      "1056\n",
      "1057\n",
      "1058\n",
      "1059\n",
      "1060\n",
      "1061\n",
      "1062\n",
      "1063\n",
      "1064\n",
      "1065\n",
      "1066\n",
      "1067\n",
      "1068\n",
      "1069\n",
      "1070\n",
      "1071\n",
      "1072\n",
      "1073\n",
      "1074\n",
      "1075\n",
      "1076\n",
      "1077\n",
      "1078\n",
      "1079\n",
      "1080\n",
      "1081\n",
      "1082\n",
      "1083\n",
      "1084\n",
      "1085\n",
      "1086\n",
      "1087\n",
      "1088\n",
      "1089\n",
      "1090\n",
      "1091\n",
      "1092\n",
      "1093\n",
      "1094\n",
      "1095\n",
      "1096\n",
      "1097\n",
      "1098\n",
      "1099\n",
      "1100\n",
      "1101\n",
      "1102\n",
      "1103\n",
      "1104\n",
      "1105\n",
      "1106\n",
      "1107\n",
      "1108\n",
      "1109\n",
      "1110\n",
      "1111\n",
      "1112\n",
      "1113\n",
      "1114\n",
      "1115\n",
      "1116\n",
      "1117\n",
      "1118\n",
      "1119\n",
      "1120\n",
      "1121\n",
      "1122\n",
      "1123\n",
      "1124\n",
      "1125\n",
      "1126\n",
      "1127\n",
      "1128\n",
      "1129\n",
      "1130\n",
      "1131\n",
      "1132\n",
      "1133\n",
      "1134\n",
      "1135\n",
      "1136\n",
      "1137\n",
      "1138\n",
      "1139\n",
      "1140\n",
      "1141\n",
      "1142\n",
      "1143\n",
      "1144\n",
      "1145\n",
      "1146\n",
      "1147\n",
      "1148\n",
      "1149\n",
      "1150\n",
      "1151\n",
      "1152\n",
      "1153\n",
      "1154\n",
      "1155\n",
      "1156\n",
      "1157\n",
      "1158\n",
      "1159\n",
      "1160\n",
      "1161\n",
      "1162\n",
      "1163\n",
      "1164\n",
      "1165\n",
      "1166\n",
      "1167\n",
      "1168\n",
      "1169\n",
      "1170\n",
      "1171\n",
      "1172\n",
      "1173\n",
      "1174\n",
      "1175\n",
      "1176\n",
      "1177\n",
      "1178\n",
      "1179\n",
      "1180\n",
      "1181\n",
      "1182\n",
      "1183\n",
      "1184\n",
      "1185\n",
      "1186\n",
      "1187\n",
      "1188\n",
      "1189\n",
      "1190\n",
      "1191\n",
      "1192\n",
      "1193\n",
      "1194\n",
      "1195\n",
      "1196\n",
      "1197\n",
      "1198\n",
      "1199\n",
      "1200\n",
      "1201\n",
      "1202\n",
      "1203\n",
      "1204\n",
      "1205\n",
      "1206\n",
      "1207\n",
      "1208\n",
      "1209\n",
      "1210\n",
      "1211\n",
      "1212\n",
      "1213\n",
      "1214\n",
      "1215\n",
      "1216\n",
      "1217\n",
      "1218\n",
      "1219\n",
      "1220\n",
      "1221\n",
      "1222\n",
      "1223\n",
      "1224\n",
      "1225\n",
      "1226\n",
      "1227\n",
      "1228\n",
      "1229\n",
      "1230\n",
      "1231\n",
      "1232\n",
      "1233\n",
      "1234\n",
      "1235\n",
      "1236\n",
      "1237\n",
      "1238\n",
      "1239\n",
      "1240\n",
      "1241\n",
      "1242\n",
      "1243\n",
      "1244\n",
      "1245\n",
      "1246\n",
      "1247\n",
      "1248\n",
      "1249\n",
      "1250\n",
      "1251\n",
      "1252\n",
      "1253\n",
      "1254\n",
      "1255\n",
      "1256\n",
      "1257\n",
      "1258\n",
      "1259\n",
      "1260\n",
      "1261\n",
      "1262\n",
      "1263\n",
      "1264\n",
      "1265\n",
      "1266\n",
      "1267\n",
      "1268\n",
      "1269\n",
      "1270\n",
      "1271\n",
      "1272\n",
      "1273\n",
      "1274\n",
      "1275\n",
      "1276\n",
      "1277\n",
      "1278\n",
      "1279\n",
      "1280\n",
      "1281\n",
      "1282\n",
      "1283\n",
      "1284\n",
      "1285\n",
      "1286\n",
      "1287\n",
      "1288\n",
      "1289\n",
      "1290\n",
      "1291\n",
      "1292\n",
      "1293\n",
      "1294\n",
      "1295\n",
      "1296\n",
      "1297\n",
      "1298\n",
      "1299\n",
      "1300\n",
      "1301\n",
      "1302\n",
      "1303\n",
      "1304\n",
      "1305\n",
      "1306\n",
      "1307\n",
      "1308\n",
      "1309\n",
      "1310\n",
      "1311\n",
      "1312\n",
      "1313\n",
      "1314\n",
      "1315\n",
      "1316\n",
      "1317\n",
      "1318\n",
      "1319\n",
      "1320\n",
      "1321\n",
      "1322\n",
      "1323\n",
      "1324\n",
      "1325\n",
      "1326\n",
      "1327\n",
      "1328\n",
      "1329\n",
      "1330\n",
      "1331\n",
      "1332\n",
      "1333\n",
      "1334\n",
      "1335\n",
      "1336\n",
      "1337\n",
      "1338\n",
      "1339\n",
      "1340\n",
      "1341\n",
      "1342\n",
      "1343\n",
      "1344\n",
      "1345\n",
      "1346\n",
      "1347\n",
      "1348\n",
      "1349\n",
      "1350\n",
      "1351\n",
      "1352\n",
      "1353\n",
      "1354\n",
      "1355\n",
      "1356\n",
      "1357\n",
      "1358\n",
      "1359\n",
      "1360\n",
      "1361\n",
      "1362\n",
      "1363\n",
      "1364\n",
      "1365\n",
      "1366\n",
      "1367\n",
      "1368\n",
      "1369\n",
      "1370\n",
      "1371\n",
      "1372\n",
      "1373\n",
      "1374\n",
      "1375\n",
      "1376\n",
      "1377\n",
      "1378\n",
      "1379\n",
      "1380\n",
      "1381\n",
      "1382\n",
      "1383\n",
      "1384\n",
      "1385\n",
      "1386\n",
      "1387\n",
      "1388\n",
      "1389\n",
      "1390\n",
      "1391\n",
      "1392\n",
      "1393\n",
      "1394\n",
      "1395\n",
      "1396\n",
      "1397\n",
      "1398\n",
      "1399\n",
      "1400\n",
      "1401\n",
      "1402\n",
      "1403\n",
      "1404\n",
      "1405\n",
      "1406\n",
      "1407\n",
      "1408\n",
      "1409\n",
      "1410\n",
      "1411\n",
      "1412\n",
      "1413\n",
      "1414\n",
      "1415\n",
      "1416\n",
      "1417\n",
      "1418\n",
      "1419\n",
      "1420\n",
      "1421\n",
      "1422\n",
      "1423\n",
      "1424\n",
      "1425\n",
      "1426\n",
      "1427\n",
      "1428\n",
      "1429\n",
      "1430\n",
      "1431\n",
      "1432\n",
      "1433\n",
      "1434\n",
      "1435\n",
      "1436\n",
      "1437\n",
      "1438\n",
      "1439\n",
      "1440\n",
      "1441\n",
      "1442\n",
      "1443\n",
      "1444\n",
      "1445\n",
      "1446\n",
      "1447\n",
      "1448\n",
      "1449\n",
      "1450\n",
      "1451\n",
      "1452\n",
      "1453\n",
      "1454\n",
      "1455\n",
      "1456\n",
      "1457\n",
      "1458\n",
      "1459\n",
      "1460\n",
      "1461\n",
      "1462\n",
      "1463\n",
      "1464\n",
      "1465\n",
      "1466\n",
      "1467\n",
      "1468\n",
      "1469\n",
      "1470\n",
      "1471\n",
      "1472\n",
      "1473\n",
      "1474\n",
      "1475\n",
      "1476\n",
      "1477\n",
      "1478\n",
      "1479\n",
      "1480\n",
      "1481\n",
      "1482\n",
      "1483\n",
      "1484\n",
      "1485\n",
      "1486\n",
      "1487\n",
      "1488\n",
      "1489\n",
      "1490\n",
      "1491\n",
      "1492\n",
      "1493\n",
      "1494\n",
      "1495\n",
      "1496\n",
      "1497\n",
      "1498\n",
      "1499\n",
      "1500\n",
      "1501\n",
      "1502\n",
      "1503\n",
      "1504\n",
      "1505\n",
      "1506\n",
      "1507\n",
      "1508\n",
      "1509\n",
      "1510\n",
      "1511\n",
      "1512\n",
      "1513\n",
      "1514\n",
      "1515\n",
      "1516\n",
      "1517\n",
      "1518\n",
      "1519\n",
      "1520\n",
      "1521\n",
      "1522\n",
      "1523\n",
      "1524\n",
      "1525\n",
      "1526\n",
      "1527\n",
      "1528\n",
      "1529\n",
      "1530\n",
      "1531\n",
      "1532\n",
      "1533\n",
      "1534\n",
      "1535\n",
      "1536\n",
      "1537\n",
      "1538\n",
      "1539\n",
      "1540\n",
      "1541\n",
      "1542\n",
      "1543\n",
      "1544\n",
      "1545\n",
      "1546\n",
      "1547\n",
      "1548\n",
      "1549\n",
      "1550\n",
      "1551\n",
      "1552\n",
      "1553\n",
      "1554\n",
      "1555\n",
      "1556\n",
      "1557\n",
      "1558\n",
      "1559\n",
      "1560\n",
      "1561\n",
      "1562\n",
      "1563\n",
      "1564\n",
      "1565\n",
      "1566\n",
      "1567\n",
      "1568\n",
      "1569\n",
      "1570\n",
      "1571\n",
      "1572\n",
      "1573\n",
      "1574\n",
      "1575\n",
      "1576\n",
      "1577\n",
      "1578\n",
      "1579\n",
      "1580\n",
      "1581\n",
      "1582\n",
      "1583\n",
      "1584\n",
      "1585\n",
      "1586\n",
      "1587\n",
      "1588\n",
      "1589\n",
      "1590\n",
      "1591\n",
      "1592\n",
      "1593\n",
      "1594\n",
      "1595\n",
      "1596\n",
      "1597\n",
      "1598\n",
      "1599\n",
      "Epoch 1.0: 14.330078125%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hyper_tuning_rsag(x_train=x_train[:2000], y_train=y_train[:2000], x_valid=x_train[2000:2200], y_valid=y_train[2000:2200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
