{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../rsag_convex.png\" alt=\"algoconvex\" />\n",
    "<img src=\"../x_update.png\" alt=\"x_update\" />\n",
    "<img src=\"../mean.png\" alt=\"mean\" />\n",
    "<img src=\"../rsag_composite.png\" alt=\"algo\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Parameters :__\n",
    "- $\\alpha$: (1-$\\alpha$) weight of aggregated x on current state, i.e. momentum\n",
    "- $\\lambda$: learning rate\n",
    "- $\\beta$: change for aggregated x\n",
    "- $p_k$ termination probability\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import time\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import path\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from util import DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "# packaging it all into a function\n",
    "def preprocess_fashion_mnist():\n",
    "  import random as rand\n",
    "\n",
    "\n",
    "  (x_train, y_train), (x_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "  mean_mat = np.mean(x_train, axis=0)\n",
    "\n",
    "  # centering the data by removing the pixel wise mean from every pixel in every image\n",
    "  x_train_centered = x_train - mean_mat\n",
    "  x_test_centered = x_test - mean_mat\n",
    "\n",
    "  # normalizing the grayscale values to values in interval [0,1]\n",
    "  x_train_normalized = x_train_centered/255.0\n",
    "  x_test_normalized = x_test_centered/255.0\n",
    "\n",
    "  #finally, flattening the data\n",
    "  x_train = np.reshape(x_train_normalized, (60000,784))\n",
    "  x_test = np.reshape(x_test_normalized, (10000, 784))\n",
    "  \n",
    "  #converting the test data to one hot encodings\n",
    "  y_train = keras.utils.to_categorical(y_train, num_classes=10)\n",
    "  y_test = keras.utils.to_categorical(y_test, num_classes=10)\n",
    "\n",
    "  return x_train, y_train, x_test, y_test\n",
    "x_train, y_train, x_test, y_test = preprocess_fashion_mnist()\n",
    "\n",
    "x_train, x_valid, y_train, y_test = train_test_split(x_train, y_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Implementation - Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_acc(pred, truth):\n",
    "  counter =0\n",
    "\n",
    "  for i in range(len(pred)):\n",
    "    maxVal = np.where(pred[i] == np.amax(pred[i]))\n",
    "    counter += 1 if maxVal == np.where(truth[i]==1) else 0\n",
    "  return counter * 100.0 / float(len(pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#activation functions\n",
    "softmax1D = lambda z: np.exp(z)/float(sum(np.exp(z)))\n",
    "softmax2D = lambda z: np.array([np.exp(i)/float(sum(np.exp(i))) for i in z])\n",
    "# relu = lambda y: y[y <= 0]=0\n",
    "def relu(x):\n",
    "  alpha = 0.1\n",
    "  x=np.array(x).astype(float)\n",
    "  # x[x<=0]=0.1*x\n",
    "  np.putmask(x, x<0, alpha*x)\n",
    "  return x\n",
    "def relu_grad(x):\n",
    "  alpha = 0.1\n",
    "  x=np.array(x).astype(float)\n",
    "  x[x>0]=1\n",
    "  x[x<=0]=alpha\n",
    "  return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn.neural_network.MLPClassifier(hidden_layer_sizes=(100,), activation='relu', solver='adam', alpha=0.0001, batch_size=64, learning_rate='constant', learning_rate_init=0.001, verbose=True)\n",
    "logistic = lambda z: 1./ (1 + np.exp(-z))\n",
    "\n",
    "class MLP:\n",
    "\n",
    "    def __init__(self, M = 128, num_classes = 10, rsag=False):\n",
    "        self.M = M\n",
    "        self.num_classes = num_classes\n",
    "        self.rsag = rsag\n",
    "        self.params = None\n",
    "        self.aggr_params = None\n",
    "\n",
    "    def fit(self, x, y, optimizer):\n",
    "        N,D = x.shape\n",
    "        def gradient(x, y, params):\n",
    "            w = params[0] # v.shape = (D, M), w.shape = (M)\n",
    "            z = np.dot(x, w)\n",
    "            yh = softmax2D(z)#N\n",
    "            dy = yh - y #N\n",
    "            train_acc = evaluate_acc(yh, y)\n",
    "            dw = np.dot(x.T, dy)/N #M\n",
    "            dparams = [dw]\n",
    "            return dparams ,train_acc\n",
    "        \n",
    "        if self.params is None:\n",
    "            initializer = keras.initializers.GlorotNormal()\n",
    "            w = initializer(shape=(D, self.num_classes))\n",
    "            self.params = [w]\n",
    "            if self.rsag:\n",
    "                self.aggr_params = [np.copy(w)]\n",
    "            print('params initialized')\n",
    "\n",
    "        if self.rsag:\n",
    "            self.params, self.aggr_params, train_accs, batch_train_acc = optimizer.mini_batch_step(gradient, x, y, self.params, self.aggr_params)\n",
    "        else:\n",
    "            self.params, train_accs, batch_train_acc = optimizer.mini_batch_step(gradient, x, y, self.params)\n",
    "\n",
    "        return self, train_accs, batch_train_acc\n",
    "\n",
    "    def predict(self, x):\n",
    "        # print('self:',self)\n",
    "        # print('self==None:',self==None)\n",
    "        w = self.params[0]\n",
    "        # print(w.shape)\n",
    "        # z = relu(np.dot(x, w)) #N x M\n",
    "        yh = softmax2D(np.dot(x, w))#N\n",
    "        return yh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini batcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mini_batcher(x, y, mini_batch_size):\n",
    "  zipped = np.hstack( (x, y ) )\n",
    "  np.random.shuffle(zipped)\n",
    "  x_batches, y_batches = [], []\n",
    "  mini_batches = []\n",
    "  batch_num = x.shape[0] // mini_batch_size\n",
    "  for i in range(batch_num):\n",
    "    x_batch = zipped[ i * mini_batch_size : (i+1) * mini_batch_size, :-10]\n",
    "    y_batch = zipped[ i * mini_batch_size : (i+1) * mini_batch_size, -10:]\n",
    "    mini_batches.append( ( x_batch, y_batch) )\n",
    "    # mini_batches.append( ( x_batch, np.argmax(y_batch,axis=1)[:,None] ) )\n",
    "  if x.shape[0] % mini_batch_size != 0:\n",
    "    x_batch = zipped[ batch_num * mini_batch_size :, :-10]\n",
    "    y_batch = zipped[ batch_num * mini_batch_size :, -10:]\n",
    "    # print(\"Length of last mini-batch =\", y_batch.shape[0])\n",
    "    mini_batches.append( ( x_batch, y_batch ) )\n",
    "    # mini_batches.append( ( x_batch, np.argmax(y_batch,axis=1) ) )\n",
    "  # print(mini_batches[0])\n",
    "  # print(\"yShape = \",y.shape)\n",
    "  return mini_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_lamda = lambda lr, t: lr/(1+t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientDescent:\n",
    "\n",
    "    def __init__(self, \n",
    "                 learning_rate=.001, \n",
    "                 max_iters=2e4, \n",
    "                 epsilon=1e-8,\n",
    "                 lr_fn = None,\n",
    "                 batch_size=32):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iters = max_iters\n",
    "        self.epsilon = epsilon\n",
    "        self.lr_fn = lr_fn\n",
    "        self.t = 0\n",
    "\n",
    "    # def run(self, gradient_fn, x, y, params):\n",
    "    #     norms = np.array([np.inf])\n",
    "    #     t = 1\n",
    "    #     while np.any(norms > self.epsilon) and t < self.max_iters:\n",
    "    #         grad = gradient_fn(x, y, params)\n",
    "    #         # print(grad[0].shape)\n",
    "    #         # print(params[0].shape)\n",
    "    #         for p in range(len(params)):\n",
    "    #             params[p] -= self.learning_rate * grad[p]\n",
    "    #         t += 1\n",
    "    #         norms = np.array([np.linalg.norm(g) for g in grad])\n",
    "    #     print(t)\n",
    "    #     return params\n",
    "\n",
    "    def mini_batch_step(self, gradient_fn, x, y, params, batch_size=32):\n",
    "        train_acc, batch_train_acc, chunk = [], [], []\n",
    "        # norms = np.array([np.inf])\n",
    "\n",
    "        mini_batches = mini_batcher(x, y, batch_size)\n",
    "    \n",
    "        if self.lr_fn is not None:\n",
    "            self.learning_rate = self.lr_fn(self.learning_rate, self.t)\n",
    "            print('New learning rate:', self.learning_rate)\n",
    "\n",
    "        for x_temp, y_temp in mini_batches:\n",
    "            # x_temp, y_temp = mini_batches[t % ( len(mini_batches)-1 ) ][0], mini_batches[t % ( len(mini_batches)-1 ) ][1]\n",
    "            grad, temp_acc = gradient_fn(x_temp, y_temp, params)\n",
    "\n",
    "            for p in range(len(params)):\n",
    "                params[p] -= self.learning_rate * grad[p]\n",
    "                \n",
    "            chunk.append(temp_acc)\n",
    "            # print(f\"Epoch{t}:{temp_acc}%\")\n",
    "            train_acc.append( ( self.t, temp_acc ) )\n",
    "            \n",
    "        self.t += 1\n",
    "        return params, train_acc, batch_train_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RSAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RSAG:\n",
    "\n",
    "    def __init__(self, \n",
    "                 learning_rate=.001, \n",
    "                 alpha=0.009, \n",
    "                 beta=.000009, \n",
    "                 max_iters=2e4, \n",
    "                 epsilon=1e-8, \n",
    "                 batch_size=32,\n",
    "                 lr_fn = None\n",
    "                 ):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iters = max_iters\n",
    "        self.epsilon = epsilon\n",
    "        self.alpha = alpha  # momentum\n",
    "        self.beta = beta \n",
    "        self.lr_fn = lr_fn\n",
    "        self.t = 0\n",
    "\n",
    "\n",
    "    # def run(self, gradient_fn, x, y, params, agg_params):\n",
    "    #     norms = np.array([np.inf])\n",
    "    #     t = 1\n",
    "    #     while np.any(norms > self.epsilon) and t < self.max_iters:\n",
    "            \n",
    "\n",
    "    #         proj_params = [(1-self.alpha) * a_p + self.alpha * p for p, a_p in zip(params, agg_params)]\n",
    "    #         grad = gradient_fn(x, y, proj_params)\n",
    "            \n",
    "    #         for p in range(len(params)):\n",
    "    #             agg_params[p] -= self.beta * grad[p]\n",
    "    #             params[p] -= self.learning_rate * grad[p]\n",
    "    #         t += 1\n",
    "    #         norms = np.array([np.linalg.norm(g) for g in grad])\n",
    "    #     print(t)\n",
    "    #     return params, agg_params\n",
    "\n",
    "    def mini_batch_step(self, \n",
    "                       gradient_fn,\n",
    "                       x, \n",
    "                       y,\n",
    "                       params, \n",
    "                       agg_params,\n",
    "                       batch_size=32,\n",
    "                       ):\n",
    "        \n",
    "        train_acc, batch_train_acc, chunk = [], [], []\n",
    "        v_acc, v_mean_acc, v_chunk = [],  [], []\n",
    "        norms = np.array([np.inf])\n",
    "        stable_cnt, base = 0, 0.0\n",
    "\n",
    "        mini_batches = mini_batcher(x, y, batch_size)\n",
    "        grad = None\n",
    "        # while np.any(norms > self.epsilon) and t < self.max_iters * len(mini_batches):\n",
    "        # print('len(mini_batches)', len(mini_batches))\n",
    "        # print('')\n",
    "\n",
    "        if self.lr_fn is not None:\n",
    "            self.learning_rate = self.lr_fn(self.learning_rate, self.t)\n",
    "            print('New learning rate:', self.learning_rate)\n",
    "\n",
    "        for x_temp, y_temp in mini_batches:\n",
    "                # x_temp, y_temp = mini_batches[t %  len(mini_batches) ][0], mini_batches[t % len(mini_batches) ][1]\n",
    "                # print(x_temp.shape)\n",
    "            # x_val, y_val = mini_batches[t %  len(mini_batches)]\n",
    "\n",
    "            proj_params = [(1-self.alpha) * a_p + self.alpha * p for p, a_p in zip(params, agg_params)]\n",
    "\n",
    "            grad, temp_acc = gradient_fn(x_temp, y_temp, proj_params)\n",
    "            # if grad == None: grad = temp_grad \n",
    "            # else:\n",
    "            # for p in range(len(params)):\n",
    "            #     grad[p] += grad[p]\n",
    "                                \n",
    "\n",
    "            # v_chunk.append(evaluate_acc())\n",
    "            chunk.append(temp_acc)\n",
    "            train_acc.append( ( self.t, temp_acc ) )\n",
    "\n",
    "            # if t%batch_size ==0:\n",
    "            # for p in range(len(params)):\n",
    "            #     agg_params[p] -= self.beta * (grad[p]/batch_size)\n",
    "            #     params[p] -= self.learning_rate * (grad[p]/batch_size)\n",
    "            for p in range(len(params)):\n",
    "                agg_params[p] -= self.beta * (grad[p])\n",
    "                params[p] -= self.learning_rate * (grad[p])\n",
    "\n",
    "            \n",
    "        self.t += 1\n",
    "            \n",
    "        return params, agg_params, train_acc, batch_train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params initialized\n",
      "acc: 3.1119791666666665\n",
      "Epoch 0: 3.5416666666666665%\n",
      "Epoch 10: 9.205729166666666%\n",
      "Epoch 20: 20.9765625%\n",
      "Epoch 30: 32.135416666666664%\n",
      "Epoch 40: 40.677083333333336%\n",
      "Epoch 50: 46.6796875%\n",
      "Epoch 60: 51.119791666666664%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[267], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m MLP(M\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m, num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m      2\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m GradientDescent(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m.006\u001b[39m, max_iters\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m y_pred, train_accs, batch_train_accs \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_valid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[255], line 13\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, optimizer, x_train, y_train, x_valid, y_valid, print_every)\u001b[0m\n\u001b[0;32m     11\u001b[0m t\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m t \u001b[38;5;241m<\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mmax_iters:\n\u001b[1;32m---> 13\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;66;03m# print('here')\u001b[39;00m\n\u001b[0;32m     15\u001b[0m     y_test_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(x_valid)\n",
      "Cell \u001b[1;32mIn[262], line 36\u001b[0m, in \u001b[0;36mMLP.fit\u001b[1;34m(self, x, y, optimizer)\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maggr_params, train_accs, batch_train_acc \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mmini_batch_step(gradient, x, y, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maggr_params)\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 36\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams, train_accs, batch_train_acc \u001b[38;5;241m=\u001b[39m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmini_batch_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m, train_accs, batch_train_acc\n",
      "Cell \u001b[1;32mIn[263], line 41\u001b[0m, in \u001b[0;36mGradientDescent.mini_batch_step\u001b[1;34m(self, gradient_fn, x, y, params, batch_size)\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNew learning rate:\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlearning_rate)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x_temp, y_temp \u001b[38;5;129;01min\u001b[39;00m mini_batches:\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;66;03m# x_temp, y_temp = mini_batches[t % ( len(mini_batches)-1 ) ][0], mini_batches[t % ( len(mini_batches)-1 ) ][1]\u001b[39;00m\n\u001b[1;32m---> 41\u001b[0m     grad, temp_acc \u001b[38;5;241m=\u001b[39m \u001b[43mgradient_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_temp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_temp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(params)):\n\u001b[0;32m     44\u001b[0m         params[p] \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlearning_rate \u001b[38;5;241m*\u001b[39m grad[p]\n",
      "Cell \u001b[1;32mIn[262], line 17\u001b[0m, in \u001b[0;36mMLP.fit.<locals>.gradient\u001b[1;34m(x, y, params)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgradient\u001b[39m(x, y, params):\n\u001b[0;32m     16\u001b[0m     w \u001b[38;5;241m=\u001b[39m params[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;66;03m# v.shape = (D, M), w.shape = (M)\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m     z \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m     yh \u001b[38;5;241m=\u001b[39m softmax2D(z)\u001b[38;5;66;03m#N\u001b[39;00m\n\u001b[0;32m     19\u001b[0m     dy \u001b[38;5;241m=\u001b[39m yh \u001b[38;5;241m-\u001b[39m y \u001b[38;5;66;03m#N\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = MLP(M=128, num_classes=10)\n",
    "optimizer = GradientDescent(learning_rate=.006, max_iters=2000, batch_size=64)\n",
    "y_pred, train_accs, batch_train_accs = train_model(model, optimizer, x_train, y_train, x_valid, y_test, print_every=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params initialized\n",
      "acc: 11.692708333333334\n",
      "Epoch 0: 12.161458333333334%\n",
      "Epoch 10: 18.958333333333332%\n",
      "Epoch 20: 28.658854166666668%\n",
      "Epoch 30: 36.875%\n",
      "Epoch 40: 42.8125%\n",
      "Epoch 50: 47.265625%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[265], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m RSAG(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m.006\u001b[39m, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.009\u001b[39m, beta\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.000009\u001b[39m, max_iters\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m)\n\u001b[0;32m      3\u001b[0m x_train, x_valid, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(x_train, y_train, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m y_pred, train_accs, batch_train_accs \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_valid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[255], line 13\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, optimizer, x_train, y_train, x_valid, y_valid, print_every)\u001b[0m\n\u001b[0;32m     11\u001b[0m t\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m t \u001b[38;5;241m<\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mmax_iters:\n\u001b[1;32m---> 13\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;66;03m# print('here')\u001b[39;00m\n\u001b[0;32m     15\u001b[0m     y_test_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(x_valid)\n",
      "Cell \u001b[1;32mIn[262], line 34\u001b[0m, in \u001b[0;36mMLP.fit\u001b[1;34m(self, x, y, optimizer)\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams initialized\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrsag:\n\u001b[1;32m---> 34\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maggr_params, train_accs, batch_train_acc \u001b[38;5;241m=\u001b[39m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmini_batch_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maggr_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams, train_accs, batch_train_acc \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mmini_batch_step(gradient, x, y, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams)\n",
      "Cell \u001b[1;32mIn[253], line 52\u001b[0m, in \u001b[0;36mRSAG.mini_batch_step\u001b[1;34m(self, gradient_fn, x, y, params, agg_params, batch_size)\u001b[0m\n\u001b[0;32m     49\u001b[0m norms \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([np\u001b[38;5;241m.\u001b[39minf])\n\u001b[0;32m     50\u001b[0m stable_cnt, base \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m---> 52\u001b[0m mini_batches \u001b[38;5;241m=\u001b[39m \u001b[43mmini_batcher\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     53\u001b[0m grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# while np.any(norms > self.epsilon) and t < self.max_iters * len(mini_batches):\u001b[39;00m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# print('len(mini_batches)', len(mini_batches))\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# print('')\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[10], line 3\u001b[0m, in \u001b[0;36mmini_batcher\u001b[1;34m(x, y, mini_batch_size)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmini_batcher\u001b[39m(x, y, mini_batch_size):\n\u001b[0;32m      2\u001b[0m   zipped \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mhstack( (x, y ) )\n\u001b[1;32m----> 3\u001b[0m   \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mzipped\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m   x_batches, y_batches \u001b[38;5;241m=\u001b[39m [], []\n\u001b[0;32m      5\u001b[0m   mini_batches \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = MLP(M=128, num_classes=10, rsag=True)\n",
    "optimizer = RSAG(learning_rate=.006, alpha=0.009, beta=0.000009, max_iters=2000, batch_size=64)\n",
    "x_train, x_valid, y_train, y_test = train_test_split(x_train, y_train, test_size=0.2, random_state=42)\n",
    "y_pred, train_accs, batch_train_accs = train_model(model, optimizer, x_train, y_train, x_valid, y_test, print_every=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyper_tuning(x_train, y_train, lr_fn):\n",
    "  from sklearn.model_selection import KFold\n",
    "  import pandas as pd\n",
    "  kf = KFold(5)\n",
    "  acc_vals = []\n",
    "  # hidden_units = [64, 128, 256, 512]\n",
    "  # activations = [relu] #,leaky_relu, tanh ]\n",
    "  learning_rate = [0.001, 0.002, 0.004]\n",
    "  batch_size = [16, 32, 64]\n",
    "  for btch in batch_size:\n",
    "    print('batchsize:',btch)\n",
    "    for lr in learning_rate:\n",
    "      \n",
    "      \n",
    "      print('--------New Model----------')\n",
    "      print(f\"learning rate: {lr}\\t Batch Size:{btch}\")\n",
    "\n",
    "      optimizer = GradientDescent(learning_rate = lr, batch_size=btch, lr_lamda=lr_fn)\n",
    "      \n",
    "      avg_acc = 0;       \n",
    "      # print(f\"for M=128, nonlinearity={activ}, lr={lr}, batch size={btch}.\")\n",
    "      start = time.time()\n",
    "      for k, (train, test) in enumerate(kf.split(x_train, y_train)):\n",
    "          print('k:',k)\n",
    "          temp_model = MLP(M=128)\n",
    "          temp_model, temp_acc = train_model(temp_model, optimizer, x_train[train], y_train[train])\n",
    "          avg_acc += temp_acc\n",
    "      avg_acc = avg_acc/5\n",
    "      acc_vals.append(avg_acc)\n",
    "      end = time.time()\n",
    "      print('time elapsed:',(end-start)/60/60,\"hrs\")\n",
    "      print('acc:',avg_acc)\n",
    "      \n",
    "  data = {'learningRate' : [0.001, 0.002, 0.004, 0.001, 0.002, 0.004, 0.001, 0.002, 0.004], \n",
    "          'batchSize':[16, 16, 16, 32, 32, 32, 64, 64, 64],\n",
    "          'accuracies': acc_vals\n",
    "          }\n",
    "  acc = pd.DataFrame(data)\n",
    "  print(acc)\n",
    "  return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyper_tuning_gd(x_train, y_train, print_every=100, lr_fn=None):\n",
    "  from sklearn.model_selection import KFold\n",
    "  import pandas as pd\n",
    "  kf = KFold(5)\n",
    "  acc_vals = []\n",
    "  # hidden_units = [64, 128, 256, 512]\n",
    "  # activations = [relu] #,leaky_relu, tanh ]\n",
    "  learning_rate = [0.001, 0.002, 0.004]\n",
    "  batch_size = [16, 32, 64]\n",
    "  for btch in batch_size:\n",
    "    print('batchsize:',btch)\n",
    "    for lr in learning_rate:\n",
    "      \n",
    "      \n",
    "      print('--------New Model----------')\n",
    "      print(f\"learning rate: {lr}\\t Batch Size:{btch}\")\n",
    "\n",
    "      optimizer = GradientDescent(learning_rate = lr, batch_size=btch, lr_fn=lr_fn)\n",
    "      \n",
    "      avg_acc = 0;       \n",
    "      # print(f\"for M=128, nonlinearity={activ}, lr={lr}, batch size={btch}.\")\n",
    "      start = time.time()\n",
    "      for k, (train, valid) in enumerate(kf.split(x_train, y_train)):\n",
    "          print('k:',k)\n",
    "          temp_model = MLP(M=128)\n",
    "          temp_model, temp_accs, max_acc = train_model(temp_model, optimizer, x_train[train], y_train[train], x_train[valid], y_train[valid], print_every=print_every)\n",
    "          avg_acc += max_acc\n",
    "      avg_acc = avg_acc/5\n",
    "      acc_vals.append(avg_acc)\n",
    "      end = time.time()\n",
    "      print('time elapsed:',(end-start)/60/60,\"hrs\")\n",
    "      print('acc:',avg_acc)\n",
    "      \n",
    "  data = {'learningRate' : [0.001, 0.002, 0.004, 0.001, 0.002, 0.004, 0.001, 0.002, 0.004], \n",
    "          'batchSize':[16, 16, 16, 32, 32, 32, 64, 64, 64],\n",
    "          'accuracies': acc_vals\n",
    "          }\n",
    "  acc = pd.DataFrame(data)\n",
    "  print(acc)\n",
    "  return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batchsize: 16\n",
      "--------New Model----------\n",
      "learning rate: 0.001\t Batch Size:16\n",
      "k: 0\n",
      "params initialized\n",
      "acc: 9.6\n",
      "Epoch 0: 9.6%\n",
      "Epoch 100: 9.65%\n",
      "Epoch 200: 9.65%\n",
      "Epoch 300: 9.65%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[248], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mhyper_tuning_gd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#x_valid=x_train[2000:2200], y_valid=y_train[2000:2200])\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[246], line 26\u001b[0m, in \u001b[0;36mhyper_tuning_gd\u001b[1;34m(x_train, y_train)\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mk:\u001b[39m\u001b[38;5;124m'\u001b[39m,k)\n\u001b[0;32m     25\u001b[0m     temp_model \u001b[38;5;241m=\u001b[39m MLP(M\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m)\n\u001b[1;32m---> 26\u001b[0m     temp_model, temp_accs, max_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtemp_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43mvalid\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43mvalid\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m     avg_acc \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m max_acc\n\u001b[0;32m     28\u001b[0m avg_acc \u001b[38;5;241m=\u001b[39m avg_acc\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m5\u001b[39m\n",
      "Cell \u001b[1;32mIn[242], line 13\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, optimizer, x_train, y_train, x_valid, y_valid, print_every)\u001b[0m\n\u001b[0;32m     11\u001b[0m t\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m t \u001b[38;5;241m<\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mmax_iters:\n\u001b[1;32m---> 13\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;66;03m# print('here')\u001b[39;00m\n\u001b[0;32m     15\u001b[0m     y_test_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(x_valid)\n",
      "Cell \u001b[1;32mIn[189], line 36\u001b[0m, in \u001b[0;36mMLP.fit\u001b[1;34m(self, x, y, optimizer)\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maggr_params, train_accs, batch_train_acc \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mmini_batch_step(gradient, x, y, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maggr_params)\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 36\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams, train_accs, batch_train_acc \u001b[38;5;241m=\u001b[39m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_mini_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m, train_accs, batch_train_acc\n",
      "Cell \u001b[1;32mIn[244], line 33\u001b[0m, in \u001b[0;36mGradientDescent.run_mini_batch\u001b[1;34m(self, gradient_fn, x, y, params, batch_size)\u001b[0m\n\u001b[0;32m     30\u001b[0m train_acc, batch_train_acc, chunk \u001b[38;5;241m=\u001b[39m [], [], []\n\u001b[0;32m     31\u001b[0m norms \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([np\u001b[38;5;241m.\u001b[39minf])\n\u001b[1;32m---> 33\u001b[0m mini_batches \u001b[38;5;241m=\u001b[39m \u001b[43mmini_batcher\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlearning_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr_fn(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlearning_rate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mt)\n",
      "Cell \u001b[1;32mIn[10], line 3\u001b[0m, in \u001b[0;36mmini_batcher\u001b[1;34m(x, y, mini_batch_size)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmini_batcher\u001b[39m(x, y, mini_batch_size):\n\u001b[0;32m      2\u001b[0m   zipped \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mhstack( (x, y ) )\n\u001b[1;32m----> 3\u001b[0m   \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mzipped\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m   x_batches, y_batches \u001b[38;5;241m=\u001b[39m [], []\n\u001b[0;32m      5\u001b[0m   mini_batches \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hyper_tuning_gd(x_train=x_train[:10000], y_train=y_train[:10000], print_every=10) #x_valid=x_train[2000:2200], y_valid=y_train[2000:2200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyper_tuning_rsag(x_train, \n",
    "                      y_train ,\n",
    "                      x_valid=None,\n",
    "                      y_valid=None):\n",
    "  from sklearn.model_selection import KFold\n",
    "  import pandas as pd\n",
    "  kf = KFold(5)\n",
    "  acc_vals = []\n",
    "  # hidden_units = [64, 128, 256, 512]\n",
    "  # activations = [relu] #,leaky_relu, tanh ]\n",
    "  learning_rate = [0.001, 0.002, 0.004]\n",
    "  alphas = [.9, .75, .7, .5]\n",
    "  betas = [.001, .002, 0.004]\n",
    "#   batch_size = [16, 32, 64]\n",
    "  for alpha in alphas:\n",
    "    print('alpha:',alpha)\n",
    "    for beta in betas:\n",
    "        for lr in learning_rate:\n",
    "            print('--------New Model----------')\n",
    "            print(f\"learning rate: {lr}\\t alpha: {alpha}\\t beta:{beta}\")\n",
    "            optimizer = RSAG(learning_rate = lr, alpha=alpha, beta=beta, batch_size=64)\n",
    "            # for activ in activations:\n",
    "            # for hu in hidden_units:   \n",
    "            avg_acc = 0;       \n",
    "            # print(f\"for M=128, nonlinearity={activ}, lr={lr}, batch size={btch}.\")\n",
    "            start = time.time()\n",
    "            for k, (train, valid) in enumerate(kf.split(x_train, y_train)):\n",
    "                print('k:',k)\n",
    "                temp_model = MLP(M=128, rsag=True)\n",
    "                temp_model, temp_accs, max_acc = train_model(temp_model, optimizer, x_train[train], y_train[train], x_train[valid], y_train[valid])\n",
    "                avg_acc += max_acc\n",
    "            avg_acc = avg_acc/5\n",
    "            acc_vals.append(avg_acc)\n",
    "            end = time.time()\n",
    "            print('time elapsed:',(end-start)/60/60,\"hrs\")\n",
    "            print('acc:',avg_acc)\n",
    "  data = {'learningRate' : [0.001, 0.002, 0.004, 0.001, 0.002, 0.004, 0.001, 0.002, 0.004], \n",
    "          'batchSize':[16, 16, 16, 32, 32, 32, 64, 64, 64],\n",
    "          'accuracies': acc_vals\n",
    "          }\n",
    "  acc = pd.DataFrame(data)\n",
    "  print(acc)\n",
    "  return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, optimizer, x_train, y_train, x_valid, y_valid, print_every=100):\n",
    "    # x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "    model.fit(x_train, y_train, optimizer)\n",
    "\n",
    "    y_test_pred = model.predict(x_valid)\n",
    "    temp_v_acc = evaluate_acc(y_test_pred, y_valid)\n",
    "    v_acc = [temp_v_acc]\n",
    "    print('acc:',temp_v_acc)\n",
    "\n",
    "    t=0\n",
    "    while t < optimizer.max_iters:\n",
    "        model.fit(x_train, y_train, optimizer)\n",
    "        # print('here')\n",
    "        y_test_pred = model.predict(x_valid)\n",
    "        temp_v_acc = evaluate_acc(y_test_pred, y_valid)\n",
    "        v_acc.append(temp_v_acc)\n",
    "        \n",
    "        if t%print_every == 0:\n",
    "            print(f\"Epoch {t}: {v_acc[-1]}%\")\n",
    "        t+=1\n",
    "        # if np.abs(v_acc[-1]-v_acc[-2]) < optimizer.epsilon:\n",
    "            \n",
    "        #     print('hfdsaafda')\n",
    "        #     break\n",
    "    return model, temp_v_acc, max(v_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha: 0.9\n",
      "--------New Model----------\n",
      "learning rate: 0.001\t alpha: 0.9\t beta:0.001\n",
      "k: 0\n",
      "params initialized\n",
      "acc: 13.0\n",
      "Epoch0: 13.0%\n",
      "Epoch100: 30.0%\n",
      "Epoch200: 36.5%\n",
      "Epoch300: 45.0%\n",
      "Epoch400: 48.5%\n",
      "Epoch500: 50.0%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_6360\\834803681.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mhyper_tuning_rsag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2000\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2000\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#, x_valid=x_train[2000:2200], y_valid=y_train[2000:2200])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_6360\\2116129435.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(x_train, y_train, x_valid, y_valid)\u001b[0m\n\u001b[0;32m     26\u001b[0m             \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'k:'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m                 \u001b[0mtemp_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMLP\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mM\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrsag\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m                 \u001b[0mtemp_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtemp_accs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemp_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mvalid\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mvalid\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m                 \u001b[0mavg_acc\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mmax_acc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m             \u001b[0mavg_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mavg_acc\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m             \u001b[0macc_vals\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mavg_acc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_6360\\1026297623.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(model, optimizer, x_train, y_train, x_valid, y_valid, print_every)\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'acc:'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtemp_v_acc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mt\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[0mt\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_iters\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m         \u001b[1;31m# print('here')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0my_test_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_valid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mtemp_v_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluate_acc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_6360\\1555615134.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, x, y, optimizer)\u001b[0m\n\u001b[0;32m     30\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maggr_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'params initialized'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrsag\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maggr_params\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_accs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_train_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmini_batch_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maggr_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_accs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_train_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_mini_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_6360\\3483094965.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, gradient_fn, x, y, params, agg_params, batch_size)\u001b[0m\n\u001b[0;32m     49\u001b[0m                 \u001b[1;31m# x_temp, y_temp = mini_batches[t %  len(mini_batches) ][0], mini_batches[t % len(mini_batches) ][1]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m                 \u001b[1;31m# print(x_temp.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[1;31m# x_val, y_val = mini_batches[t %  len(mini_batches)]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m             \u001b[0mproj_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0ma_p\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malpha\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mp\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma_p\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0magg_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m             \u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtemp_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgradient_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_temp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_temp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproj_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m             \u001b[1;31m# if grad == None: grad = temp_grad\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_6360\\3483094965.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(.0)\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m     def mini_batch_step(self, \n\u001b[0m\u001b[0;32m     54\u001b[0m                        \u001b[0mgradient_fn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m                        \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m                        \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\camgr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 155\u001b[1;33m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\camgr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(y, x)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1510\u001b[0m       \u001b[1;31m# TODO(b/178860388): Figure out why binary_op_wrapper and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1511\u001b[0m       \u001b[1;31m#   r_binary_op_wrapper use different force_same_dtype values.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1512\u001b[0m       \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmaybe_promote_tensors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mforce_same_dtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1513\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\camgr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(x, y, name)\u001b[0m\n\u001b[0;32m   1877\u001b[0m     new_vals = gen_sparse_ops.sparse_dense_cwise_mul(y.indices, y.values,\n\u001b[0;32m   1878\u001b[0m                                                      y.dense_shape, x, name)\n\u001b[0;32m   1879\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0msparse_tensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSparseTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdense_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1880\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1881\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mmultiply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\camgr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\ops\\weak_tensor_ops.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    140\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_auto_dtype_conversion_enabled\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 142\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    143\u001b[0m     \u001b[0mbound_arguments\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m     \u001b[0mbound_arguments\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_defaults\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m     \u001b[0mbound_kwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbound_arguments\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marguments\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\camgr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 155\u001b[1;33m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\camgr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1257\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1258\u001b[0m       \u001b[1;31m# Fallback dispatch system (dispatch v1):\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1259\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1260\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdispatch_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1261\u001b[1;33m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1262\u001b[0m         \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1263\u001b[0m         \u001b[1;31m# TypeError, when given unexpected types.  So we need to catch both.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1264\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop_dispatch_handler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\camgr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(x, y, name)\u001b[0m\n\u001b[0;32m    526\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    527\u001b[0m    \u001b[1;33m*\u001b[0m \u001b[0mInvalidArgumentError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mWhen\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mx\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0my\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mhave\u001b[0m \u001b[0mincompatible\u001b[0m \u001b[0mshapes\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    528\u001b[0m   \"\"\"\n\u001b[0;32m    529\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 530\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\camgr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(x, y, name)\u001b[0m\n\u001b[0;32m   6752\u001b[0m         _ctx, \"Mul\", name, x, y)\n\u001b[0;32m   6753\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6754\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6755\u001b[0m       \u001b[0m_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6756\u001b[1;33m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   6757\u001b[0m       \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6758\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6759\u001b[0m       return mul_eager_fallback(\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hyper_tuning_rsag(x_train=x_train[:2000], y_train=y_train[:2000]) #, x_valid=x_train[2000:2200], y_valid=y_train[2000:2200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
