{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../rsag_convex.png\" alt=\"algoconvex\" />\n",
    "<img src=\"../x_update.png\" alt=\"x_update\" />\n",
    "<img src=\"../mean.png\" alt=\"mean\" />\n",
    "<img src=\"../rsag_composite.png\" alt=\"algo\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Parameters :__\n",
    "- $\\alpha$: (1-$\\alpha$) weight of aggregated x on current state, i.e. momentum\n",
    "- $\\lambda$: learning rate\n",
    "- $\\beta$: change for aggregated x\n",
    "- $p_k$ termination probability\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\camgr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import time\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "  from sklearn.model_selection import KFold\n",
    "  import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import path\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from util import DataLoader, plot_accuracies\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_mnist():\n",
    "  import random as rand\n",
    "\n",
    "\n",
    "  (x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data(path=\"mnist.npz\")\n",
    "  \n",
    "  \n",
    "  assert x_train.shape == (60000, 28, 28)\n",
    "  assert x_test.shape == (10000, 28, 28)\n",
    "  assert y_train.shape == (60000,)\n",
    "  assert y_test.shape == (10000,)\n",
    "\n",
    "  mean_mat = np.mean(x_train, axis=0)\n",
    "  # centering the data by removing the pixel wise mean from every pixel in every image\n",
    "  x_train_centered = x_train - mean_mat\n",
    "  x_test_centered = x_test - mean_mat\n",
    "\n",
    "  # normalizing the grayscale values to values in interval [0,1]\n",
    "  x_train_normalized = x_train_centered/255.0\n",
    "  x_test_normalized = x_test_centered/255.0\n",
    "\n",
    "  # finally, flattening the data\n",
    "  x_train = np.reshape(x_train_normalized, (60000,784))\n",
    "  x_test = np.reshape(x_test_normalized, (10000, 784))\n",
    "  \n",
    "  #converting the test data to one hot encodings\n",
    "  y_train = keras.utils.to_categorical(y_train, num_classes=10)\n",
    "  y_test = keras.utils.to_categorical(y_test, num_classes=10)\n",
    "\n",
    "  return x_train, y_train, x_test, y_test\n",
    "x_train, y_train, x_test, y_test = preprocess_mnist()\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Implementation - Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_acc(truth, pred):\n",
    "  counter =0\n",
    "\n",
    "  for i in range(len(pred)):\n",
    "    maxVal = np.where(pred[i] == np.amax(pred[i]))\n",
    "    counter += 1 if maxVal == np.where(truth[i]==1) else 0\n",
    "  return counter / float(len(pred))\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#activation functions\n",
    "softmax1D = lambda z: np.exp(z)/float(sum(np.exp(z)))\n",
    "softmax2D = lambda z: np.array([np.exp(i)/float(sum(np.exp(i))) for i in z])\n",
    "linear = lambda z: np.array([i/float(sum(i)) for i in z])\n",
    "# logistic = lambda z: 1./ (1 + np.exp(-z))\n",
    "# relu = lambda y: y[y <= 0]=0\n",
    "def relu(x):\n",
    "  alpha = 0.1\n",
    "  x=np.array(x).astype(float)\n",
    "  # x[x<=0]=0.1*x\n",
    "  np.putmask(x, x<0, alpha*x)\n",
    "  return x\n",
    "def relu_grad(x):\n",
    "  alpha = 0.1\n",
    "  x=np.array(x).astype(float)\n",
    "  x[x>0]=1\n",
    "  x[x<=0]=alpha\n",
    "  return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_2_Layer_Softmax:\n",
    "\n",
    "    def __init__(self, M = 128, num_classes = 10, rsag=False):\n",
    "        self.M = M\n",
    "        self.num_classes = num_classes\n",
    "        self.rsag = rsag\n",
    "        self.params = None\n",
    "        self.aggr_params = None\n",
    "\n",
    "    def fit(self, x, y, optimizer):\n",
    "        N,D = x.shape\n",
    "        def gradient(x, y, params):\n",
    "            w = params[0] # v.shape = (D, M), w.shape = (M)\n",
    "            z = np.dot(x, w)\n",
    "            yh = softmax2D(z)#N\n",
    "            dy = yh - y #N\n",
    "            train_acc = evaluate_acc(y, yh)\n",
    "\n",
    "            # Softmax Gradient\n",
    "            dw = np.dot(x.T, dy)/N #M\n",
    "            dparams = [dw]\n",
    "            return dparams, train_acc\n",
    "        \n",
    "        if self.params is None:\n",
    "            initializer = keras.initializers.GlorotNormal()\n",
    "            w = initializer(shape=(D, self.num_classes))\n",
    "            self.params = [w]\n",
    "            if self.rsag:\n",
    "                self.aggr_params = [np.copy(w)]\n",
    "            print('params initialized')\n",
    "\n",
    "        if self.rsag:\n",
    "            self.params, self.aggr_params, train_accs, batch_train_acc = optimizer.mini_batch_step(gradient, x, y, self.params, self.aggr_params)\n",
    "        else:\n",
    "            self.params, train_accs, batch_train_acc = optimizer.mini_batch_step(gradient, x, y, self.params)\n",
    "\n",
    "        return self, train_accs, batch_train_acc\n",
    "\n",
    "    def predict(self, x):\n",
    "        # print('self:',self)\n",
    "        # print('self==None:',self==None)\n",
    "        w = self.params[0]\n",
    "        # print(w.shape)\n",
    "        # z = relu(np.dot(x, w)) #N x M\n",
    "        yh = softmax2D(np.dot(x, w))#N\n",
    "        return yh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_1_Layer_Linear:\n",
    "\n",
    "    def __init__(self, M = 128, num_classes = 10, rsag=False):\n",
    "        self.M = M\n",
    "        self.num_classes = num_classes\n",
    "        self.rsag = rsag\n",
    "        self.params = None\n",
    "        self.aggr_params = None\n",
    "\n",
    "    def fit(self, x, y, optimizer):\n",
    "        N,D = x.shape\n",
    "        def gradient(x, y, params):\n",
    "            w = params[0] # v.shape = (D, M), w.shape = (M)\n",
    "            z = np.dot(x, w)\n",
    "            yh = linear(z)#N\n",
    "            dw = np.sum(yh - y)/N #N\n",
    "            train_acc = evaluate_acc(yh, y)\n",
    "\n",
    "            # L2 Loss Gradient\n",
    "            # dw = np.dot(x.T, dy)/N #M\n",
    "            dparams = [dw]\n",
    "            return dparams ,train_acc\n",
    "        \n",
    "        if self.params is None:\n",
    "            initializer = keras.initializers.GlorotNormal()\n",
    "            w = initializer(shape=(D, self.num_classes))\n",
    "            self.params = [w]\n",
    "            if self.rsag:\n",
    "                self.aggr_params = [np.copy(w)]\n",
    "            print('params initialized')\n",
    "\n",
    "        if self.rsag:\n",
    "            self.params, self.aggr_params, train_accs, batch_train_acc = optimizer.mini_batch_step(gradient, x, y, self.params, self.aggr_params)\n",
    "        else:\n",
    "            self.params, train_accs, batch_train_acc = optimizer.mini_batch_step(gradient, x, y, self.params)\n",
    "\n",
    "        return self, train_accs, batch_train_acc\n",
    "\n",
    "    def predict(self, x):\n",
    "        # print('self:',self)\n",
    "        # print('self==None:',self==None)\n",
    "        w = self.params[0]\n",
    "\n",
    "        yh = linear(np.dot(x, w))#N\n",
    "        return yh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RSAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_lamda = lambda lr, t: lr/(1+t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, \n",
    "                optimizer, \n",
    "                x_train, \n",
    "                y_train, \n",
    "                x_valid, \n",
    "                y_valid, \n",
    "                verbose=True, \n",
    "                print_every=10, \n",
    "                patience=20,\n",
    "                save_log=False):\n",
    "    # x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "    # model.fit(x_train, y_train, optimizer)\n",
    "\n",
    "    # y_test_pred = model.predict(x_valid)\n",
    "    # temp_v_acc = evaluate_acc(y_test_pred, y_valid)\n",
    "    # v_acc = [temp_v_acc]\n",
    "\n",
    "    # print('acc:',temp_v_acc)\n",
    "    log = {'loss': [], 'accuracy': [], 'v_loss': [], 'v_accuracy': [], 'time': []}\n",
    "    t=0\n",
    "    log['time'].append(time.time())\n",
    "    curr_patience = 0\n",
    "    accept_update = (optimizer.lr_fn is not None) or (optimizer.alpha_fn is not None) or (optimizer.beta_fn is not None)\n",
    "\n",
    "\n",
    "    while t < optimizer.max_iters:\n",
    "        \n",
    "        model.fit(x_train, y_train, optimizer)\n",
    "\n",
    "        log['accuracy'].append(evaluate_acc(y_train, model.predict(x_train)))\n",
    "        log['loss'].append(log_loss(y_train, model.predict(x_train)))\n",
    "\n",
    "        y_valid_pred = model.predict(x_valid)\n",
    "        temp_v_acc = evaluate_acc(y_valid, y_valid_pred)\n",
    "        log['v_accuracy'].append(temp_v_acc)\n",
    "        log['v_loss'].append(log_loss(y_valid, y_valid_pred))\n",
    "        \n",
    "        if verbose:\n",
    "            if t%print_every == 0:\n",
    "                print('Epoch {}/{}'.format(t, optimizer.max_iters))\n",
    "                print('-' * 10)\n",
    "                print('Loss {:.4f}'.format(log['loss'][-1]))\n",
    "                print('Accuracy:  {:.4f}'.format(log['accuracy'][-1]))\n",
    "                print('Validation Loss {:.4f}'.format(log['v_loss'][-1]))\n",
    "                print('Validation Accuracy:  {:.4f}'.format(log['v_accuracy'][-1]))\n",
    "                print('Time :', log['time'][-1]-log['time'][0])\n",
    "        \n",
    "        log['time'].append(time.time())\n",
    "\n",
    "        if len(log['v_accuracy']) > 1 and (np.abs(log['v_accuracy'][-1]-log['v_accuracy'][-2])<0.001):\n",
    "            curr_patience += 1\n",
    "            if curr_patience > patience:\n",
    "                if accept_update:\n",
    "                    optimizer.set_update_params(True)\n",
    "                    curr_patience = 0\n",
    "                else:\n",
    "                    print('Early stopping at epoch %d'%t)\n",
    "                    break\n",
    "        else:\n",
    "            curr_patience = 0\n",
    "        t+=1\n",
    "    if verbose:\n",
    "        print(log)\n",
    "    # if save_log == True:\n",
    "    #     np.save('log.csv', log)\n",
    "    return model, log, max(log['v_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RSAG:\n",
    "\n",
    "    def __init__(self, \n",
    "                 learning_rate=.001, \n",
    "                 alpha=0.009, \n",
    "                 beta=.000009, \n",
    "                 max_iters=200, \n",
    "                 epsilon=1e-8, \n",
    "                 batch_size=64,\n",
    "                 lr_fn = None,\n",
    "                    alpha_fn = None,\n",
    "                    beta_fn = None,\n",
    "                    start_adap = 30,\n",
    "                 ):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iters = max_iters\n",
    "        self.epsilon = epsilon\n",
    "        self.alpha = alpha  # momentum param\n",
    "        self.beta = beta \n",
    "\n",
    "        self.lr_fn = lr_fn\n",
    "        self.alpha_fn = alpha_fn\n",
    "        self.beta_fn = beta_fn\n",
    "\n",
    "        self.update_params = False\n",
    "\n",
    "        self.t = 0\n",
    "\n",
    "\n",
    "    def set_update_params(self, update_params):\n",
    "        self.update_params = update_params\n",
    "\n",
    "    def mini_batch_step(self, \n",
    "                       gradient_fn,\n",
    "                       x, \n",
    "                       y,\n",
    "                       params, \n",
    "                       agg_params,\n",
    "                       batch_size=32,\n",
    "                       ):\n",
    "        \n",
    "        train_acc, batch_train_acc, chunk = [], [], []\n",
    "\n",
    "\n",
    "        slices = sklearn.utils.gen_batches(x.shape[0], batch_size)\n",
    "\n",
    "\n",
    "        if self.update_params:\n",
    "            print('Update params')\n",
    "            if self.lr_fn is not None:\n",
    "                self.learning_rate = self.lr_fn(self.learning_rate, self.t)\n",
    "                # print('New learning rate:', self.learning_rate)\n",
    "            if self.alpha_fn is not None:\n",
    "                self.alpha = self.alpha_fn(self.alpha, self.t)\n",
    "                # print('New alpha:', self.alpha)\n",
    "            if self.beta_fn is not None:\n",
    "                self.beta = self.beta_fn(self.beta, self.t)\n",
    "                # print('New beta:', self.beta)\n",
    "            self.update_params = False\n",
    "\n",
    "        grad = None\n",
    "        for batch in slices:\n",
    "            x_temp, y_temp = x[batch], y[batch]\n",
    "\n",
    "            proj_params = [(1-self.alpha) * a_p + self.alpha * p for p, a_p in zip(params, agg_params)]\n",
    "\n",
    "            grad, temp_acc = gradient_fn(x_temp, y_temp, proj_params)\n",
    "\n",
    "            chunk.append(temp_acc)\n",
    "            train_acc.append( ( self.t, temp_acc ) )\n",
    "\n",
    "            for p in range(len(params)):\n",
    "                agg_params[p] -= self.beta * (grad[p])\n",
    "                params[p] -= self.learning_rate * (grad[p])\n",
    "\n",
    "            \n",
    "        self.t += 1\n",
    "            \n",
    "        return params, agg_params, train_acc, batch_train_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.04339363\n",
      "Iteration 2, loss = 0.52378460\n",
      "CPU times: total: 44.5 s\n",
      "Wall time: 25 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(batch_size=64, hidden_layer_sizes=(512,), momentum=0.8,\n",
       "              solver=&#x27;sgd&#x27;, verbose=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(batch_size=64, hidden_layer_sizes=(512,), momentum=0.8,\n",
       "              solver=&#x27;sgd&#x27;, verbose=True)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MLPClassifier(batch_size=64, hidden_layer_sizes=(512,), momentum=0.8,\n",
       "              solver='sgd', verbose=True)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "c_clf = MLPClassifier(hidden_layer_sizes=(), activation='identity', \n",
    "                    solver='sgd', \n",
    "                    alpha=0.0001, momentum=0.8, nesterovs_momentum=True,\n",
    "                    batch_size=64, \n",
    "                    learning_rate='constant', learning_rate_init=0.09, \n",
    "                    verbose=True)\n",
    "%time c_clf.fit(x_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8017\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_test, c_clf.predict(x_test)))\n",
    "# activation='relu', solver='sgd', alpha=0.0001, batch_size=64, learning_rate='constant', learning_rate_init=0.001, nesterovs_momentum=True\n",
    "# [1.8023864815965787, 1.1122309566767148, 0.9845296966527957, 0.9154617448390219, 0.8699689145736055, 0.8365705881224808, 0.8107982315381743, 0.7895384784268797, 0.771082085742921, 0.7552732548141347, 0.7415558847414158, 0.7287698625332969, 0.7172472639691361, 0.706809744230456, 0.697198112331996, 0.6880630036115248, 0.6799326487354387, 0.6720643946144156, 0.6646984452814398, 0.6576804957640667, 0.6510529769693165, 0.6446246119656461]\n",
    "# 0.8017\n",
    "# CPU times: total: 7min 10s\n",
    "# Wall time: 3min 44s\n",
    "# solver='adam', alpha=0.0001, batch_size=64, learning_rate='constant', learning_rate_init=0.001, nesterovs_momentum=True,verbose=True)\n",
    "# 84.57\n",
    "# [0.8612156609999259, 0.6249129242272599, 0.5581398511623741, 0.5108802118568585, 0.4691072568910733, 0.4353992497295652, 0.40704429576434176, 0.3805020388852907, 0.35406887324687, 0.3298601532867995, 0.30623046652949937, 0.29002717397885547, 0.2713237669144441, 0.2514935455522592, 0.23837528939129574, 0.22462548777331365, 0.2122725128906315, 0.19964016897140632, 0.18637880570153922, 0.17356588508840962, 0.1658213505209685, 0.15647343628720575, 0.14783796049234926, 0.1333524127764709, 0.14204921604363624, 0.12750238350269527, 0.11881357392933954, 0.11200988128682542, 0.1130529385547876, 0.11354326204537345, 0.09885686873471046, 0.0930096509081991, 0.09655665834549663, 0.09260154624164536, 0.08115000406704216, 0.07959780062782788, 0.08250181458817121, 0.08106108561985036, 0.07341696734054892, 0.07604154997056141, 0.06295011133958502, 0.08018669386804467, 0.0616817261430883, 0.07367609626831328, 0.06523213700827496, 0.06964252739224731, 0.05752994633281319, 0.05197989734371084, 0.07381226348438415, 0.05830003207888134, 0.059422724682382105, 0.05041107174843518, 0.05727327599541805, 0.045635822816795256, 0.07568988410558206, 0.051876567841403196, 0.046329602952653766, 0.05991002631492367, 0.053050026014173995, 0.05121909793694829, 0.05788941712186149, 0.043738605256584066, 0.062191099081914634, 0.040420605015593726, 0.04949669605103939, 0.05005099457912783, 0.046754105687687286, 0.055313401757353574, 0.031450249271437095, 0.07335947120026465, 0.03956862519001744, 0.0557280472207543, 0.04982615933858711, 0.05109769744532108, 0.04067857278228474, 0.03421287144297862, 0.05925097099087448, 0.04785653973425865, 0.03998925377281498, 0.04366581677240043]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params initialized\n",
      "Epoch 0/200\n",
      "----------\n",
      "Loss 17.4952\n",
      "Accuracy:  0.0000\n",
      "Validation Loss 17.2828\n",
      "Validation Accuracy:  0.0000\n",
      "Time : 0.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[89], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m MLP_1_Layer_Linear(M\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m, num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, rsag\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      2\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m RSAG(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m.09\u001b[39m, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.009\u001b[39m, beta\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.00009\u001b[39m, lr_fn\u001b[38;5;241m=\u001b[39mlr_lamda, alpha_fn\u001b[38;5;241m=\u001b[39mlr_lamda, beta_fn\u001b[38;5;241m=\u001b[39mlr_lamda, start_adap\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m49\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m model, log, acc_max \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_valid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_valid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[63], line 32\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, optimizer, x_train, y_train, x_valid, y_valid, verbose, print_every, patience, save_log)\u001b[0m\n\u001b[0;32m     29\u001b[0m log[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(log_loss(y_train, model\u001b[38;5;241m.\u001b[39mpredict(x_train)))\n\u001b[0;32m     31\u001b[0m y_valid_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(x_valid)\n\u001b[1;32m---> 32\u001b[0m temp_v_acc \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_acc\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_valid_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_valid\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m log[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mv_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(temp_v_acc)\n\u001b[0;32m     34\u001b[0m log[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mv_loss\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(log_loss(y_valid, y_valid_pred))\n",
      "Cell \u001b[1;32mIn[80], line 6\u001b[0m, in \u001b[0;36mevaluate_acc\u001b[1;34m(truth, pred)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(pred)):\n\u001b[0;32m      5\u001b[0m   maxVal \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(pred[i] \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mamax(pred[i]))\n\u001b[1;32m----> 6\u001b[0m   counter \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m maxVal \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(truth[i]\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m counter \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;28mlen\u001b[39m(pred))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = MLP_1_Layer_Linear(M=128, num_classes=10, rsag=True)\n",
    "optimizer = RSAG(learning_rate=.09, alpha=0.009, beta=0.00009, lr_fn=lr_lamda, alpha_fn=lr_lamda, beta_fn=lr_lamda, start_adap=49)\n",
    "\n",
    "model, log, acc_max = train_model(model, optimizer, x_train, y_train, x_valid, y_valid, print_every=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params initialized\n",
      "Epoch 0/200\n",
      "----------\n",
      "Loss 14.7551\n",
      "Accuracy:  0.1216\n",
      "Validation Loss 14.7445\n",
      "Validation Accuracy:  0.1266\n",
      "Time : 0.0\n",
      "Epoch 10/200\n",
      "----------\n",
      "Loss 14.7551\n",
      "Accuracy:  0.1216\n",
      "Validation Loss 14.7445\n",
      "Validation Accuracy:  0.1266\n",
      "Time : 30.225996255874634\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[95], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m MLP_1_Layer_Linear(M\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m, num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, rsag\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      2\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m RSAG(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m.09\u001b[39m, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.009\u001b[39m, beta\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.00009\u001b[39m, lr_fn\u001b[38;5;241m=\u001b[39mlr_lamda, alpha_fn\u001b[38;5;241m=\u001b[39mlr_lamda, beta_fn\u001b[38;5;241m=\u001b[39mlr_lamda, start_adap\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m49\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m model, log, acc_max \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_valid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_valid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[94], line 29\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, optimizer, x_train, y_train, x_valid, y_valid, verbose, print_every, patience, save_log)\u001b[0m\n\u001b[0;32m     27\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(x_train, y_train, optimizer)\n\u001b[0;32m     28\u001b[0m log[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(evaluate_acc(y_train, model\u001b[38;5;241m.\u001b[39mpredict(x_train)))\n\u001b[1;32m---> 29\u001b[0m log[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(log_loss(y_train, \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[0;32m     31\u001b[0m y_valid_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(x_valid)\n\u001b[0;32m     32\u001b[0m temp_v_acc \u001b[38;5;241m=\u001b[39m evaluate_acc(y_valid, y_valid_pred)\n",
      "Cell \u001b[1;32mIn[61], line 44\u001b[0m, in \u001b[0;36mMLP_1_Layer_Linear.predict\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;66;03m# print('self:',self)\u001b[39;00m\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;66;03m# print('self==None:',self==None)\u001b[39;00m\n\u001b[0;32m     42\u001b[0m     w \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m---> 44\u001b[0m     yh \u001b[38;5;241m=\u001b[39m \u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;66;03m#N\u001b[39;00m\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m yh\n",
      "Cell \u001b[1;32mIn[59], line 4\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(z)\u001b[0m\n\u001b[0;32m      2\u001b[0m softmax1D \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m z: np\u001b[38;5;241m.\u001b[39mexp(z)\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;28msum\u001b[39m(np\u001b[38;5;241m.\u001b[39mexp(z)))\n\u001b[0;32m      3\u001b[0m softmax2D \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m z: np\u001b[38;5;241m.\u001b[39marray([np\u001b[38;5;241m.\u001b[39mexp(i)\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;28msum\u001b[39m(np\u001b[38;5;241m.\u001b[39mexp(i))) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m z])\n\u001b[1;32m----> 4\u001b[0m linear \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m z: np\u001b[38;5;241m.\u001b[39marray([i\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;28msum\u001b[39m(i)) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m z])\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# logistic = lambda z: 1./ (1 + np.exp(-z))\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# relu = lambda y: y[y <= 0]=0\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrelu\u001b[39m(x):\n",
      "Cell \u001b[1;32mIn[59], line 4\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      2\u001b[0m softmax1D \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m z: np\u001b[38;5;241m.\u001b[39mexp(z)\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;28msum\u001b[39m(np\u001b[38;5;241m.\u001b[39mexp(z)))\n\u001b[0;32m      3\u001b[0m softmax2D \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m z: np\u001b[38;5;241m.\u001b[39marray([np\u001b[38;5;241m.\u001b[39mexp(i)\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;28msum\u001b[39m(np\u001b[38;5;241m.\u001b[39mexp(i))) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m z])\n\u001b[1;32m----> 4\u001b[0m linear \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m z: np\u001b[38;5;241m.\u001b[39marray([i\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m z])\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# logistic = lambda z: 1./ (1 + np.exp(-z))\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# relu = lambda y: y[y <= 0]=0\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrelu\u001b[39m(x):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = MLP_1_Layer_Linear(M=128, num_classes=10, rsag=True)\n",
    "optimizer = RSAG(learning_rate=.09, alpha=0.009, beta=0.00009, lr_fn=lr_lamda, alpha_fn=lr_lamda, beta_fn=lr_lamda, start_adap=49)\n",
    "\n",
    "model, log, acc_max = train_model(model, optimizer, x_train, y_train, x_valid, y_valid, print_every=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-Convex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constant Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "adam_clf = MLPClassifier(hidden_layer_sizes=(512,), activation='relu', solver='adam', alpha=0.0001, momentum=0.8, batch_size=64, learning_rate='constant', learning_rate_init=0.09, nesterovs_momentum=True,verbose=True)\n",
    "%time adam_clf.fit(x_train, y_train)\n",
    "print(adam_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.45144808\n",
      "Iteration 2, loss = 0.20030202\n",
      "Iteration 3, loss = 0.13861566\n",
      "Iteration 4, loss = 0.10182641\n",
      "Iteration 5, loss = 0.07576465\n",
      "Iteration 6, loss = 0.05712919\n",
      "Iteration 7, loss = 0.04323124\n",
      "Iteration 8, loss = 0.03155732\n",
      "Iteration 9, loss = 0.02423139\n",
      "Iteration 10, loss = 0.01871788\n",
      "Iteration 11, loss = 0.01531092\n",
      "Iteration 12, loss = 0.01264694\n",
      "Iteration 13, loss = 0.01075438\n",
      "Iteration 14, loss = 0.00947037\n",
      "Iteration 15, loss = 0.00827882\n",
      "Iteration 16, loss = 0.00756713\n",
      "Iteration 17, loss = 0.00694991\n",
      "Iteration 18, loss = 0.00643158\n",
      "Iteration 19, loss = 0.00595323\n",
      "Iteration 20, loss = 0.00565800\n",
      "Iteration 21, loss = 0.00535291\n",
      "Iteration 22, loss = 0.00507294\n",
      "Iteration 23, loss = 0.00488000\n",
      "Iteration 24, loss = 0.00465542\n",
      "Iteration 25, loss = 0.00448840\n",
      "Iteration 26, loss = 0.00436394\n",
      "Iteration 27, loss = 0.00421457\n",
      "Iteration 28, loss = 0.00408843\n",
      "Iteration 29, loss = 0.00399359\n",
      "Iteration 30, loss = 0.00389515\n",
      "Iteration 31, loss = 0.00379460\n",
      "Iteration 32, loss = 0.00372098\n",
      "Iteration 33, loss = 0.00365465\n",
      "Iteration 34, loss = 0.00357370\n",
      "Iteration 35, loss = 0.00352187\n",
      "Iteration 36, loss = 0.00345490\n",
      "Iteration 37, loss = 0.00339894\n",
      "Iteration 38, loss = 0.00335934\n",
      "Iteration 39, loss = 0.00330845\n",
      "Iteration 40, loss = 0.00326557\n",
      "Iteration 41, loss = 0.00322551\n",
      "Iteration 42, loss = 0.00318556\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "CPU times: total: 14min 22s\n",
      "Wall time: 7min 56s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(batch_size=64, hidden_layer_sizes=(512,), learning_rate_init=0.09,\n",
       "              momentum=0.8, solver=&#x27;sgd&#x27;, verbose=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(batch_size=64, hidden_layer_sizes=(512,), learning_rate_init=0.09,\n",
       "              momentum=0.8, solver=&#x27;sgd&#x27;, verbose=True)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MLPClassifier(batch_size=64, hidden_layer_sizes=(512,), learning_rate_init=0.09,\n",
       "              momentum=0.8, solver='sgd', verbose=True)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "clf = MLPClassifier(hidden_layer_sizes=(512,), activation='relu', solver='sgd', alpha=0.0001, momentum=0.8, batch_size=64, learning_rate='constant', learning_rate_init=0.09, nesterovs_momentum=True,verbose=True)\n",
    "%time clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params initialized\n",
      "Epoch 0/200\n",
      "----------\n",
      "Loss 2.2549\n",
      "Accuracy:  0.1478\n",
      "Validation Loss 2.2535\n",
      "Validation Accuracy:  0.1478\n",
      "Time : 0.0\n",
      "Epoch 10/200\n",
      "----------\n",
      "Loss 1.5257\n",
      "Accuracy:  0.6403\n",
      "Validation Loss 1.5279\n",
      "Validation Accuracy:  0.6391\n",
      "Time : 31.632031440734863\n",
      "Epoch 20/200\n",
      "----------\n",
      "Loss 1.1125\n",
      "Accuracy:  0.7077\n",
      "Validation Loss 1.1151\n",
      "Validation Accuracy:  0.7047\n",
      "Time : 63.96003222465515\n",
      "Epoch 30/200\n",
      "----------\n",
      "Loss 0.9175\n",
      "Accuracy:  0.7224\n",
      "Validation Loss 0.9203\n",
      "Validation Accuracy:  0.7215\n",
      "Time : 96.51003050804138\n",
      "Epoch 40/200\n",
      "----------\n",
      "Loss 0.8393\n",
      "Accuracy:  0.7273\n",
      "Validation Loss 0.8430\n",
      "Validation Accuracy:  0.7267\n",
      "Time : 129.29102683067322\n",
      "Epoch 50/200\n",
      "----------\n",
      "Loss 0.8228\n",
      "Accuracy:  0.7295\n",
      "Validation Loss 0.8280\n",
      "Validation Accuracy:  0.7298\n",
      "Time : 162.48103308677673\n",
      "Early stopping at epoch 56\n",
      "{'loss': [2.2548951593048785, 2.1681448416511957, 2.084150150450492, 2.003054597431855, 1.924995940079093, 1.8500990655938359, 1.7784685347480476, 1.7101829061236276, 1.6452901909665052, 1.5838063703940006, 1.525716178887104, 1.470975388337659, 1.419514723369808, 1.3712438653822332, 1.3260556131374273, 1.2838303480040845, 1.2444393062234615, 1.2077480580687154, 1.1736192319280856, 1.1419151162610723, 1.1124993859990395, 1.0852387829191557, 1.0600043221557174, 1.0366721934510068, 1.0151242262772169, 0.9952483378966324, 0.9769385857961028, 0.9600952386068108, 0.9446246512904207, 0.9304391707624818, 0.9174567615000906, 0.9056008414840461, 0.8947999312320546, 0.8849874611793099, 0.8761013313465075, 0.8680837197588119, 0.8608808209422169, 0.8544424279365916, 0.8487217165910738, 0.8436752104215457, 0.8392623660485848, 0.8354452768504124, 0.8321887112459229, 0.8294597506287424, 0.827227638480667, 0.825463710292648, 0.8241412146746804, 0.8232350803899773, 0.8227219528940058, 0.822579890674374, 0.8227883604117717, 0.82332819728809, 0.8241814538655929, 0.8253312604900835, 0.82676184864467, 0.828458377122787, 0.8304068960490206], 'accuracy': [0.14783333333333334, 0.24060416666666667, 0.33095833333333335, 0.4033125, 0.4615416666666667, 0.5100416666666666, 0.5488958333333334, 0.5796458333333333, 0.6050208333333333, 0.6242291666666666, 0.6403333333333333, 0.65325, 0.6648125, 0.6732916666666666, 0.6807916666666667, 0.6875, 0.6922916666666666, 0.69725, 0.7011041666666666, 0.7046875, 0.7076666666666667, 0.7099583333333334, 0.7120833333333333, 0.7137916666666667, 0.7149791666666667, 0.716625, 0.7182083333333333, 0.7194791666666667, 0.7208125, 0.7215208333333333, 0.7223958333333333, 0.7230416666666667, 0.7232708333333333, 0.723875, 0.7245, 0.7249375, 0.7251875, 0.7257916666666666, 0.7264166666666667, 0.7268125, 0.7272708333333333, 0.7275625, 0.7277291666666666, 0.7279583333333334, 0.72825, 0.7284583333333333, 0.7285833333333334, 0.7290416666666667, 0.7293125, 0.7294583333333333, 0.7295416666666666, 0.7296041666666667, 0.7296041666666667, 0.729875, 0.7299791666666666, 0.7301458333333334, 0.7302708333333333], 'v_loss': [2.253527430636488, 2.1672800448950587, 2.0837632795100496, 2.003117692880976, 1.925478034435076, 1.8509662127481668, 1.7796839659604993, 1.7117074386885252, 1.6470829741895474, 1.5858259620112833, 1.5279218468558338, 1.4733284439939471, 1.4219797153815197, 1.3737894751183304, 1.328655205764657, 1.2864622034927002, 1.2470865431618388, 1.2103983690192086, 1.1762644578053552, 1.1445507536226474, 1.115124070517085, 1.0878537581343246, 1.0626129579311172, 1.0392795497761989, 1.0177366509551025, 0.997873135945757, 0.9795837265668854, 0.9627691163211114, 0.947335888414015, 0.9331964617059831, 0.9202687408198245, 0.9084759853450015, 0.8977464776125152, 0.888013365694805, 0.8792142235046103, 0.8712908700750414, 0.8641891330645146, 0.8578584271121266, 0.852251545176144, 0.8473246338937476, 0.843036792995328, 0.8393497576356127, 0.8362279571499034, 0.8336381572381103, 0.8315492941406604, 0.8299324111308976, 0.8287604786300825, 0.8280081669660688, 0.8276518848720719, 0.8276694645682413, 0.8280401689689012, 0.8287446259726562, 0.8297647134201892, 0.8310833963769992, 0.8326847447886991, 0.834553780331595, 0.8366764207008379], 'v_accuracy': [0.14783333333333334, 0.24025, 0.33108333333333334, 0.4088333333333333, 0.4681666666666667, 0.5128333333333334, 0.55, 0.5795, 0.6044166666666667, 0.6248333333333334, 0.6390833333333333, 0.65275, 0.66275, 0.67, 0.6785833333333333, 0.686, 0.6921666666666667, 0.69525, 0.6995, 0.70325, 0.70475, 0.7073333333333334, 0.7105, 0.7128333333333333, 0.714, 0.7159166666666666, 0.71725, 0.71875, 0.7194166666666667, 0.72025, 0.7215, 0.7226666666666667, 0.724, 0.7243333333333334, 0.7243333333333334, 0.7255833333333334, 0.72625, 0.7263333333333334, 0.7266666666666667, 0.72625, 0.7266666666666667, 0.7265833333333334, 0.72725, 0.7279166666666667, 0.7278333333333333, 0.7284166666666667, 0.7286666666666667, 0.7285833333333334, 0.7291666666666666, 0.72975, 0.7298333333333333, 0.7299166666666667, 0.7299166666666667, 0.7305833333333334, 0.7311666666666666, 0.73125, 0.731], 'time': [1703953018.9496017, 1703953022.3516316, 1703953025.4656327, 1703953028.5686328, 1703953031.7015977, 1703953034.8215983, 1703953037.9765975, 1703953041.153598, 1703953044.3185997, 1703953047.4786003, 1703953050.581633, 1703953053.791597, 1703953056.9796348, 1703953060.2546325, 1703953063.5425956, 1703953066.7076313, 1703953069.9166324, 1703953073.1706011, 1703953076.3616323, 1703953079.5766325, 1703953082.9096339, 1703953086.180597, 1703953089.430596, 1703953092.691597, 1703953095.9296002, 1703953099.1806006, 1703953102.401597, 1703953105.6475964, 1703953108.9165957, 1703953112.196638, 1703953115.4596322, 1703953118.7766314, 1703953122.0745971, 1703953125.3146002, 1703953128.5466309, 1703953131.8546336, 1703953135.1446347, 1703953138.4295971, 1703953141.6916335, 1703953144.9465966, 1703953148.2406285, 1703953151.5666327, 1703953154.887597, 1703953158.2436328, 1703953161.5776331, 1703953164.8596308, 1703953168.155628, 1703953171.4656315, 1703953174.7535987, 1703953178.0665967, 1703953181.4306347, 1703953184.6996334, 1703953188.0105984, 1703953191.3266363, 1703953194.575597, 1703953197.8476326, 1703953201.1316028, 1703953204.419599]}\n"
     ]
    }
   ],
   "source": [
    "model = MLP_2_Layer_Softmax(M=128, num_classes=10, rsag=True)\n",
    "optimizer = RSAG(learning_rate=.08, alpha=0.009, beta=0.0009)\n",
    "model, log, acc_max = train_model(model, optimizer, x_train, y_train, x_valid, y_valid, print_every=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nesterov's Momentum:  0.9665\n",
      "RSAG:  0.7206\n"
     ]
    }
   ],
   "source": [
    "print('Nesterov\\'s Momentum: ', accuracy_score(y_test, clf.predict(x_test)))\n",
    "# print('Adam', adam_clf.eval_acc(x_test, y_test))\n",
    "print('RSAG: ', evaluate_acc(y_test, model.predict(x_test)))\n",
    "# Nesterov's Momentum:  0.9665\n",
    "# RSAG:  0.7206\n",
    "# {'loss': [2.357493262704862, 2.3471664639089758, 2.336871564699908, 2.3266087629948156, 2.3163782663969625, 2.306180261969263, 2.296014954792932, 2.2858825514370684, 2.275783276062723, 2.2657173287436483, 2.2556849100078917, 2.2456862319197106, 2.2357215177010676, 2.2257909599465244, 2.2158947770600275, 2.2060331973150737, 2.196206441997492, 2.1864147096406477, 2.176658217864617, 2.1669371854196693, 2.157251819200863, 2.14760234162527, 2.137988972627998, 2.1284119197860307, 2.118871425117049, 2.109367674948375, 2.0999008857870667, 2.090471269311462, 2.08107904117874, 2.0717244160673745, 2.0624075955401304, 2.0531287910323406, 2.04388826380772, 2.0346861655173676, 2.0255226950944136, 2.0163980549045832, 2.0073124405917144, 1.9982660519594215, 1.9892590871820437, 1.9802917504084085, 1.9713642381485412, 1.9624767285533795, 1.9536294092370041, 1.9448224721737466, 1.9360560871976538, 1.927330438909241, 1.9186457078648194, 1.9100020793230257, 1.9013996939811455, 1.8928387139891918, 1.8843192982920383, 1.8758416041721995, 1.867405786634906, 1.8590119973651469, 1.8506603741164986, 1.8423510708487176, 1.8340842025822006, 1.8258598867168763, 1.8176782536714275, 1.8095394062750911, 1.8014434603255238, 1.7933905253239064, 1.7853807145097298, 1.77741413657924, 1.7694908570719345, 1.761610974587742, 1.7537745600777388, 1.745981711821348, 1.7382324795136586, 1.7305269323037258, 1.7228651379315445, 1.7152471544864067, 1.7076730185872229, 1.7001427764456825, 1.692656461394606, 1.685214114127181, 1.6778157432326959, 1.6704613852950891, 1.663151042325173, 1.6558847239251082, 1.6486624205084093, 1.6414841584081972, 1.634349925091247, 1.6272597036491694, 1.6202134840268336, 1.613211239475934, 1.6062529404694201, 1.599338549310541, 1.5924680306813344, 1.5856413422482407, 1.5788584306457842, 1.5721192430191588, 1.5654237257144978, 1.5587718117003326, 1.552163438485007, 1.5455985356840478, 1.539077018356512, 1.5325988192898454, 1.5261638526559544, 1.5197720257093084, 1.5134232479389964, 1.5071174298821348, 1.5008544654452682, 1.4946342470550513, 1.4884566597806794, 1.482321605227411, 1.4762289688978025, 1.470178623328491, 1.4641704381432095, 1.458204291786358, 1.452280058499483, 1.4463976093343172, 1.4405568138450227, 1.4347575358249771, 1.4289996230858726, 1.4232829289511146, 1.4176073120800945, 1.4119726269764141, 1.4063787157941727, 1.4008254311498027, 1.3953126101865407, 1.3898401094373254, 1.3844077445091605, 1.3790153681629478, 1.373662804813439, 1.3683498867817576, 1.363076448688257, 1.3578423279983778, 1.352647343813666, 1.347491321779183, 1.3423740796751333, 1.3372954394044785, 1.332255223853742, 1.3272532568189093, 1.3222893615117548, 1.3173633443465387, 1.3124750242808674, 1.307624207317904, 1.3028107069865984, 1.298034345636163, 1.2932949311900148, 1.2885922710110211, 1.2839261762429317, 1.2792964573220336, 1.2747029192256465, 1.2701453703025112, 1.2656236179953457, 1.2611374637325692, 1.2566867099515513, 1.2522711703236982, 1.2478906349710015, 1.2435449073574296, 1.2392337982222992, 1.2349571140146063, 1.230714658922692, 1.2265062404549303, 1.2223316460218046, 1.218190686900038, 1.2140831605096771, 1.2100088684362353, 1.2059676134426724, 1.2019591867520412, 1.1979833987161441, 1.19404005150336, 1.190128946267287, 1.1862498786299527], 'accuracy': [7.954166666666667, 8.35625, 8.808333333333334, 9.4125, 9.929166666666667, 10.495833333333334, 11.175, 11.78125, 12.425, 13.064583333333333, 13.789583333333333, 14.554166666666667, 15.375, 16.2, 17.072916666666668, 17.897916666666667, 18.772916666666667, 19.697916666666668, 20.589583333333334, 21.527083333333334, 22.46875, 23.5125, 24.516666666666666, 25.54375, 26.502083333333335, 27.408333333333335, 28.414583333333333, 29.485416666666666, 30.414583333333333, 31.447916666666668, 32.46458333333333, 33.39791666666667, 34.260416666666664, 35.233333333333334, 36.1125, 36.96458333333333, 37.87291666666667, 38.7125, 39.483333333333334, 40.21041666666667, 40.99791666666667, 41.76875, 42.50416666666667, 43.2, 43.916666666666664, 44.56041666666667, 45.24166666666667, 45.90625, 46.5625, 47.175, 47.75625, 48.30625, 48.87708333333333, 49.422916666666666, 50.00833333333333, 50.577083333333334, 51.13125, 51.6875, 52.229166666666664, 52.75833333333333, 53.2625, 53.77708333333333, 54.21458333333333, 54.65416666666667, 55.1125, 55.608333333333334, 56.00416666666667, 56.37708333333333, 56.775, 57.114583333333336, 57.427083333333336, 57.77291666666667, 58.18125, 58.514583333333334, 58.8375, 59.1375, 59.46458333333333, 59.75, 60.03541666666667, 60.27291666666667, 60.52916666666667, 60.766666666666666, 61.05416666666667, 61.3, 61.5, 61.76875, 62.02708333333333, 62.275, 62.51875, 62.71041666666667, 62.925, 63.15416666666667, 63.36666666666667, 63.59166666666667, 63.78333333333333, 63.979166666666664, 64.16458333333334, 64.35833333333333, 64.51875, 64.68958333333333, 64.83333333333333, 64.99375, 65.13541666666667, 65.31458333333333, 65.49166666666666, 65.66041666666666, 65.80625, 65.97083333333333, 66.13125, 66.24166666666666, 66.38125, 66.5375, 66.675, 66.76458333333333, 66.90625, 67.04166666666667, 67.17708333333333, 67.2875, 67.39791666666666, 67.52708333333334, 67.64791666666666, 67.76666666666667, 67.8875, 67.99583333333334, 68.11041666666667, 68.2375, 68.3625, 68.46666666666667, 68.5875, 68.68541666666667, 68.75833333333334, 68.83958333333334, 68.94375, 69.04166666666667, 69.10416666666667, 69.19583333333334, 69.27291666666666, 69.34166666666667, 69.40416666666667, 69.47083333333333, 69.4875, 69.575, 69.63958333333333, 69.69583333333334, 69.75208333333333, 69.78958333333334, 69.85416666666667, 69.92083333333333, 70.00416666666666, 70.0625, 70.11458333333333, 70.16666666666667, 70.20833333333333, 70.25833333333334, 70.30416666666666, 70.34791666666666, 70.39166666666667, 70.43125, 70.49583333333334, 70.53333333333333, 70.59791666666666, 70.64583333333333, 70.7, 70.76666666666667, 70.79791666666667, 70.84583333333333], 'v_loss': [2.3575893921522804, 2.347310633922136, 2.3370634920179945, 2.326848160651769, 2.3166648412814554, 2.306513717011736, 2.296394988167253, 2.286308856368212, 2.276255540847012, 2.266235235048125, 2.2562481364534883, 2.246294452155379, 2.2363743996404275, 2.2264881680363025, 2.216635970401511, 2.20681803162157, 2.1970345675767, 2.1872857715673923, 2.1775718567336466, 2.167893037909416, 2.158249517188516, 2.1486415123311926, 2.139069239120084, 2.1295329003857835, 2.120032732951087, 2.110568919251057, 2.10114167129062, 2.0917511965640183, 2.0823977065829884, 2.0730814113331273, 2.063802509355281, 2.0545612078299014, 2.045357763425631, 2.0361923237160995, 2.027065084029318, 2.0179762429591896, 2.008925991929431, 1.999914527193297, 1.9909420435097551, 1.9820087415491647, 1.9731148130100646, 1.9642604341411618, 1.9554457883983836, 1.9466710634871287, 1.9379364261071441, 1.9292420573227032, 1.9205881346975124, 1.911974840416999, 1.903402311685569, 1.8948707079207263, 1.8863801850373074, 1.8779308977198543, 1.8695229975179604, 1.8611566345121597, 1.8528319435511784, 1.8445490756004295, 1.836308143303213, 1.8281092624004456, 1.819952560644179, 1.8118381386216897, 1.803766110754626, 1.7957365837595116, 1.7877496699548294, 1.7798054766253617, 1.771904067165392, 1.7640455394284453, 1.7562299633154403, 1.748457435707695, 1.7407280062816715, 1.733041743490038, 1.7253987133835744, 1.7177989746893734, 1.7102425633743223, 1.7027295262894526, 1.6952598965434509, 1.6878337151579423, 1.680450991739085, 1.6731117644232696, 1.6658160359868064, 1.6585638174963937, 1.6513551006594516, 1.6441899124678154, 1.6370682427881853, 1.6299900763502821, 1.622955406002054, 1.6159642077312644, 1.6090164546128527, 1.6021121127681675, 1.59525114849903, 1.5884335238968494, 1.5816591883205469, 1.574928092637531, 1.568240187742883, 1.5615954107350374, 1.5549937033055397, 1.548434999392862, 1.5419192194640878, 1.5354463006606045, 1.5290161624494643, 1.5226287171728767, 1.5162838801632545, 1.509981567416252, 1.5037216782541782, 1.4975041109779026, 1.4913287569705316, 1.4851955246012383, 1.4791043053045678, 1.4730549777893571, 1.4670474183422764, 1.4610815129268862, 1.4551571421047722, 1.449274183704767, 1.4434325135988877, 1.4376320020914257, 1.4318725056548207, 1.4261538841092205, 1.4204760024431358, 1.4148387234280078, 1.409241896222079, 1.4036853816404897, 1.3981690245772582, 1.3926926887320712, 1.387256196861038, 1.3818594093958394, 1.3765021584859793, 1.371184283588369, 1.36590562702681, 1.360666033338891, 1.3554653293092178, 1.3503033481473847, 1.3451799142159577, 1.3400948576971714, 1.3350480088359895, 1.3300391985072253, 1.3250682575499002, 1.3201349986918638, 1.3152392493373128, 1.3103808212758619, 1.305559536424419, 1.3007752240926898, 1.2960276992792057, 1.2913167759910609, 1.2866422718990236, 1.2820040043449357, 1.2774017852257296, 1.2728354291292547, 1.2683047503689577, 1.2638095570498964, 1.2593496576501206, 1.2549248722624788, 1.2505349971060384, 1.246179841166178, 1.2418592220949032, 1.2375729523946908, 1.2333208419269788, 1.2291027038451399, 1.2249183312797363, 1.2207675404406864, 1.2166501348022476, 1.2125659213436153, 1.2085147076273997, 1.2044962903274274, 1.2005104852997213, 1.1965570992855818, 1.1926359378376434, 1.1887468023454533], 'v_accuracy': [8.008333333333333, 8.4, 8.833333333333334, 9.366666666666667, 9.95, 10.508333333333333, 11.083333333333334, 11.625, 12.233333333333333, 12.841666666666667, 13.55, 14.316666666666666, 15.025, 15.825, 16.591666666666665, 17.4, 18.416666666666668, 19.308333333333334, 20.166666666666668, 21.116666666666667, 22.175, 23.183333333333334, 24.166666666666668, 25.175, 26.35, 27.508333333333333, 28.691666666666666, 29.666666666666668, 30.533333333333335, 31.458333333333332, 32.391666666666666, 33.40833333333333, 34.425, 35.233333333333334, 36.18333333333333, 36.99166666666667, 37.84166666666667, 38.641666666666666, 39.4, 40.05833333333333, 40.94166666666667, 41.625, 42.275, 43.05, 43.71666666666667, 44.333333333333336, 44.983333333333334, 45.666666666666664, 46.166666666666664, 46.775, 47.30833333333333, 47.925, 48.50833333333333, 49.06666666666667, 49.766666666666666, 50.31666666666667, 50.88333333333333, 51.43333333333333, 51.96666666666667, 52.416666666666664, 52.916666666666664, 53.391666666666666, 53.925, 54.541666666666664, 55.016666666666666, 55.416666666666664, 55.81666666666667, 56.2, 56.69166666666667, 57.19166666666667, 57.63333333333333, 58.0, 58.333333333333336, 58.583333333333336, 58.858333333333334, 59.28333333333333, 59.516666666666666, 59.666666666666664, 59.975, 60.28333333333333, 60.45, 60.69166666666667, 60.94166666666667, 61.2, 61.49166666666667, 61.81666666666667, 62.141666666666666, 62.358333333333334, 62.608333333333334, 62.78333333333333, 63.025, 63.3, 63.46666666666667, 63.7, 63.86666666666667, 64.13333333333334, 64.35, 64.59166666666667, 64.85, 65.0, 65.225, 65.375, 65.54166666666667, 65.71666666666667, 65.9, 66.06666666666666, 66.2, 66.31666666666666, 66.425, 66.66666666666667, 66.75833333333334, 66.95833333333333, 67.125, 67.26666666666667, 67.4, 67.525, 67.64166666666667, 67.7, 67.84166666666667, 67.9, 67.94166666666666, 68.025, 68.21666666666667, 68.3, 68.43333333333334, 68.50833333333334, 68.625, 68.71666666666667, 68.80833333333334, 68.90833333333333, 68.94166666666666, 68.98333333333333, 69.09166666666667, 69.21666666666667, 69.30833333333334, 69.425, 69.475, 69.53333333333333, 69.55, 69.675, 69.71666666666667, 69.825, 69.85833333333333, 69.91666666666667, 70.025, 70.10833333333333, 70.19166666666666, 70.225, 70.275, 70.31666666666666, 70.35833333333333, 70.39166666666667, 70.40833333333333, 70.41666666666667, 70.45833333333333, 70.525, 70.56666666666666, 70.59166666666667, 70.61666666666666, 70.675, 70.725, 70.75833333333334, 70.79166666666667, 70.79166666666667, 70.84166666666667, 70.85833333333333], 'time': [1703949600.870197, 1703949604.2891946, 1703949607.9621935, 1703949611.3631942, 1703949614.7471917, 1703949617.9801943, 1703949621.374193, 1703949624.6571922, 1703949627.8852284, 1703949631.1401927, 1703949634.440192, 1703949637.755196, 1703949640.9302123, 1703949644.2072084, 1703949647.617213, 1703949651.0732124, 1703949654.1662095, 1703949657.228212, 1703949660.3032446, 1703949663.3462086, 1703949666.4732094, 1703949669.5592127, 1703949672.7642465, 1703949675.981247, 1703949679.0772462, 1703949682.1612139, 1703949685.2292452, 1703949688.4192088, 1703949691.5182106, 1703949694.622209, 1703949697.7012095, 1703949700.811209, 1703949703.9202132, 1703949707.01221, 1703949710.0942118, 1703949713.1722128, 1703949716.316213, 1703949719.4232092, 1703949722.5292494, 1703949725.6572127, 1703949728.764246, 1703949731.8712091, 1703949734.9422116, 1703949738.0562088, 1703949741.1372123, 1703949744.2712097, 1703949747.3732138, 1703949750.4662101, 1703949753.5272462, 1703949756.619209, 1703949759.7262504, 1703949762.7842112, 1703949765.8442092, 1703949768.9062464, 1703949772.0172083, 1703949775.146211, 1703949778.2582088, 1703949781.3812091, 1703949784.4672089, 1703949787.604209, 1703949790.7322457, 1703949793.8202085, 1703949796.9112086, 1703949800.0262456, 1703949803.128211, 1703949806.2102091, 1703949809.3052454, 1703949812.3732102, 1703949815.4772086, 1703949818.5282092, 1703949821.6342108, 1703949824.708213, 1703949827.8092139, 1703949830.9252455, 1703949833.9842122, 1703949837.0302088, 1703949840.1402447, 1703949843.272247, 1703949846.3422132, 1703949849.489209, 1703949852.5662122, 1703949855.6642087, 1703949858.7602432, 1703949861.8782089, 1703949865.075245, 1703949868.176209, 1703949871.247246, 1703949874.3412118, 1703949877.4662087, 1703949880.5482125, 1703949883.6712096, 1703949886.749246, 1703949889.8522127, 1703949892.9892104, 1703949896.2372112, 1703949899.3602087, 1703949902.44121, 1703949905.535209, 1703949908.7172127, 1703949911.8302445, 1703949914.9142094, 1703949918.0082095, 1703949921.1202126, 1703949924.2332091, 1703949927.3172116, 1703949930.4642491, 1703949933.577208, 1703949936.638209, 1703949939.7592454, 1703949942.8562453, 1703949945.9522111, 1703949949.0382087, 1703949952.1482441, 1703949955.3052456, 1703949958.615209, 1703949961.709245, 1703949965.0152457, 1703949968.5502093, 1703949972.0512455, 1703949975.6682117, 1703949979.795208, 1703949983.5702472, 1703949987.000211, 1703949990.7862089, 1703949994.0822477, 1703949998.1162117, 1703950002.0132096, 1703950005.630247, 1703950009.0662143, 1703950012.5982094, 1703950016.4362466, 1703950020.1952114, 1703950023.846209, 1703950027.818215, 1703950031.4872096, 1703950035.1532118, 1703950038.6392157, 1703950041.9772122, 1703950045.5602083, 1703950049.5952094, 1703950053.21621, 1703950056.877248, 1703950060.6822133, 1703950064.2012088, 1703950067.8502438, 1703950071.5822124, 1703950075.5992086, 1703950079.430245, 1703950082.9082105, 1703950086.534209, 1703950089.9962504, 1703950093.2852128, 1703950096.6192133, 1703950099.7932086, 1703950103.009247, 1703950106.2172093, 1703950109.4372087, 1703950112.5972087, 1703950115.7562113, 1703950118.8942108, 1703950122.1032088, 1703950125.285214, 1703950128.515213, 1703950131.686213, 1703950134.841213, 1703950138.0182126, 1703950141.2102091]}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaptive Rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.44681104\n",
      "Iteration 2, loss = 0.20019664\n",
      "Iteration 3, loss = 0.13903225\n",
      "Iteration 4, loss = 0.10424848\n",
      "Iteration 5, loss = 0.07820500\n",
      "Iteration 6, loss = 0.05875216\n",
      "Iteration 7, loss = 0.04341030\n",
      "Iteration 8, loss = 0.03309851\n",
      "Iteration 9, loss = 0.02487610\n",
      "Iteration 10, loss = 0.01929009\n",
      "Iteration 11, loss = 0.01581994\n",
      "Iteration 12, loss = 0.01311998\n",
      "Iteration 13, loss = 0.01097303\n",
      "Iteration 14, loss = 0.00954054\n",
      "Iteration 15, loss = 0.00844811\n",
      "Iteration 16, loss = 0.00766434\n",
      "Iteration 17, loss = 0.00701332\n",
      "Iteration 18, loss = 0.00648637\n",
      "Iteration 19, loss = 0.00609529\n",
      "Iteration 20, loss = 0.00567588\n",
      "Iteration 21, loss = 0.00540224\n",
      "Iteration 22, loss = 0.00513811\n",
      "Iteration 23, loss = 0.00489430\n",
      "Iteration 24, loss = 0.00469150\n",
      "Iteration 25, loss = 0.00452197\n",
      "Iteration 26, loss = 0.00438212\n",
      "Iteration 27, loss = 0.00422534\n",
      "Iteration 28, loss = 0.00410595\n",
      "Iteration 29, loss = 0.00402248\n",
      "Iteration 30, loss = 0.00391626\n",
      "Iteration 31, loss = 0.00381627\n",
      "Iteration 32, loss = 0.00373413\n",
      "Iteration 33, loss = 0.00365851\n",
      "Iteration 34, loss = 0.00358928\n",
      "Iteration 35, loss = 0.00353511\n",
      "Iteration 36, loss = 0.00347289\n",
      "Iteration 37, loss = 0.00342324\n",
      "Iteration 38, loss = 0.00336137\n",
      "Iteration 39, loss = 0.00332092\n",
      "Iteration 40, loss = 0.00327500\n",
      "Iteration 41, loss = 0.00324282\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.018000\n",
      "Iteration 42, loss = 0.00314726\n",
      "Iteration 43, loss = 0.00313540\n",
      "Iteration 44, loss = 0.00312747\n",
      "Iteration 45, loss = 0.00312126\n",
      "Iteration 46, loss = 0.00311425\n",
      "Iteration 47, loss = 0.00310803\n",
      "Iteration 48, loss = 0.00310170\n",
      "Iteration 49, loss = 0.00309536\n",
      "Iteration 50, loss = 0.00308970\n",
      "Iteration 51, loss = 0.00308334\n",
      "Iteration 52, loss = 0.00307821\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.003600\n",
      "Iteration 53, loss = 0.00306174\n",
      "Iteration 54, loss = 0.00306055\n",
      "Iteration 55, loss = 0.00305933\n",
      "Iteration 56, loss = 0.00305815\n",
      "Iteration 57, loss = 0.00305708\n",
      "Iteration 58, loss = 0.00305591\n",
      "Iteration 59, loss = 0.00305479\n",
      "Iteration 60, loss = 0.00305365\n",
      "Iteration 61, loss = 0.00305260\n",
      "Iteration 62, loss = 0.00305149\n",
      "Iteration 63, loss = 0.00305040\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000720\n",
      "Iteration 64, loss = 0.00304709\n",
      "Iteration 65, loss = 0.00304687\n",
      "Iteration 66, loss = 0.00304665\n",
      "Iteration 67, loss = 0.00304644\n",
      "Iteration 68, loss = 0.00304621\n",
      "Iteration 69, loss = 0.00304599\n",
      "Iteration 70, loss = 0.00304578\n",
      "Iteration 71, loss = 0.00304556\n",
      "Iteration 72, loss = 0.00304534\n",
      "Iteration 73, loss = 0.00304512\n",
      "Iteration 74, loss = 0.00304491\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000144\n",
      "Iteration 75, loss = 0.00304424\n",
      "Iteration 76, loss = 0.00304420\n",
      "Iteration 77, loss = 0.00304416\n",
      "Iteration 78, loss = 0.00304411\n",
      "Iteration 79, loss = 0.00304407\n",
      "Iteration 80, loss = 0.00304402\n",
      "Iteration 81, loss = 0.00304398\n",
      "Iteration 82, loss = 0.00304394\n",
      "Iteration 83, loss = 0.00304389\n",
      "Iteration 84, loss = 0.00304385\n",
      "Iteration 85, loss = 0.00304381\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000029\n",
      "Iteration 86, loss = 0.00304367\n",
      "Iteration 87, loss = 0.00304367\n",
      "Iteration 88, loss = 0.00304366\n",
      "Iteration 89, loss = 0.00304365\n",
      "Iteration 90, loss = 0.00304364\n",
      "Iteration 91, loss = 0.00304363\n",
      "Iteration 92, loss = 0.00304362\n",
      "Iteration 93, loss = 0.00304361\n",
      "Iteration 94, loss = 0.00304361\n",
      "Iteration 95, loss = 0.00304360\n",
      "Iteration 96, loss = 0.00304359\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000006\n",
      "Iteration 97, loss = 0.00304356\n",
      "Iteration 98, loss = 0.00304356\n",
      "Iteration 99, loss = 0.00304356\n",
      "Iteration 100, loss = 0.00304356\n",
      "Iteration 101, loss = 0.00304355\n",
      "Iteration 102, loss = 0.00304355\n",
      "Iteration 103, loss = 0.00304355\n",
      "Iteration 104, loss = 0.00304355\n",
      "Iteration 105, loss = 0.00304355\n",
      "Iteration 106, loss = 0.00304355\n",
      "Iteration 107, loss = 0.00304354\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000001\n",
      "Iteration 108, loss = 0.00304354\n",
      "Iteration 109, loss = 0.00304354\n",
      "Iteration 110, loss = 0.00304354\n",
      "Iteration 111, loss = 0.00304354\n",
      "Iteration 112, loss = 0.00304354\n",
      "Iteration 113, loss = 0.00304354\n",
      "Iteration 114, loss = 0.00304354\n",
      "Iteration 115, loss = 0.00304354\n",
      "Iteration 116, loss = 0.00304354\n",
      "Iteration 117, loss = 0.00304354\n",
      "Iteration 118, loss = 0.00304354\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
      "Iteration 119, loss = 0.00304353\n",
      "Iteration 120, loss = 0.00304353\n",
      "Iteration 121, loss = 0.00304353\n",
      "Iteration 122, loss = 0.00304353\n",
      "Iteration 123, loss = 0.00304353\n",
      "Iteration 124, loss = 0.00304353\n",
      "Iteration 125, loss = 0.00304353\n",
      "Iteration 126, loss = 0.00304353\n",
      "Iteration 127, loss = 0.00304353\n",
      "Iteration 128, loss = 0.00304353\n",
      "Iteration 129, loss = 0.00304353\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
      "CPU times: total: 42min 17s\n",
      "Wall time: 22min 30s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-5 {color: black;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(batch_size=64, hidden_layer_sizes=(512,),\n",
       "              learning_rate=&#x27;adaptive&#x27;, learning_rate_init=0.09, momentum=0.8,\n",
       "              solver=&#x27;sgd&#x27;, verbose=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" checked><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(batch_size=64, hidden_layer_sizes=(512,),\n",
       "              learning_rate=&#x27;adaptive&#x27;, learning_rate_init=0.09, momentum=0.8,\n",
       "              solver=&#x27;sgd&#x27;, verbose=True)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MLPClassifier(batch_size=64, hidden_layer_sizes=(512,),\n",
       "              learning_rate='adaptive', learning_rate_init=0.09, momentum=0.8,\n",
       "              solver='sgd', verbose=True)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "ada_nc_clf = MLPClassifier(hidden_layer_sizes=(512,), \n",
    "                    activation='relu', \n",
    "                    solver='sgd', \n",
    "                    nesterovs_momentum=True, momentum=0.8, \n",
    "                    alpha=0.0001, \n",
    "                    batch_size=64, \n",
    "                    learning_rate='adaptive', learning_rate_init=0.09, \n",
    "                    verbose=True)\n",
    "%time ada_nc_clf.fit(x_train, y_train)\n",
    "print(ada_nc_clf.loss_curve_)\n",
    "# [0.44681103581513676, 0.20019664490280964, 0.13903224539205372, 0.10424847724141931, 0.07820499920454431, 0.058752163212104365, 0.043410302283061385, 0.03309851304613901, 0.024876101979232398, 0.01929008625679405, 0.015819937931698343, 0.013119976535614704, 0.010973030088508333, 0.009540544547209177, 0.008448110407638392, 0.007664338032747635, 0.007013316780792199, 0.006486374157175714, 0.006095292770812613, 0.005675877345495901, 0.0054022429375176085, 0.00513810528301851, 0.004894300819477778, 0.0046914968425572304, 0.004521968420409352, 0.004382123050836891, 0.004225336735758417, 0.004105947003241883, 0.004022477045464209, 0.00391625698267327, 0.00381627076430297, 0.003734125082134867, 0.0036585145164223704, 0.0035892765943956887, 0.003535108147590659, 0.0034728889894520094, 0.003423243203272597, 0.0033613748221156515, 0.0033209246686070446, 0.0032750012111578595, 0.0032428164285514404, 0.0031472642626693514, 0.00313540047623953, 0.0031274728038711342, 0.0031212632827213012, 0.003114252978888586, 0.0031080337824674776, 0.003101702468251574, 0.0030953610962955085, 0.0030896990514066754, 0.0030833365283901883, 0.0030782073457771234, 0.0030617350283354996, 0.003060549141933265, 0.0030593253836724964, 0.0030581476059231704, 0.0030570752884987, 0.0030559063233432083, 0.0030547919126509217, 0.0030536457900612515, 0.003052595325926041, 0.003051489594663097, 0.003050402807525037, 0.003047089985460461, 0.0030468709101408916, 0.00304664551618608, 0.0030464373152081815, 0.0030462080789324364, 0.0030459880671757436, 0.0030457771840295507, 0.003045556586654198, 0.0030453388259683405, 0.003045121461271467, 0.0030449053634260008, 0.003044243744314005, 0.003044199859050067, 0.0030441554267542706, 0.003044113223247068, 0.0030440669153180746, 0.003044024809002121, 0.0030439809670705074, 0.003043937660844538, 0.003043893460652599, 0.0030438492201060137, 0.0030438063584988352, 0.0030436746449449643, 0.0030436661456755966, 0.003043657153375949, 0.0030436483079828943, 0.0030436399384663215, 0.003043631149097291, 0.0030436226278294947, 0.0030436140071533023, 0.0030436051023823587, 0.003043596657471788, 0.0030435877939500645, 0.0030435612724777723, 0.003043559480786767, 0.0030435576795176137, 0.0030435559909718557, 0.0030435543156908484, 0.0030435524919245087, 0.0030435508733789004, 0.0030435490561886423, 0.0030435473792563687, 0.0030435455905639722, 0.003043543878271781, 0.003043538543579084, 0.003043538195354039, 0.0030435378405210514, 0.003043537500316023, 0.003043537158119788, 0.0030435368009090824, 0.0030435364589390434, 0.0030435361249210625, 0.003043535774513943, 0.0030435354281969276, 0.003043535068763328, 0.003043534007049275, 0.0030435339380192826, 0.0030435338680986786, 0.003043533801818329, 0.0030435337274862567, 0.0030435336587934713, 0.003043533590966916, 0.0030435335231968684, 0.0030435334535933857, 0.0030435333838051744, 0.0030435333118708843]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params initialized\n",
      "Epoch 0/200\n",
      "----------\n",
      "Loss 2.2858\n",
      "Accuracy:  0.1419\n",
      "Validation Loss 2.2843\n",
      "Validation Accuracy:  0.1423\n",
      "Time : 0.0\n",
      "Epoch 10/200\n",
      "----------\n",
      "Loss 1.4711\n",
      "Accuracy:  0.6388\n",
      "Validation Loss 1.4738\n",
      "Validation Accuracy:  0.6418\n",
      "Time : 30.267032146453857\n",
      "Epoch 20/200\n",
      "----------\n",
      "Loss 1.0591\n",
      "Accuracy:  0.7058\n",
      "Validation Loss 1.0616\n",
      "Validation Accuracy:  0.7088\n",
      "Time : 60.39199757575989\n",
      "Epoch 30/200\n",
      "----------\n",
      "Loss 0.8980\n",
      "Accuracy:  0.7189\n",
      "Validation Loss 0.9004\n",
      "Validation Accuracy:  0.7182\n",
      "Time : 90.4540319442749\n",
      "Epoch 40/200\n",
      "----------\n",
      "Loss 0.8574\n",
      "Accuracy:  0.7201\n",
      "Validation Loss 0.8608\n",
      "Validation Accuracy:  0.7198\n",
      "Time : 120.69400072097778\n",
      "Update params\n",
      "Epoch 50/200\n",
      "----------\n",
      "Loss 0.8646\n",
      "Accuracy:  0.7194\n",
      "Validation Loss 0.8689\n",
      "Validation Accuracy:  0.7192\n",
      "Time : 150.87800192832947\n",
      "Epoch 60/200\n",
      "----------\n",
      "Loss 0.8651\n",
      "Accuracy:  0.7194\n",
      "Validation Loss 0.8695\n",
      "Validation Accuracy:  0.7192\n",
      "Time : 180.9679982662201\n",
      "Update params\n",
      "Epoch 70/200\n",
      "----------\n",
      "Loss 0.8655\n",
      "Accuracy:  0.7194\n",
      "Validation Loss 0.8699\n",
      "Validation Accuracy:  0.7192\n",
      "Time : 211.11399817466736\n",
      "Epoch 80/200\n",
      "----------\n",
      "Loss 0.8655\n",
      "Accuracy:  0.7194\n",
      "Validation Loss 0.8699\n",
      "Validation Accuracy:  0.7192\n",
      "Time : 241.50699853897095\n",
      "Update params\n",
      "Epoch 90/200\n",
      "----------\n",
      "Loss 0.8655\n",
      "Accuracy:  0.7194\n",
      "Validation Loss 0.8699\n",
      "Validation Accuracy:  0.7192\n",
      "Time : 272.12599778175354\n",
      "Epoch 100/200\n",
      "----------\n",
      "Loss 0.8655\n",
      "Accuracy:  0.7194\n",
      "Validation Loss 0.8699\n",
      "Validation Accuracy:  0.7192\n",
      "Time : 302.7079975605011\n",
      "Update params\n",
      "Epoch 110/200\n",
      "----------\n",
      "Loss 0.8655\n",
      "Accuracy:  0.7194\n",
      "Validation Loss 0.8699\n",
      "Validation Accuracy:  0.7192\n",
      "Time : 333.2039997577667\n",
      "Epoch 120/200\n",
      "----------\n",
      "Loss 0.8655\n",
      "Accuracy:  0.7194\n",
      "Validation Loss 0.8699\n",
      "Validation Accuracy:  0.7192\n",
      "Time : 364.145001411438\n",
      "Epoch 130/200\n",
      "----------\n",
      "Loss 0.8655\n",
      "Accuracy:  0.7194\n",
      "Validation Loss 0.8699\n",
      "Validation Accuracy:  0.7192\n",
      "Time : 394.9549980163574\n",
      "Update params\n",
      "Epoch 140/200\n",
      "----------\n",
      "Loss 0.8655\n",
      "Accuracy:  0.7194\n",
      "Validation Loss 0.8699\n",
      "Validation Accuracy:  0.7192\n",
      "Time : 425.84900188446045\n",
      "Epoch 150/200\n",
      "----------\n",
      "Loss 0.8655\n",
      "Accuracy:  0.7194\n",
      "Validation Loss 0.8699\n",
      "Validation Accuracy:  0.7192\n",
      "Time : 456.68799924850464\n",
      "Update params\n",
      "Epoch 160/200\n",
      "----------\n",
      "Loss 0.8655\n",
      "Accuracy:  0.7194\n",
      "Validation Loss 0.8699\n",
      "Validation Accuracy:  0.7192\n",
      "Time : 487.598997592926\n",
      "Epoch 170/200\n",
      "----------\n",
      "Loss 0.8655\n",
      "Accuracy:  0.7194\n",
      "Validation Loss 0.8699\n",
      "Validation Accuracy:  0.7192\n",
      "Time : 518.4030027389526\n",
      "Update params\n",
      "Epoch 180/200\n",
      "----------\n",
      "Loss 0.8655\n",
      "Accuracy:  0.7194\n",
      "Validation Loss 0.8699\n",
      "Validation Accuracy:  0.7192\n",
      "Time : 549.3689978122711\n",
      "Epoch 190/200\n",
      "----------\n",
      "Loss 0.8655\n",
      "Accuracy:  0.7194\n",
      "Validation Loss 0.8699\n",
      "Validation Accuracy:  0.7192\n",
      "Time : 580.5180022716522\n",
      "Update params\n",
      "{'loss': [2.285809944702769, 2.1870222830102386, 2.0915630006395327, 1.9996409742048808, 1.9114743796156684, 1.8272784570630272, 1.7472485460468397, 1.6715423653559294, 1.6002649858320686, 1.5334598696939774, 1.4711071081608666, 1.4131282052078478, 1.3593955795617254, 1.30974354445089, 1.263979621357407, 1.2218944609925615, 1.1832702817951712, 1.1478876608488873, 1.1155305458211386, 1.0859898626044415, 1.0590659894969812, 1.0345702338679021, 1.0123256456695247, 0.9921672550816566, 0.9739420438323478, 0.9575083609050405, 0.9427354923961337, 0.9295030512696834, 0.9177001502121884, 0.907224847925168, 0.8979833336632715, 0.8898893012453598, 0.8828633596216808, 0.8768323998627463, 0.8717290798506621, 0.8674913784565481, 0.8640620004294717, 0.8613880597280348, 0.8594207789337387, 0.8581149912727001, 0.8574288173946567, 0.8573235923522255, 0.8577634175605838, 0.8587150325436462, 0.8601474404029794, 0.8620320168800989, 0.8643419772339536, 0.8643972565713348, 0.864452714786062, 0.8645083496740361, 0.8645641627384413, 0.8646201556760423, 0.864676325893552, 0.8647326719106623, 0.864789195005782, 0.8648458940387677, 0.8649027677791273, 0.864959819092936, 0.8650170479476528, 0.8650744538057291, 0.8651320359958372, 0.8651897944971714, 0.8652477284638876, 0.865305837992999, 0.8653641228953396, 0.8654225816719869, 0.8654812155645116, 0.8655400240746309, 0.8655398482731771, 0.8655396724721995, 0.8655394966716983, 0.8655393208716732, 0.8655391450721244, 0.8655389692730517, 0.8655387934744558, 0.8655386176763491, 0.8655384418787186, 0.8655382660815643, 0.8655380902848862, 0.8655379144886747, 0.8655377386929387, 0.8655375628976792, 0.8655373871028967, 0.8655372113085896, 0.8655370355147596, 0.8655368597214047, 0.8655366839285261, 0.8655365081361237, 0.8655363323442081, 0.8655363323340043, 0.8655363323238005, 0.8655363323135963, 0.8655363323033924, 0.8655363322931886, 0.8655363322829845, 0.8655363322727808, 0.8655363322625768, 0.8655363322523728, 0.865536332242169, 0.8655363322319651, 0.865536332221761, 0.8655363322115572, 0.8655363322013532, 0.8655363321911493, 0.8655363321809455, 0.8655363321707414, 0.8655363321605376, 0.8655363321503335, 0.8655363321401298, 0.8655363321299259, 0.8655363321298583, 0.8655363321297906, 0.865536332129723, 0.8655363321296554, 0.8655363321295878, 0.8655363321295202, 0.8655363321294526, 0.8655363321293849, 0.8655363321293172, 0.8655363321292497, 0.8655363321291821, 0.8655363321291144, 0.865536332129047, 0.8655363321289794, 0.8655363321289116, 0.865536332128844, 0.8655363321287763, 0.8655363321287088, 0.8655363321286412, 0.8655363321285736, 0.8655363321285061, 0.8655363321285061, 0.8655363321285061, 0.8655363321285061, 0.8655363321285061, 0.8655363321285061, 0.8655363321285061, 0.8655363321285061, 0.8655363321285061, 0.8655363321285061, 0.8655363321285061, 0.8655363321285061, 0.8655363321285061, 0.8655363321285061, 0.8655363321285061, 0.8655363321285061, 0.8655363321285061, 0.8655363321285061, 0.8655363321285061, 0.8655363321285061, 0.8655363321285061, 0.8655363321285061, 0.8655363321285061, 0.8655363321285061, 0.8655363321285061, 0.8655363321285061, 0.8655363321285061, 0.8655363321285061, 0.8655363321285061, 0.8655363321285061, 0.8655363321285061, 0.8655363321285061, 0.8655363321285061, 0.8655363321285061, 0.8655363321285061, 0.8655363321285061, 0.8655363321285061, 0.8655363321285061, 0.8655363321285061, 0.8655363321285061, 0.8655363321285061, 0.8655363321285061, 0.8655363321285061, 0.8655363321285061, 0.8655363321285061, 0.8655363321285061, 0.8655363321285061, 0.8655363321285061, 0.8655363321285061, 0.8655363321285061, 0.8655363321285061, 0.8655363321285061, 0.8655363321285061, 0.8655363321285061, 0.8655363321285061, 0.8655363321285061, 0.8655363321285061, 0.8655363321285061, 0.8655363321285061, 0.8655363321285061, 0.8655363321285061, 0.8655363321285061, 0.8655363321285061, 0.8655363321285061, 0.8655363321285061, 0.8655363321285061, 0.8655363321285061, 0.8655363321285061, 0.8655363321285061, 0.8655363321285061], 'accuracy': [0.1419375, 0.19866666666666666, 0.2931875, 0.38347916666666665, 0.45047916666666665, 0.5032916666666667, 0.5468958333333334, 0.579875, 0.6044583333333333, 0.6243333333333333, 0.6387708333333333, 0.6516666666666666, 0.6621875, 0.6713958333333333, 0.6782916666666666, 0.6857291666666666, 0.691125, 0.6959791666666667, 0.6998125, 0.7027916666666667, 0.7057916666666667, 0.7085208333333334, 0.7103958333333333, 0.7116458333333333, 0.7134166666666667, 0.714875, 0.7155, 0.7163958333333333, 0.7170416666666667, 0.7180833333333333, 0.718875, 0.719125, 0.7195208333333334, 0.71975, 0.7198125, 0.7197083333333333, 0.7202083333333333, 0.7202916666666667, 0.7202708333333333, 0.720375, 0.720125, 0.7201875, 0.7201875, 0.7200208333333333, 0.7199583333333334, 0.7196875, 0.719375, 0.7193541666666666, 0.7193541666666666, 0.7193541666666666, 0.719375, 0.719375, 0.7193541666666666, 0.719375, 0.7193958333333333, 0.7193958333333333, 0.719375, 0.7193958333333333, 0.7193958333333333, 0.7194583333333333, 0.7194166666666667, 0.7194166666666667, 0.719375, 0.7193958333333333, 0.7193958333333333, 0.7193958333333333, 0.7193958333333333, 0.7193958333333333, 0.7193958333333333, 0.7193958333333333, 0.7193958333333333, 0.7193958333333333, 0.7193958333333333, 0.7193958333333333, 0.7193958333333333, 0.7193958333333333, 0.7193958333333333, 0.7193958333333333, 0.7193958333333333, 0.7193958333333333, 0.7193958333333333, 0.7193958333333333, 0.7193958333333333, 0.7193958333333333, 0.7193958333333333, 0.7193958333333333, 0.7193958333333333, 0.7193958333333333, 0.7193958333333333, 0.7193958333333333, 0.7193958333333333, 0.7193958333333333, 0.7193958333333333, 0.7193958333333333, 0.7193958333333333, 0.7193958333333333, 0.7193958333333333, 0.7193958333333333, 0.7193958333333333, 0.7193958333333333, 0.7193958333333333, 0.7193958333333333, 0.7193958333333333, 0.7193958333333333, 0.7193958333333333, 0.7193958333333333, 0.7193958333333333, 0.7193958333333333, 0.7193958333333333, 0.7193958333333333, 0.7193958333333333, 0.7193958333333333, 0.7193958333333333, 0.7193958333333333, 0.7193958333333333, 0.7193958333333333, 0.7193958333333333, 0.7193958333333333, 0.7193958333333333, 0.7193958333333333, 0.7193958333333333, 0.7193958333333333, 0.7193958333333333, 0.7193958333333333, 0.7193958333333333, 0.7193958333333333, 0.7193958333333333, 0.7193958333333333, 0.7193958333333333, 0.7193958333333333, 0.7193958333333333, 0.7193958333333333, 0.7193958333333333, 0.7193958333333333, 0.7193958333333333, 0.7193958333333333, 0.7193958333333333, 0.7193958333333333, 0.7193958333333333, 0.7193958333333333, 0.7193958333333333, 0.7193958333333333, 0.7193958333333333, 0.7193958333333333, 0.7193958333333333, 0.7193958333333333, 0.7193958333333333, 0.7193958333333333, 0.7193958333333333, 0.7193958333333333, 0.7193958333333333, 0.7193958333333333, 0.7193958333333333, 0.7193958333333333, 0.7193958333333333, 0.7193958333333333, 0.7193958333333333, 0.7193958333333333, 0.7193958333333333, 0.7193958333333333, 0.7193958333333333, 0.7193958333333333, 0.7193958333333333, 0.7193958333333333, 0.7193958333333333, 0.7193958333333333, 0.7193958333333333, 0.7193958333333333, 0.7193958333333333, 0.7193958333333333, 0.7193958333333333, 0.7193958333333333, 0.7193958333333333, 0.7193958333333333, 0.7193958333333333, 0.7193958333333333, 0.7193958333333333, 0.7193958333333333, 0.7193958333333333, 0.7193958333333333, 0.7193958333333333, 0.7193958333333333, 0.7193958333333333, 0.7193958333333333, 0.7193958333333333, 0.7193958333333333, 0.7193958333333333, 0.7193958333333333, 0.7193958333333333, 0.7193958333333333, 0.7193958333333333, 0.7193958333333333, 0.7193958333333333, 0.7193958333333333, 0.7193958333333333, 0.7193958333333333, 0.7193958333333333, 0.7193958333333333, 0.7193958333333333, 0.7193958333333333], 'v_loss': [2.2843033377124837, 2.1860935980092746, 2.091189784520564, 1.9997960168890176, 1.9121255051949442, 1.8283882096329995, 1.7487737803788543, 1.6734339883859775, 1.6024683551693972, 1.535916294537206, 1.4737564596900565, 1.4159121166498183, 1.362260509518855, 1.312643121944021, 1.2668760147637927, 1.224758792744733, 1.1860822877007062, 1.1506348293015232, 1.118207008098665, 1.0885951988929519, 1.0616040664774256, 1.0370481677775603, 1.0147529042011756, 0.9945549176693016, 0.9763021987873614, 0.9598536353079963, 0.9450786850212779, 0.9318568603779304, 0.9200769687157944, 0.9096366306630116, 0.9004414978829436, 0.8924046717531683, 0.8854461361291733, 0.8794921419004061, 0.8744747192186164, 0.870331233003485, 0.8670037929825042, 0.8644389521744198, 0.8625874069525986, 0.861403495777967, 0.8608448757225974, 0.8608724532012202, 0.8614499317272233, 0.8625436841335086, 0.864122383515312, 0.866157089883684, 0.8686207403289081, 0.8686792555111327, 0.8687379510414988, 0.8687968242914876, 0.8688558771665595, 0.8689151111531959, 0.868974523953367, 0.8690341139991259, 0.8690938826395618, 0.8691538287278584, 0.8692139509349095, 0.8692742523241462, 0.8693347327767609, 0.869395391578962, 0.869456228200873, 0.8695172425030506, 0.8695784337059308, 0.8696398019205437, 0.8697013469184379, 0.8697630672350178, 0.8698249641199163, 0.8698870369842231, 0.8698868714250475, 0.8698867058663434, 0.8698865403081114, 0.8698863747503511, 0.8698862091930628, 0.8698860436362464, 0.8698858780799021, 0.8698857125240445, 0.8698855469686585, 0.8698853814137444, 0.8698852158593022, 0.8698850503053214, 0.8698848847518111, 0.8698847191987732, 0.869884553646208, 0.8698843880941134, 0.8698842225424921, 0.8698840569913412, 0.8698838914406622, 0.8698837258904552, 0.8698835603407318, 0.869883560327941, 0.8698835603151505, 0.8698835603023598, 0.869883560289569, 0.8698835602767786, 0.869883560263988, 0.8698835602511972, 0.8698835602384066, 0.8698835602256159, 0.8698835602128253, 0.8698835602000347, 0.869883560187244, 0.8698835601744535, 0.8698835601616628, 0.869883560148872, 0.8698835601360814, 0.8698835601232908, 0.8698835601105002, 0.8698835600977096, 0.8698835600849188, 0.8698835600721283, 0.8698835600720524, 0.8698835600719764, 0.8698835600719005, 0.8698835600718245, 0.8698835600717487, 0.8698835600716728, 0.8698835600715968, 0.869883560071521, 0.869883560071445, 0.869883560071369, 0.8698835600712931, 0.8698835600712173, 0.8698835600711412, 0.8698835600710654, 0.8698835600709897, 0.8698835600709135, 0.8698835600708376, 0.8698835600707616, 0.8698835600706858, 0.8698835600706097, 0.8698835600705339, 0.8698835600705339, 0.8698835600705339, 0.8698835600705339, 0.8698835600705339, 0.8698835600705339, 0.8698835600705339, 0.8698835600705339, 0.8698835600705339, 0.8698835600705339, 0.8698835600705339, 0.8698835600705339, 0.8698835600705339, 0.8698835600705339, 0.8698835600705339, 0.8698835600705339, 0.8698835600705339, 0.8698835600705339, 0.8698835600705339, 0.8698835600705339, 0.8698835600705339, 0.8698835600705339, 0.8698835600705339, 0.8698835600705339, 0.8698835600705339, 0.8698835600705339, 0.8698835600705339, 0.8698835600705339, 0.8698835600705339, 0.8698835600705339, 0.8698835600705339, 0.8698835600705339, 0.8698835600705339, 0.8698835600705339, 0.8698835600705339, 0.8698835600705339, 0.8698835600705339, 0.8698835600705339, 0.8698835600705339, 0.8698835600705339, 0.8698835600705339, 0.8698835600705339, 0.8698835600705339, 0.8698835600705339, 0.8698835600705339, 0.8698835600705339, 0.8698835600705339, 0.8698835600705339, 0.8698835600705339, 0.8698835600705339, 0.8698835600705339, 0.8698835600705339, 0.8698835600705339, 0.8698835600705339, 0.8698835600705339, 0.8698835600705339, 0.8698835600705339, 0.8698835600705339, 0.8698835600705339, 0.8698835600705339, 0.8698835600705339, 0.8698835600705339, 0.8698835600705339, 0.8698835600705339, 0.8698835600705339, 0.8698835600705339, 0.8698835600705339, 0.8698835600705339, 0.8698835600705339, 0.8698835600705339], 'v_accuracy': [0.14233333333333334, 0.20533333333333334, 0.30091666666666667, 0.3898333333333333, 0.4529166666666667, 0.5068333333333334, 0.5494166666666667, 0.5805833333333333, 0.6053333333333333, 0.6268333333333334, 0.64175, 0.6555833333333333, 0.6654166666666667, 0.6758333333333333, 0.6836666666666666, 0.6915833333333333, 0.696, 0.70025, 0.7040833333333333, 0.7066666666666667, 0.7088333333333333, 0.7116666666666667, 0.7136666666666667, 0.715, 0.7159166666666666, 0.7175833333333334, 0.7175833333333334, 0.7170833333333333, 0.7179166666666666, 0.7184166666666667, 0.7181666666666666, 0.71825, 0.7185833333333334, 0.7191666666666666, 0.7193333333333334, 0.7194166666666667, 0.7196666666666667, 0.7199166666666666, 0.7201666666666666, 0.71975, 0.7198333333333333, 0.71975, 0.7195833333333334, 0.71925, 0.71925, 0.7191666666666666, 0.7191666666666666, 0.7191666666666666, 0.7191666666666666, 0.7191666666666666, 0.71925, 0.71925, 0.7191666666666666, 0.7191666666666666, 0.7191666666666666, 0.7191666666666666, 0.7191666666666666, 0.7191666666666666, 0.71925, 0.71925, 0.71925, 0.71925, 0.71925, 0.71925, 0.71925, 0.7191666666666666, 0.7191666666666666, 0.7191666666666666, 0.7191666666666666, 0.7191666666666666, 0.7191666666666666, 0.7191666666666666, 0.7191666666666666, 0.7191666666666666, 0.7191666666666666, 0.7191666666666666, 0.7191666666666666, 0.7191666666666666, 0.7191666666666666, 0.7191666666666666, 0.7191666666666666, 0.7191666666666666, 0.7191666666666666, 0.7191666666666666, 0.7191666666666666, 0.7191666666666666, 0.7191666666666666, 0.7191666666666666, 0.7191666666666666, 0.7191666666666666, 0.7191666666666666, 0.7191666666666666, 0.7191666666666666, 0.7191666666666666, 0.7191666666666666, 0.7191666666666666, 0.7191666666666666, 0.7191666666666666, 0.7191666666666666, 0.7191666666666666, 0.7191666666666666, 0.7191666666666666, 0.7191666666666666, 0.7191666666666666, 0.7191666666666666, 0.7191666666666666, 0.7191666666666666, 0.7191666666666666, 0.7191666666666666, 0.7191666666666666, 0.7191666666666666, 0.7191666666666666, 0.7191666666666666, 0.7191666666666666, 0.7191666666666666, 0.7191666666666666, 0.7191666666666666, 0.7191666666666666, 0.7191666666666666, 0.7191666666666666, 0.7191666666666666, 0.7191666666666666, 0.7191666666666666, 0.7191666666666666, 0.7191666666666666, 0.7191666666666666, 0.7191666666666666, 0.7191666666666666, 0.7191666666666666, 0.7191666666666666, 0.7191666666666666, 0.7191666666666666, 0.7191666666666666, 0.7191666666666666, 0.7191666666666666, 0.7191666666666666, 0.7191666666666666, 0.7191666666666666, 0.7191666666666666, 0.7191666666666666, 0.7191666666666666, 0.7191666666666666, 0.7191666666666666, 0.7191666666666666, 0.7191666666666666, 0.7191666666666666, 0.7191666666666666, 0.7191666666666666, 0.7191666666666666, 0.7191666666666666, 0.7191666666666666, 0.7191666666666666, 0.7191666666666666, 0.7191666666666666, 0.7191666666666666, 0.7191666666666666, 0.7191666666666666, 0.7191666666666666, 0.7191666666666666, 0.7191666666666666, 0.7191666666666666, 0.7191666666666666, 0.7191666666666666, 0.7191666666666666, 0.7191666666666666, 0.7191666666666666, 0.7191666666666666, 0.7191666666666666, 0.7191666666666666, 0.7191666666666666, 0.7191666666666666, 0.7191666666666666, 0.7191666666666666, 0.7191666666666666, 0.7191666666666666, 0.7191666666666666, 0.7191666666666666, 0.7191666666666666, 0.7191666666666666, 0.7191666666666666, 0.7191666666666666, 0.7191666666666666, 0.7191666666666666, 0.7191666666666666, 0.7191666666666666, 0.7191666666666666, 0.7191666666666666, 0.7191666666666666, 0.7191666666666666, 0.7191666666666666, 0.7191666666666666, 0.7191666666666666, 0.7191666666666666, 0.7191666666666666, 0.7191666666666666, 0.7191666666666666, 0.7191666666666666, 0.7191666666666666, 0.7191666666666666, 0.7191666666666666], 'time': [1704021957.2374508, 1704021960.5244508, 1704021963.5614858, 1704021966.5524487, 1704021969.5464487, 1704021972.527449, 1704021975.5124505, 1704021978.4974487, 1704021981.514485, 1704021984.5044487, 1704021987.504483, 1704021990.4754858, 1704021993.4814498, 1704021996.4814487, 1704021999.4974496, 1704022002.481449, 1704022005.4894524, 1704022008.4794507, 1704022011.6444497, 1704022014.6604488, 1704022017.6294484, 1704022020.6114485, 1704022023.6134486, 1704022026.5924509, 1704022029.5914853, 1704022032.5434847, 1704022035.5274496, 1704022038.50845, 1704022041.682451, 1704022044.6984487, 1704022047.6914828, 1704022050.6784852, 1704022053.6904485, 1704022056.6834826, 1704022059.668449, 1704022062.7024848, 1704022065.8174837, 1704022068.8744552, 1704022071.9614542, 1704022074.9594495, 1704022077.9314516, 1704022080.936485, 1704022083.9884863, 1704022086.9894488, 1704022089.9854496, 1704022092.9864514, 1704022095.9974852, 1704022099.0184836, 1704022102.083483, 1704022105.0994482, 1704022108.1154528, 1704022111.1144536, 1704022114.1034875, 1704022117.1104841, 1704022120.1114507, 1704022123.0994496, 1704022126.0994508, 1704022129.1064854, 1704022132.1934488, 1704022135.2074485, 1704022138.205449, 1704022141.2304485, 1704022144.2724857, 1704022147.2664487, 1704022150.2864487, 1704022153.291449, 1704022156.3024533, 1704022159.3144495, 1704022162.3404489, 1704022165.3464487, 1704022168.351449, 1704022171.3594496, 1704022174.384453, 1704022177.3864837, 1704022180.4464812, 1704022183.5084498, 1704022186.5194535, 1704022189.5564501, 1704022192.6824512, 1704022195.7164485, 1704022198.7444494, 1704022201.8144522, 1704022204.8564491, 1704022207.9384499, 1704022211.0374498, 1704022214.07145, 1704022217.160449, 1704022220.198486, 1704022223.2654488, 1704022226.3054838, 1704022229.3634486, 1704022232.370451, 1704022235.439449, 1704022238.4814527, 1704022241.5684845, 1704022244.618487, 1704022247.6444488, 1704022250.6984544, 1704022253.7844539, 1704022256.8874483, 1704022259.9454484, 1704022263.025449, 1704022266.0514872, 1704022269.090449, 1704022272.1554513, 1704022275.231449, 1704022278.2834501, 1704022281.3194485, 1704022284.3534484, 1704022287.3794498, 1704022290.4414506, 1704022293.491449, 1704022296.5544524, 1704022299.5864518, 1704022302.6574495, 1704022305.7324488, 1704022308.9304485, 1704022312.0274491, 1704022315.0864484, 1704022318.14845, 1704022321.3824522, 1704022324.4544508, 1704022327.5704792, 1704022330.6304483, 1704022333.7434485, 1704022336.84045, 1704022339.8874495, 1704022342.952453, 1704022346.0554485, 1704022349.1254516, 1704022352.1924489, 1704022355.2394502, 1704022358.3394504, 1704022361.4284484, 1704022364.5304883, 1704022367.6024487, 1704022370.6754487, 1704022373.8234844, 1704022376.9064815, 1704022380.01745, 1704022383.0864527, 1704022386.138449, 1704022389.221454, 1704022392.2844486, 1704022395.37345, 1704022398.469484, 1704022401.5414486, 1704022404.6414518, 1704022407.71545, 1704022410.7874494, 1704022413.92545, 1704022416.9994538, 1704022420.0654497, 1704022423.1714485, 1704022426.2844486, 1704022429.3824852, 1704022432.4864483, 1704022435.5794485, 1704022438.65845, 1704022441.771449, 1704022444.8364484, 1704022447.8804858, 1704022450.9634514, 1704022454.101451, 1704022457.1884487, 1704022460.2364492, 1704022463.3324487, 1704022466.4124842, 1704022469.4794493, 1704022472.5534499, 1704022475.6404536, 1704022478.7044845, 1704022481.7864492, 1704022484.8754902, 1704022488.0214882, 1704022491.1424882, 1704022494.3074887, 1704022497.3834505, 1704022500.4514499, 1704022503.5084846, 1704022506.6064487, 1704022509.6844485, 1704022512.7604492, 1704022515.9194527, 1704022519.0114486, 1704022522.1244497, 1704022525.2574503, 1704022528.3774502, 1704022531.5324507, 1704022534.6494865, 1704022537.755453, 1704022540.8864484, 1704022543.9604816, 1704022547.0414486, 1704022550.190451, 1704022553.326451, 1704022556.4294503, 1704022559.5414526, 1704022562.6564498, 1704022565.7824867, 1704022568.8674872]}\n",
      "CPU times: total: 11min 48s\n",
      "Wall time: 10min 11s\n"
     ]
    }
   ],
   "source": [
    "ada_nc_model = MLP_2_Layer_Softmax(M=128, num_classes=10, rsag=True)\n",
    "optimizer = RSAG(learning_rate=.09, alpha=0.009, beta=0.00009,\n",
    "                lr_fn=lr_lamda, alpha_fn=lr_lamda, beta_fn=lr_lamda, \n",
    "                start_adap=49)\n",
    "%time ada_nc_model, ada_nc_log, acc_max = train_model(ada_nc_model, optimizer, x_train, y_train, x_valid, y_valid, print_every=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nesterov's Momentum:  0.9662\n",
      "RSAG:  0.7395\n"
     ]
    }
   ],
   "source": [
    "print('Nesterov\\'s Momentum: ', accuracy_score(y_test, ada_nc_clf.predict(x_test)))\n",
    "print('RSAG: ', evaluate_acc(y_test, ada_nc_model.predict(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_accuracies([ada_nc_clf.loss_curve_, log['v_loss']], ['NAG', 'RSAG'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_accuracies(logs, labels):\n",
    "    for log, label in zip(logs,labels):\n",
    "        plt.plot(range(len(log)), log, label=label)\n",
    "        plt.xlabel('Epoch (s)')\n",
    "        plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_accuracies_vs_time(logs):\n",
    "    for log in logs:\n",
    "        plt.plot(log['v_accuracy'], log['time'], label=log['label'])\n",
    "        plt.xlabel('Time (s)')\n",
    "        plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_accuracies(log['v_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_accuracies(log['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#earning_rate=.006, alpha=0.009, beta=0.000009 {'loss': [2.5160519958632497, 2.4973575056239055, 2.478812720935548, 2.460418367816293, 2.4421750728527756, 2.424083243209927, 2.406143218943164, 2.388355619024832, 2.370720531779371, 2.3532388507830846, 2.3359103385579245, 2.3187356583615517, 2.3017151449607045, 2.284848798003, 2.268137113025858, 2.251580131464062, 2.2351781560975392, 2.2189313277624816, 2.2028397324627114, 2.186903482608769, 2.171122576020296, 2.1554971802396707, 2.1400273974261985, 2.12471322420883, 2.1095543291442924, 2.0945509674032827, 2.0797030372028, 2.0650101785621415, 2.0504724696688656, 2.0360895385965785, 2.021861265377499, 2.007787395317621, 1.9938677452480706, 1.9801019385579555, 1.9664896459769876, 1.9530305341659135, 1.9397243989520938, 1.9265708088428968, 1.913569264293767, 1.9007191856791694, 1.8880201770698002, 1.8754717765074902, 1.8630734839007053, 1.8508246629653273, 1.8387247167204983, 1.8267731012263388, 1.8149689028182607, 1.8033118168215017, 1.7918008676058652, 1.780435443199607, 1.7692147867065802, 1.758138262094848, 1.7472050244117556, 1.736414184032042, 1.725764925254068, 1.7152563634988953, 1.704887640485752, 1.6946577872277346, 1.6845660798988868, 1.6746114205550002, 1.6647928081632135, 1.655109453942081, 1.6455602044679913, 1.63614411612935, 1.6268601594469219, 1.6177073423341959, 1.6086844168984595, 1.5997904959232623, 1.5910244241810505, 1.5823851782440828, 1.573871518177679, 1.5654824421577596, 1.557216748641881, 1.5490733628087985, 1.5410510119995289, 1.5331486888063552, 1.5253652436822294, 1.5176995752563607, 1.5101503292857068, 1.5027163181320302, 1.4953964058131015, 1.4881894832016638, 1.481094316008594, 1.4741097819182238, 1.4672346230198323, 1.4604677039683713, 1.453807736800002, 1.4472536140469547, 1.440804143388772, 1.4344579918486622, 1.4282141241856425, 1.4220714320677037, 1.4160285570580309, 1.4100845667564426, 1.4042380347574344, 1.3984878527799462, 1.392832983561538, 1.3872721741401781, 1.3818042746328631, 1.3764282049267276, 1.3711428537472001, 1.3659470475748883, 1.3608397097045988, 1.3558197165376955, 1.350885922540901, 1.3460371923663124, 1.3412725411089275, 1.336590902774661, 1.3319911511496636, 1.3274722697497163, 1.3230331517395404, 1.3186727883587195, 1.3143902311457627, 1.3101844006747139, 1.3060542439016587, 1.3019988505028786, 1.2980171772661853, 1.2941082629039047, 1.2902711788933614, 1.2865049123578411, 1.2828085680651276, 1.2791812381756231, 1.2756219102534354, 1.2721297229491526, 1.2687038625549176, 1.2653433682122233, 1.2620473589301207, 1.2588149634015926, 1.2556453335623932, 1.2525376170849052, 1.2494910262907375, 1.246504696143371, 1.243577830703593, 1.2407096924293133, 1.237899363279785, 1.2351462232563044, 1.2324494032576179, 1.229808122809259, 1.2272217424748364, 1.2246893998625108, 1.2222104309766801, 1.2197840963234217, 1.217409798994363, 1.2150867337041764, 1.2128142780481603, 1.210591738654992, 1.2084184182913702, 1.206293681621064, 1.2042168761799936, 1.2021873765389202, 1.2002045784726234, 1.1982678221929342, 1.196376580417403, 1.1945301193133262, 1.1927279806807676, 1.1909695424957498], 'accuracy': [4.3, 4.75, 5.341666666666667, 6.0125, 6.716666666666667, 7.477083333333334, 8.441666666666666, 9.40625, 10.4625, 11.641666666666667, 12.902083333333334, 14.0375, 15.26875, 16.56875, 17.841666666666665, 19.060416666666665, 20.266666666666666, 21.464583333333334, 22.55, 23.65625, 24.722916666666666, 25.739583333333332, 26.658333333333335, 27.55625, 28.470833333333335, 29.266666666666666, 30.05, 30.80625, 31.525, 32.172916666666666, 32.829166666666666, 33.43541666666667, 34.08125, 34.6875, 35.295833333333334, 35.80416666666667, 36.35208333333333, 36.87291666666667, 37.33541666666667, 37.81875, 38.385416666666664, 38.81666666666667, 39.24166666666667, 39.78333333333333, 40.22708333333333, 40.6125, 41.02291666666667, 41.4625, 41.87708333333333, 42.24375, 42.66875, 42.99375, 43.34375, 43.729166666666664, 44.1375, 44.43333333333333, 44.73125, 45.083333333333336, 45.395833333333336, 45.71041666666667, 45.989583333333336, 46.31875, 46.610416666666666, 46.895833333333336, 47.1875, 47.45625, 47.77916666666667, 48.0625, 48.31875, 48.614583333333336, 48.88125, 49.175, 49.43333333333333, 49.735416666666666, 50.07083333333333, 50.28958333333333, 50.49583333333333, 50.725, 50.96666666666667, 51.15625, 51.40625, 51.68333333333333, 51.90833333333333, 52.135416666666664, 52.3125, 52.53541666666667, 52.7375, 52.979166666666664, 53.29375, 53.49375, 53.708333333333336, 53.983333333333334, 54.22083333333333, 54.46041666666667, 54.65833333333333, 54.86875, 55.05625, 55.2125, 55.416666666666664, 55.55833333333333, 55.74166666666667, 55.927083333333336, 56.15833333333333, 56.30416666666667, 56.458333333333336, 56.58958333333333, 56.704166666666666, 56.86875, 57.05416666666667, 57.204166666666666, 57.33125, 57.43958333333333, 57.56041666666667, 57.670833333333334, 57.791666666666664, 57.90625, 58.00208333333333, 58.12708333333333, 58.24166666666667, 58.34583333333333, 58.454166666666666, 58.575, 58.7, 58.81458333333333, 58.9375, 59.06041666666667, 59.1625, 59.2375, 59.327083333333334, 59.427083333333336, 59.516666666666666, 59.577083333333334, 59.6375, 59.72291666666667, 59.78958333333333, 59.85625, 59.94166666666667, 60.03333333333333, 60.11666666666667, 60.15416666666667, 60.208333333333336, 60.260416666666664, 60.322916666666664, 60.39375, 60.43125, 60.49166666666667, 60.55833333333333, 60.63125, 60.677083333333336, 60.73125, 60.77708333333333, 60.83125, 60.90416666666667, 60.94583333333333, 60.979166666666664, 61.01875], 'v_loss': [2.5148063596832437, 2.49615115174158, 2.477645865095786, 2.459291206852179, 2.4410877930084904, 2.423036027156197, 2.4051362327818278, 2.3873890088997767, 2.369794446786681, 2.3523534307749725, 2.3350657023587504, 2.317931923993111, 2.3009524178478657, 2.2841271702453367, 2.267456656406367, 2.2509409248654566, 2.234580261467171, 2.218374792645572, 2.2023246052354963, 2.1864297853873835, 2.1706903345144157, 2.1551064139866174, 2.1396780997641, 2.1244053980517505, 2.109287957373581, 2.0943260345083266, 2.0795195103309645, 2.064868031715993, 2.050371651931761, 2.036030004501688, 2.0218429533263245, 2.007810240884165, 1.9939316872527018, 1.9802069020080229, 1.9666355495342172, 1.9532172950622861, 1.9399519304682387, 1.9268390158325168, 1.9138780509548645, 1.9010684523960792, 1.8884098183659395, 1.8759016882093311, 1.8635435585499627, 1.8513347910801574, 1.839274781648382, 1.8273629929940471, 1.8155985062359348, 1.8039810073094362, 1.7925095340134105, 1.7811834604671386, 1.7700020314668363, 1.7589646178779323, 1.7480703688756496, 1.7373183974596424, 1.7267078812669163, 1.716237935998055, 1.7059077065490273, 1.695716225615338, 1.6856627558836956, 1.6757462147164703, 1.6659655958825723, 1.6563201089086288, 1.646808603382893, 1.6374301328060965, 1.6281836675477173, 1.6190682167844326, 1.6100825409944048, 1.6012257423668843, 1.5924966624999113, 1.5838942853757854, 1.575417371423427, 1.5670649123915237, 1.5588357061659899, 1.550728687265933, 1.5427425655045626, 1.5348763544970312, 1.527128887522649, 1.519499066906007, 1.5119855398998892, 1.5045871159380524, 1.4973026538189615, 1.490131047626756, 1.4830710600306156, 1.4761215594216688, 1.4692813007763483, 1.4625491384682816, 1.4559237849669846, 1.4494041271107097, 1.4429889762008965, 1.4366769980378882, 1.4304671523642343, 1.4243583229909194, 1.4183491562713653, 1.4124387104299518, 1.4066255652717912, 1.4009086091516714, 1.3952867979658947, 1.3897588798909377, 1.3843236980094316, 1.3789801723933008, 1.3737271927204455, 1.3685635673111576, 1.3634882249170845, 1.3585000468824315, 1.3535978732176912, 1.3487805759161873, 1.3440471659343356, 1.3393965670122039, 1.334827656155502, 1.3303394145720735, 1.3259307271746612, 1.3216005858707518, 1.317348036777376, 1.3131720032899277, 1.309071424047942, 1.3050453893105254, 1.3010928511784616, 1.2972128510397276, 1.2934044490677945, 1.2896666332736608, 1.2859985116232655, 1.2823991680248932, 1.2788675915936247, 1.2754029178727024, 1.2720043281895772, 1.268670861721328, 1.265401637638159, 1.2621957819253145, 1.2590524413181323, 1.2559707630679149, 1.2529499631767145, 1.2499891689371267, 1.2470875881754835, 1.244244475245979, 1.2414589177784916, 1.2387302947582481, 1.236057727017808, 1.2334404448858396, 1.2308778046353446, 1.2283689407638467, 1.225913188960611, 1.2235098051851063, 1.2211581990139224, 1.2188575695573516, 1.2166072748931558, 1.2144066429936986, 1.2122549648512528, 1.2101516076721892, 1.2080959183405033, 1.2060872707084564, 1.2041250600952305, 1.2022086331282513, 1.200337453655215, 1.1985107994263455, 1.196728201150955, 1.1949890517948278], 'v_accuracy': [4.375, 4.766666666666667, 5.208333333333333, 5.958333333333333, 6.583333333333333, 7.383333333333334, 8.191666666666666, 9.325, 10.258333333333333, 11.541666666666666, 12.758333333333333, 13.975, 15.366666666666667, 16.716666666666665, 17.933333333333334, 19.316666666666666, 20.608333333333334, 22.0, 23.316666666666666, 24.391666666666666, 25.433333333333334, 26.55, 27.45, 28.258333333333333, 29.241666666666667, 30.175, 30.975, 31.691666666666666, 32.5, 33.21666666666667, 33.833333333333336, 34.333333333333336, 35.03333333333333, 35.45, 36.15833333333333, 36.74166666666667, 37.275, 37.74166666666667, 38.375, 38.725, 39.05833333333333, 39.55833333333333, 40.00833333333333, 40.6, 41.00833333333333, 41.50833333333333, 41.90833333333333, 42.28333333333333, 42.541666666666664, 42.96666666666667, 43.25, 43.608333333333334, 43.96666666666667, 44.4, 44.775, 45.15833333333333, 45.45, 45.75833333333333, 46.108333333333334, 46.31666666666667, 46.6, 46.958333333333336, 47.225, 47.475, 47.666666666666664, 47.94166666666667, 48.25, 48.375, 48.65, 48.916666666666664, 49.25, 49.44166666666667, 49.775, 49.96666666666667, 50.21666666666667, 50.50833333333333, 50.725, 50.94166666666667, 51.15, 51.475, 51.625, 51.833333333333336, 52.016666666666666, 52.233333333333334, 52.46666666666667, 52.641666666666666, 52.858333333333334, 53.083333333333336, 53.35, 53.575, 53.81666666666667, 53.958333333333336, 54.19166666666667, 54.40833333333333, 54.59166666666667, 54.71666666666667, 54.88333333333333, 55.016666666666666, 55.175, 55.36666666666667, 55.53333333333333, 55.666666666666664, 55.86666666666667, 56.083333333333336, 56.25833333333333, 56.458333333333336, 56.583333333333336, 56.666666666666664, 56.75, 56.84166666666667, 57.09166666666667, 57.233333333333334, 57.375, 57.55833333333333, 57.583333333333336, 57.675, 57.75833333333333, 57.925, 58.06666666666667, 58.275, 58.40833333333333, 58.38333333333333, 58.416666666666664, 58.50833333333333, 58.541666666666664, 58.641666666666666, 58.68333333333333, 58.775, 58.825, 58.88333333333333, 58.958333333333336, 59.025, 59.09166666666667, 59.13333333333333, 59.233333333333334, 59.31666666666667, 59.4, 59.458333333333336, 59.53333333333333, 59.625, 59.7, 59.74166666666667, 59.791666666666664, 59.833333333333336, 59.9, 59.925, 59.958333333333336, 59.975, 60.03333333333333, 60.11666666666667, 60.18333333333333, 60.225, 60.275, 60.275, 60.333333333333336, 60.36666666666667], 'time': [1703873184.3395016, 1703873188.1885018, 1703873191.74653, 1703873195.4125347, 1703873199.2805028, 1703873202.8574984, 1703873206.4135027, 1703873210.0285332, 1703873213.625532, 1703873217.2345355, 1703873220.821503, 1703873224.4305027, 1703873228.0215025, 1703873231.798522, 1703873235.4224987, 1703873239.0034986, 1703873242.5375032, 1703873246.0975373, 1703873249.9974995, 1703873254.0204985, 1703873258.798502, 1703873262.692498, 1703873266.701503, 1703873270.7034998, 1703873274.7695308, 1703873278.6334984, 1703873282.3705359, 1703873286.2105021, 1703873290.109498, 1703873293.7004986, 1703873297.371504, 1703873301.2295046, 1703873304.8285036, 1703873308.6995306, 1703873312.2875292, 1703873315.8975368, 1703873319.5065024, 1703873323.0665357, 1703873326.6325352, 1703873330.499538, 1703873334.1755025, 1703873337.9205039, 1703873341.6715, 1703873345.343499, 1703873348.9994996, 1703873352.5794983, 1703873356.132502, 1703873359.725535, 1703873363.306537, 1703873366.9655333, 1703873370.5275023, 1703873374.052503, 1703873377.6445022, 1703873381.2025008, 1703873384.742531, 1703873388.2855005, 1703873391.8815331, 1703873395.3805366, 1703873398.9334984, 1703873402.4845045, 1703873405.994503, 1703873409.5765352, 1703873413.1555355, 1703873416.692539, 1703873420.2695358, 1703873423.8435357, 1703873427.3725035, 1703873430.9085355, 1703873434.4765043, 1703873437.997501, 1703873441.5085018, 1703873445.0425003, 1703873448.5775356, 1703873452.145503, 1703873455.6915019, 1703873459.1914992, 1703873462.7075346, 1703873466.2524981, 1703873469.7525358, 1703873473.298503, 1703873476.8185337, 1703873480.4035356, 1703873483.9434988, 1703873487.502501, 1703873491.0235026, 1703873494.4624977, 1703873498.008499, 1703873501.5634985, 1703873505.1045027, 1703873508.6104982, 1703873512.1935358, 1703873515.6935427, 1703873519.198533, 1703873522.7155004, 1703873526.2585351, 1703873529.9525323, 1703873533.793535, 1703873537.4435356, 1703873541.0405002, 1703873544.5664992, 1703873548.0734975, 1703873551.5734987, 1703873555.0594978, 1703873558.5574987, 1703873562.0594985, 1703873565.543498, 1703873569.099536, 1703873572.6095374, 1703873576.113501, 1703873579.635535, 1703873583.1325357, 1703873586.6285028, 1703873590.1265025, 1703873593.6285346, 1703873597.2464995, 1703873600.768535, 1703873604.358533, 1703873607.8424993, 1703873611.3244975, 1703873614.7965043, 1703873618.399499, 1703873621.9274986, 1703873625.4225006, 1703873628.9004989, 1703873632.4055023, 1703873635.9894989, 1703873639.458503, 1703873642.9644985, 1703873646.4784985, 1703873649.9695032, 1703873653.4464984, 1703873656.9505029, 1703873660.4524977, 1703873663.9995322, 1703873667.4694974, 1703873670.9974983, 1703873674.495502, 1703873677.9714994, 1703873681.4595375, 1703873684.906499, 1703873688.8154988, 1703873692.9855366, 1703873696.9705014, 1703873700.941535, 1703873704.7325015, 1703873708.6955342, 1703873712.2925386, 1703873715.8814983, 1703873719.3925016, 1703873722.964502, 1703873726.4614983, 1703873730.088498, 1703873733.6855006, 1703873737.2135024, 1703873740.731503, 1703873744.236503, 1703873747.6925356]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyper_tuning(x_train, y_train, lr_fn=None):\n",
    "  from sklearn.model_selection import KFold\n",
    "  import pandas as pd\n",
    "  kf = KFold(5)\n",
    "  acc_vals = []\n",
    "  # hidden_units = [64, 128, 256, 512]\n",
    "  # activations = [relu] #,leaky_relu, tanh ]\n",
    "  learning_rate = [0.001, 0.002, 0.004]\n",
    "  batch_size = [16, 32, 64]\n",
    "  momentum = [0.9, 0.99, 0.75, 0.5]\n",
    "  for btch in batch_size:\n",
    "    print('batchsize:',btch)\n",
    "    for lr in learning_rate:\n",
    "      for mom in momentum:\n",
    "        print('--------New Model----------')\n",
    "        print(f\"learning rate: {lr}\\t Batch Size:{btch}\\t Momentum:{mom}\")\n",
    "\n",
    "        optimizer = GradientDescent(learning_rate = lr, batch_size=btch, momentum=mom, lr_lamda=lr_fn)\n",
    "        \n",
    "        avg_acc = 0;       \n",
    "        # print(f\"for M=128, nonlinearity={activ}, lr={lr}, batch size={btch}.\")\n",
    "        start = time.time()\n",
    "        for k, (train, test) in enumerate(kf.split(x_train, y_train)):\n",
    "            print('k:',k)\n",
    "            temp_model = MLP(M=128)\n",
    "            temp_model, log, temp_acc = train_model(temp_model, optimizer, x_train[train], y_train[train])\n",
    "            avg_acc += temp_acc\n",
    "        avg_acc = avg_acc/5\n",
    "        acc_vals.append(avg_acc)\n",
    "        end = time.time()\n",
    "        print('time elapsed:',(end-start)/60/60,\"hrs\")\n",
    "        print('acc:',avg_acc)\n",
    "      \n",
    "  data = {'learningRate' : [0.001, 0.002, 0.004, 0.001, 0.002, 0.004, 0.001, 0.002, 0.004], \n",
    "          'batchSize':[16, 16, 16, 32, 32, 32, 64, 64, 64],\n",
    "          'accuracies': acc_vals\n",
    "          }\n",
    "  acc = pd.DataFrame(data)\n",
    "  print(acc)\n",
    "  return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyper_tuning_gd(x_train, y_train, print_every=100, lr_fn=None):\n",
    "  from sklearn.model_selection import KFold\n",
    "  import pandas as pd\n",
    "  kf = KFold(5)\n",
    "  acc_vals = []\n",
    "  # hidden_units = [64, 128, 256, 512]\n",
    "  # activations = [relu] #,leaky_relu, tanh ]\n",
    "  learning_rate = [0.001, 0.002, 0.004]\n",
    "  batch_size = [16, 32, 64]\n",
    "  for btch in batch_size:\n",
    "    print('batchsize:',btch)\n",
    "    for lr in learning_rate:\n",
    "      \n",
    "      \n",
    "      print('--------New Model----------')\n",
    "      print(f\"learning rate: {lr}\\t Batch Size:{btch}\")\n",
    "\n",
    "      optimizer = GradientDescent(learning_rate = lr, batch_size=btch, lr_fn=lr_fn)\n",
    "      \n",
    "      avg_acc = 0;       \n",
    "      # print(f\"for M=128, nonlinearity={activ}, lr={lr}, batch size={btch}.\")\n",
    "      start = time.time()\n",
    "      for k, (train, valid) in enumerate(kf.split(x_train, y_train)):\n",
    "          print('k:',k)\n",
    "          temp_model = MLP(M=128)\n",
    "          temp_model, log, max_acc = train_model(temp_model, optimizer, x_train[train], y_train[train], x_train[valid], y_train[valid], print_every=print_every)\n",
    "          avg_acc += max_acc\n",
    "      avg_acc = avg_acc/5\n",
    "      acc_vals.append(avg_acc)\n",
    "      end = time.time()\n",
    "      print('time elapsed:',(end-start)/60/60,\"hrs\")\n",
    "      print('acc:',avg_acc)\n",
    "      \n",
    "  data = {'learningRate' : [0.001, 0.002, 0.004, 0.001, 0.002, 0.004, 0.001, 0.002, 0.004], \n",
    "          'batchSize':[16, 16, 16, 32, 32, 32, 64, 64, 64],\n",
    "          'accuracies': acc_vals\n",
    "          }\n",
    "  acc = pd.DataFrame(data)\n",
    "  print(acc)\n",
    "  return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batchsize: 16\n",
      "--------New Model----------\n",
      "learning rate: 0.001\t Batch Size:16\n",
      "k: 0\n",
      "params initialized\n",
      "acc: 9.6\n",
      "Epoch 0: 9.6%\n",
      "Epoch 100: 9.65%\n",
      "Epoch 200: 9.65%\n",
      "Epoch 300: 9.65%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[248], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mhyper_tuning_gd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#x_valid=x_train[2000:2200], y_valid=y_train[2000:2200])\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[246], line 26\u001b[0m, in \u001b[0;36mhyper_tuning_gd\u001b[1;34m(x_train, y_train)\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mk:\u001b[39m\u001b[38;5;124m'\u001b[39m,k)\n\u001b[0;32m     25\u001b[0m     temp_model \u001b[38;5;241m=\u001b[39m MLP(M\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m)\n\u001b[1;32m---> 26\u001b[0m     temp_model, temp_accs, max_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtemp_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43mvalid\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43mvalid\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m     avg_acc \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m max_acc\n\u001b[0;32m     28\u001b[0m avg_acc \u001b[38;5;241m=\u001b[39m avg_acc\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m5\u001b[39m\n",
      "Cell \u001b[1;32mIn[242], line 13\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, optimizer, x_train, y_train, x_valid, y_valid, print_every)\u001b[0m\n\u001b[0;32m     11\u001b[0m t\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m t \u001b[38;5;241m<\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mmax_iters:\n\u001b[1;32m---> 13\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;66;03m# print('here')\u001b[39;00m\n\u001b[0;32m     15\u001b[0m     y_test_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(x_valid)\n",
      "Cell \u001b[1;32mIn[189], line 36\u001b[0m, in \u001b[0;36mMLP.fit\u001b[1;34m(self, x, y, optimizer)\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maggr_params, train_accs, batch_train_acc \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mmini_batch_step(gradient, x, y, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maggr_params)\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 36\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams, train_accs, batch_train_acc \u001b[38;5;241m=\u001b[39m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_mini_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m, train_accs, batch_train_acc\n",
      "Cell \u001b[1;32mIn[244], line 33\u001b[0m, in \u001b[0;36mGradientDescent.run_mini_batch\u001b[1;34m(self, gradient_fn, x, y, params, batch_size)\u001b[0m\n\u001b[0;32m     30\u001b[0m train_acc, batch_train_acc, chunk \u001b[38;5;241m=\u001b[39m [], [], []\n\u001b[0;32m     31\u001b[0m norms \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([np\u001b[38;5;241m.\u001b[39minf])\n\u001b[1;32m---> 33\u001b[0m mini_batches \u001b[38;5;241m=\u001b[39m \u001b[43mmini_batcher\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlearning_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr_fn(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlearning_rate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mt)\n",
      "Cell \u001b[1;32mIn[10], line 3\u001b[0m, in \u001b[0;36mmini_batcher\u001b[1;34m(x, y, mini_batch_size)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmini_batcher\u001b[39m(x, y, mini_batch_size):\n\u001b[0;32m      2\u001b[0m   zipped \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mhstack( (x, y ) )\n\u001b[1;32m----> 3\u001b[0m   \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mzipped\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m   x_batches, y_batches \u001b[38;5;241m=\u001b[39m [], []\n\u001b[0;32m      5\u001b[0m   mini_batches \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hyper_tuning_gd(x_train=x_train[:10000], y_train=y_train[:10000], print_every=10) #x_valid=x_train[2000:2200], y_valid=y_train[2000:2200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyper_tuning_rsag(x_train, \n",
    "                      y_train ,\n",
    "                      x_valid=None,\n",
    "                      y_valid=None,\n",
    "                      adaptive=False,\n",
    "                      convex=False,\n",
    "                      ):\n",
    "\n",
    "  nb_folds = 5\n",
    "  kf = KFold(nb_folds)\n",
    "  acc_vals = []\n",
    "  # hidden_units = [64, 128, 256, 512]\n",
    "  # activations = [relu] #,leaky_relu, tanh ]\n",
    "  learning_rate = [0.3, 0.09, 0.01]\n",
    "  alphas = [.9, .09, .009, .0009]\n",
    "  betas = [.09, .009, 0.00009]\n",
    "#   batch_size = [16, 32, 64]\n",
    "  for alpha in alphas:\n",
    "    print('alpha:',alpha)\n",
    "    for beta in betas:\n",
    "        for lr in learning_rate:\n",
    "            print('--------New Model----------')\n",
    "            print(f\"learning rate: {lr}\\t alpha: {alpha}\\t beta:{beta}\")\n",
    "            if adaptive:\n",
    "                optimizer = RSAG(learning_rate = lr, \n",
    "                                 alpha=alpha, \n",
    "                                 beta=beta, \n",
    "                                 batch_size=64, \n",
    "                                 lr_fn=lr_lamda,\n",
    "                                 alpha_fn=lr_lamda,\n",
    "                                 beta_fn=lr_lamda)\n",
    "            else:\n",
    "              optimizer = RSAG(learning_rate = lr, alpha=alpha, beta=beta, batch_size=64)\n",
    "\n",
    "            avg_acc = 0;       \n",
    "            start = time.time()\n",
    "            for k, (train, valid) in enumerate(kf.split(x_train, y_train)):\n",
    "                print('k:',k)\n",
    "\n",
    "                if convex:\n",
    "                  temp_model = MLP_1_Layer_Linear(M=128, num_classes=10, rsag=True)\n",
    "                else:\n",
    "                  temp_model = MLP_2_Layer_Softmax(M=128, num_classes=10, rsag=True)\n",
    "\n",
    "                temp_model, temp_accs, max_acc = train_model(temp_model, optimizer, x_train[train], y_train[train], x_train[valid], y_train[valid], verbose=False)\n",
    "                print(f'Fold {k} max acc: {max_acc}')\n",
    "                avg_acc += max_acc\n",
    "            \n",
    "            avg_acc = avg_acc/nb_folds\n",
    "            acc_vals.append(avg_acc)\n",
    "            end = time.time()\n",
    "            print('time elapsed:',(end-start)/60/60,\"hrs\")\n",
    "            print('acc:',avg_acc)\n",
    "            \n",
    "  data = {'learningRate' : [0.3]*len(betas)*len(alphas).append([0.09]*len(betas)*len(alphas)).append([0.01]*len(betas)*len(alphas)).append([0.001]*len(betas)*len(alphas)).append([0.002]*len(betas)*len(alphas)).append([0.004]*len(betas)*len(alphas)).append([0.001]*len(betas)*len(alphas)).append([0.002]*len(betas)*len(alphas)).append([0.004]*len(betas)*len(alphas)),\n",
    "          'alpha':[.9]*len(betas).append([.09]*len(betas)).append([.009]*len(betas)).append([.0009]*len(betas))*len(learning_rate),\n",
    "          'beta':[.001, .002, 0.004]*len(alphas)*len(learning_rate),\n",
    "          'accuracies': acc_vals\n",
    "          }\n",
    "  acc = pd.DataFrame(data)\n",
    "  print(acc)\n",
    "  return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha: 0.9\n",
      "--------New Model----------\n",
      "learning rate: 0.3\t alpha: 0.9\t beta:0.09\n",
      "k: 0\n",
      "params initialized\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Fold 0 max acc: 0.89\n",
      "k: 1\n",
      "params initialized\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Fold 1 max acc: 0.0725\n",
      "k: 2\n",
      "params initialized\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Fold 2 max acc: 0.08\n",
      "k: 3\n",
      "params initialized\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Fold 3 max acc: 0.1775\n",
      "k: 4\n",
      "params initialized\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Fold 4 max acc: 0.1125\n",
      "time elapsed: 0.03326808551947276 hrs\n",
      "acc: 0.2665\n",
      "--------New Model----------\n",
      "learning rate: 0.09\t alpha: 0.9\t beta:0.09\n",
      "k: 0\n",
      "params initialized\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Fold 0 max acc: 0.865\n",
      "k: 1\n",
      "params initialized\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Fold 1 max acc: 0.1275\n",
      "k: 2\n",
      "params initialized\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Fold 2 max acc: 0.0775\n",
      "k: 3\n",
      "params initialized\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Fold 3 max acc: 0.0775\n",
      "k: 4\n",
      "params initialized\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Fold 4 max acc: 0.1425\n",
      "time elapsed: 0.03237738105985853 hrs\n",
      "acc: 0.25799999999999995\n",
      "--------New Model----------\n",
      "learning rate: 0.01\t alpha: 0.9\t beta:0.09\n",
      "k: 0\n",
      "params initialized\n",
      "Fold 0 max acc: 0.73\n",
      "k: 1\n",
      "params initialized\n",
      "Fold 1 max acc: 0.755\n",
      "k: 2\n",
      "params initialized\n",
      "Fold 2 max acc: 0.735\n",
      "k: 3\n",
      "params initialized\n",
      "Fold 3 max acc: 0.705\n",
      "k: 4\n",
      "params initialized\n",
      "Fold 4 max acc: 0.7425\n",
      "time elapsed: 0.03750944667392307 hrs\n",
      "acc: 0.7335\n",
      "--------New Model----------\n",
      "learning rate: 0.3\t alpha: 0.9\t beta:0.009\n",
      "k: 0\n",
      "params initialized\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Fold 0 max acc: 0.89\n",
      "k: 1\n",
      "params initialized\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Fold 1 max acc: 0.1475\n",
      "k: 2\n",
      "params initialized\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Fold 2 max acc: 0.0925\n",
      "k: 3\n",
      "params initialized\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Fold 3 max acc: 0.1225\n",
      "k: 4\n",
      "params initialized\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Fold 4 max acc: 0.0625\n",
      "time elapsed: 0.03417303952905867 hrs\n",
      "acc: 0.263\n",
      "--------New Model----------\n",
      "learning rate: 0.09\t alpha: 0.9\t beta:0.009\n",
      "k: 0\n",
      "params initialized\n",
      "Fold 0 max acc: 0.8825\n",
      "k: 1\n",
      "params initialized\n",
      "Fold 1 max acc: 0.8375\n",
      "k: 2\n",
      "params initialized\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Fold 2 max acc: 0.8225\n",
      "k: 3\n",
      "params initialized\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Fold 3 max acc: 0.13\n",
      "k: 4\n",
      "params initialized\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Fold 4 max acc: 0.0975\n",
      "time elapsed: 0.0338764038350847 hrs\n",
      "acc: 0.554\n",
      "--------New Model----------\n",
      "learning rate: 0.01\t alpha: 0.9\t beta:0.009\n",
      "k: 0\n",
      "params initialized\n",
      "Fold 0 max acc: 0.7575\n",
      "k: 1\n",
      "params initialized\n",
      "Fold 1 max acc: 0.73\n",
      "k: 2\n",
      "params initialized\n",
      "Fold 2 max acc: 0.72\n",
      "k: 3\n",
      "params initialized\n",
      "Fold 3 max acc: 0.6925\n",
      "k: 4\n",
      "params initialized\n",
      "Fold 4 max acc: 0.7525\n",
      "time elapsed: 0.032987706793679135 hrs\n",
      "acc: 0.7304999999999999\n",
      "--------New Model----------\n",
      "learning rate: 0.3\t alpha: 0.9\t beta:9e-05\n",
      "k: 0\n",
      "params initialized\n",
      "Update params\n",
      "Update params\n",
      "Fold 0 max acc: 0.8825\n",
      "k: 1\n",
      "params initialized\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Fold 1 max acc: 0.0675\n",
      "k: 2\n",
      "params initialized\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Fold 2 max acc: 0.11\n",
      "k: 3\n",
      "params initialized\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Fold 3 max acc: 0.08\n",
      "k: 4\n",
      "params initialized\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Fold 4 max acc: 0.1425\n",
      "time elapsed: 0.03290637228224012 hrs\n",
      "acc: 0.25650000000000006\n",
      "--------New Model----------\n",
      "learning rate: 0.09\t alpha: 0.9\t beta:9e-05\n",
      "k: 0\n",
      "params initialized\n",
      "Update params\n",
      "Update params\n",
      "Fold 0 max acc: 0.8725\n",
      "k: 1\n",
      "params initialized\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Fold 1 max acc: 0.1225\n",
      "k: 2\n",
      "params initialized\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Fold 2 max acc: 0.1025\n",
      "k: 3\n",
      "params initialized\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Fold 3 max acc: 0.08\n",
      "k: 4\n",
      "params initialized\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Fold 4 max acc: 0.0875\n",
      "time elapsed: 0.03392718248897129 hrs\n",
      "acc: 0.253\n",
      "--------New Model----------\n",
      "learning rate: 0.01\t alpha: 0.9\t beta:9e-05\n",
      "k: 0\n",
      "params initialized\n",
      "Fold 0 max acc: 0.775\n",
      "k: 1\n",
      "params initialized\n",
      "Fold 1 max acc: 0.725\n",
      "k: 2\n",
      "params initialized\n",
      "Fold 2 max acc: 0.715\n",
      "k: 3\n",
      "params initialized\n",
      "Fold 3 max acc: 0.6775\n",
      "k: 4\n",
      "params initialized\n",
      "Fold 4 max acc: 0.74\n",
      "time elapsed: 0.03390250623226166 hrs\n",
      "acc: 0.7265\n",
      "alpha: 0.09\n",
      "--------New Model----------\n",
      "learning rate: 0.3\t alpha: 0.09\t beta:0.09\n",
      "k: 0\n",
      "params initialized\n",
      "Update params\n",
      "Update params\n",
      "Fold 0 max acc: 0.865\n",
      "k: 1\n",
      "params initialized\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Fold 1 max acc: 0.115\n",
      "k: 2\n",
      "params initialized\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Fold 2 max acc: 0.0825\n",
      "k: 3\n",
      "params initialized\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Fold 3 max acc: 0.115\n",
      "k: 4\n",
      "params initialized\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Fold 4 max acc: 0.0975\n",
      "time elapsed: 0.031845418413480125 hrs\n",
      "acc: 0.255\n",
      "--------New Model----------\n",
      "learning rate: 0.09\t alpha: 0.09\t beta:0.09\n",
      "k: 0\n",
      "params initialized\n",
      "Update params\n",
      "Update params\n",
      "Fold 0 max acc: 0.86\n",
      "k: 1\n",
      "params initialized\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Fold 1 max acc: 0.0725\n",
      "k: 2\n",
      "params initialized\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Fold 2 max acc: 0.0875\n",
      "k: 3\n",
      "params initialized\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Fold 3 max acc: 0.095\n",
      "k: 4\n",
      "params initialized\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Fold 4 max acc: 0.1275\n",
      "time elapsed: 0.03246577454937829 hrs\n",
      "acc: 0.2485\n",
      "--------New Model----------\n",
      "learning rate: 0.01\t alpha: 0.09\t beta:0.09\n",
      "k: 0\n",
      "params initialized\n",
      "Fold 0 max acc: 0.43\n",
      "k: 1\n",
      "params initialized\n",
      "Fold 1 max acc: 0.525\n",
      "k: 2\n",
      "params initialized\n",
      "Fold 2 max acc: 0.445\n",
      "k: 3\n",
      "params initialized\n",
      "Fold 3 max acc: 0.45\n",
      "k: 4\n",
      "params initialized\n",
      "Fold 4 max acc: 0.3725\n",
      "time elapsed: 0.029835577938291762 hrs\n",
      "acc: 0.4445\n",
      "--------New Model----------\n",
      "learning rate: 0.3\t alpha: 0.09\t beta:0.009\n",
      "k: 0\n",
      "params initialized\n",
      "Update params\n",
      "Fold 0 max acc: 0.855\n",
      "k: 1\n",
      "params initialized\n",
      "Fold 1 max acc: 0.3225\n",
      "k: 2\n",
      "params initialized\n",
      "Fold 2 max acc: 0.325\n",
      "k: 3\n",
      "params initialized\n",
      "Fold 3 max acc: 0.2625\n",
      "k: 4\n",
      "params initialized\n",
      "Fold 4 max acc: 0.47\n",
      "time elapsed: 0.02818501181072659 hrs\n",
      "acc: 0.44699999999999995\n",
      "--------New Model----------\n",
      "learning rate: 0.09\t alpha: 0.09\t beta:0.009\n",
      "k: 0\n",
      "params initialized\n",
      "Fold 0 max acc: 0.8125\n",
      "k: 1\n",
      "params initialized\n",
      "Update params\n",
      "Update params\n",
      "Fold 1 max acc: 0.765\n",
      "k: 2\n",
      "params initialized\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Fold 2 max acc: 0.115\n",
      "k: 3\n",
      "params initialized\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Fold 3 max acc: 0.12\n",
      "k: 4\n",
      "params initialized\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Fold 4 max acc: 0.0675\n",
      "time elapsed: 0.03012477159500122 hrs\n",
      "acc: 0.376\n",
      "--------New Model----------\n",
      "learning rate: 0.01\t alpha: 0.09\t beta:0.009\n",
      "k: 0\n",
      "params initialized\n",
      "Fold 0 max acc: 0.7525\n",
      "k: 1\n",
      "params initialized\n",
      "Fold 1 max acc: 0.74\n",
      "k: 2\n",
      "params initialized\n",
      "Fold 2 max acc: 0.7375\n",
      "k: 3\n",
      "params initialized\n",
      "Fold 3 max acc: 0.7\n",
      "k: 4\n",
      "params initialized\n",
      "Fold 4 max acc: 0.78\n",
      "time elapsed: 0.03280294471316867 hrs\n",
      "acc: 0.742\n",
      "--------New Model----------\n",
      "learning rate: 0.3\t alpha: 0.09\t beta:9e-05\n",
      "k: 0\n",
      "params initialized\n",
      "Update params\n",
      "Fold 0 max acc: 0.8275\n",
      "k: 1\n",
      "params initialized\n",
      "Fold 1 max acc: 0.3325\n",
      "k: 2\n",
      "params initialized\n",
      "Fold 2 max acc: 0.3175\n",
      "k: 3\n",
      "params initialized\n",
      "Fold 3 max acc: 0.395\n",
      "k: 4\n",
      "params initialized\n",
      "Fold 4 max acc: 0.4775\n",
      "time elapsed: 0.03288263850741916 hrs\n",
      "acc: 0.47000000000000003\n",
      "--------New Model----------\n",
      "learning rate: 0.09\t alpha: 0.09\t beta:9e-05\n",
      "k: 0\n",
      "params initialized\n",
      "Update params\n",
      "Update params\n",
      "Fold 0 max acc: 0.8025\n",
      "k: 1\n",
      "params initialized\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Fold 1 max acc: 0.07\n",
      "k: 2\n",
      "params initialized\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Fold 2 max acc: 0.1675\n",
      "k: 3\n",
      "params initialized\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Fold 3 max acc: 0.075\n",
      "k: 4\n",
      "params initialized\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Fold 4 max acc: 0.06\n",
      "time elapsed: 0.032727401852607724 hrs\n",
      "acc: 0.23500000000000001\n",
      "--------New Model----------\n",
      "learning rate: 0.01\t alpha: 0.09\t beta:9e-05\n",
      "k: 0\n",
      "params initialized\n",
      "Fold 0 max acc: 0.7075\n",
      "k: 1\n",
      "params initialized\n",
      "Fold 1 max acc: 0.715\n",
      "k: 2\n",
      "params initialized\n",
      "Fold 2 max acc: 0.685\n",
      "k: 3\n",
      "params initialized\n",
      "Fold 3 max acc: 0.675\n",
      "k: 4\n",
      "params initialized\n",
      "Fold 4 max acc: 0.725\n",
      "time elapsed: 0.03178573661380344 hrs\n",
      "acc: 0.7015\n",
      "alpha: 0.009\n",
      "--------New Model----------\n",
      "learning rate: 0.3\t alpha: 0.009\t beta:0.09\n",
      "k: 0\n",
      "params initialized\n",
      "Fold 0 max acc: 0.88\n",
      "k: 1\n",
      "params initialized\n",
      "Fold 1 max acc: 0.8425\n",
      "k: 2\n",
      "params initialized\n",
      "Update params\n",
      "Fold 2 max acc: 0.8375\n",
      "k: 3\n",
      "params initialized\n",
      "Fold 3 max acc: 0.1525\n",
      "k: 4\n",
      "params initialized\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Fold 4 max acc: 0.12\n",
      "time elapsed: 0.02919709901014964 hrs\n",
      "acc: 0.5665\n",
      "--------New Model----------\n",
      "learning rate: 0.09\t alpha: 0.009\t beta:0.09\n",
      "k: 0\n",
      "params initialized\n",
      "Update params\n",
      "Fold 0 max acc: 0.88\n",
      "k: 1\n",
      "params initialized\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Fold 1 max acc: 0.06\n",
      "k: 2\n",
      "params initialized\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Fold 2 max acc: 0.0825\n",
      "k: 3\n",
      "params initialized\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Fold 3 max acc: 0.1125\n",
      "k: 4\n",
      "params initialized\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Fold 4 max acc: 0.08\n",
      "time elapsed: 0.029397376047240364 hrs\n",
      "acc: 0.24300000000000002\n",
      "--------New Model----------\n",
      "learning rate: 0.01\t alpha: 0.009\t beta:0.09\n",
      "k: 0\n",
      "params initialized\n",
      "Fold 0 max acc: 0.4375\n",
      "k: 1\n",
      "params initialized\n",
      "Update params\n",
      "Update params\n",
      "Fold 1 max acc: 0.32\n",
      "k: 2\n",
      "params initialized\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Fold 2 max acc: 0.0825\n",
      "k: 3\n",
      "params initialized\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Fold 3 max acc: 0.13\n",
      "k: 4\n",
      "params initialized\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Fold 4 max acc: 0.0925\n",
      "time elapsed: 0.029247712029351128 hrs\n",
      "acc: 0.2125\n",
      "--------New Model----------\n",
      "learning rate: 0.3\t alpha: 0.009\t beta:0.009\n",
      "k: 0\n",
      "params initialized\n",
      "Update params\n",
      "Fold 0 max acc: 0.8125\n",
      "k: 1\n",
      "params initialized\n",
      "Fold 1 max acc: 0.3575\n",
      "k: 2\n",
      "params initialized\n",
      "Fold 2 max acc: 0.4\n",
      "k: 3\n",
      "params initialized\n",
      "Fold 3 max acc: 0.3625\n",
      "k: 4\n",
      "params initialized\n",
      "Fold 4 max acc: 0.3275\n",
      "time elapsed: 0.029032971262931823 hrs\n",
      "acc: 0.45199999999999996\n",
      "--------New Model----------\n",
      "learning rate: 0.09\t alpha: 0.009\t beta:0.009\n",
      "k: 0\n",
      "params initialized\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Fold 0 max acc: 0.79\n",
      "k: 1\n",
      "params initialized\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Fold 1 max acc: 0.1125\n",
      "k: 2\n",
      "params initialized\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Fold 2 max acc: 0.1575\n",
      "k: 3\n",
      "params initialized\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Fold 3 max acc: 0.15\n",
      "k: 4\n",
      "params initialized\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Fold 4 max acc: 0.1075\n",
      "time elapsed: 0.0293650930457645 hrs\n",
      "acc: 0.26349999999999996\n",
      "--------New Model----------\n",
      "learning rate: 0.01\t alpha: 0.009\t beta:0.009\n",
      "k: 0\n",
      "params initialized\n",
      "Fold 0 max acc: 0.7575\n",
      "k: 1\n",
      "params initialized\n",
      "Fold 1 max acc: 0.7225\n",
      "k: 2\n",
      "params initialized\n",
      "Fold 2 max acc: 0.7425\n",
      "k: 3\n",
      "params initialized\n",
      "Fold 3 max acc: 0.71\n",
      "k: 4\n",
      "params initialized\n",
      "Fold 4 max acc: 0.74\n",
      "time elapsed: 0.029096486899587843 hrs\n",
      "acc: 0.7345\n",
      "--------New Model----------\n",
      "learning rate: 0.3\t alpha: 0.009\t beta:9e-05\n",
      "k: 0\n",
      "params initialized\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Fold 0 max acc: 0.7225\n",
      "k: 1\n",
      "params initialized\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Fold 1 max acc: 0.0875\n",
      "k: 2\n",
      "params initialized\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Fold 2 max acc: 0.125\n",
      "k: 3\n",
      "params initialized\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Fold 3 max acc: 0.1075\n",
      "k: 4\n",
      "params initialized\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Fold 4 max acc: 0.115\n",
      "time elapsed: 0.029331862992710536 hrs\n",
      "acc: 0.23149999999999998\n",
      "--------New Model----------\n",
      "learning rate: 0.09\t alpha: 0.009\t beta:9e-05\n",
      "k: 0\n",
      "params initialized\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Fold 0 max acc: 0.7525\n",
      "k: 1\n",
      "params initialized\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Fold 1 max acc: 0.085\n",
      "k: 2\n",
      "params initialized\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Fold 2 max acc: 0.1275\n",
      "k: 3\n",
      "params initialized\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Fold 3 max acc: 0.1425\n",
      "k: 4\n",
      "params initialized\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Fold 4 max acc: 0.12\n",
      "time elapsed: 0.02933461374706692 hrs\n",
      "acc: 0.2455\n",
      "--------New Model----------\n",
      "learning rate: 0.01\t alpha: 0.009\t beta:9e-05\n",
      "k: 0\n",
      "params initialized\n",
      "Fold 0 max acc: 0.7\n",
      "k: 1\n",
      "params initialized\n",
      "Fold 1 max acc: 0.7025\n",
      "k: 2\n",
      "params initialized\n",
      "Fold 2 max acc: 0.695\n",
      "k: 3\n",
      "params initialized\n",
      "Fold 3 max acc: 0.6875\n",
      "k: 4\n",
      "params initialized\n",
      "Fold 4 max acc: 0.7475\n",
      "time elapsed: 0.0291841059923172 hrs\n",
      "acc: 0.7064999999999999\n",
      "alpha: 0.0009\n",
      "--------New Model----------\n",
      "learning rate: 0.3\t alpha: 0.0009\t beta:0.09\n",
      "k: 0\n",
      "params initialized\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Fold 0 max acc: 0.87\n",
      "k: 1\n",
      "params initialized\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Fold 1 max acc: 0.055\n",
      "k: 2\n",
      "params initialized\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Fold 2 max acc: 0.1475\n",
      "k: 3\n",
      "params initialized\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Fold 3 max acc: 0.1325\n",
      "k: 4\n",
      "params initialized\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Fold 4 max acc: 0.1425\n",
      "time elapsed: 0.029445448186662462 hrs\n",
      "acc: 0.2695\n",
      "--------New Model----------\n",
      "learning rate: 0.09\t alpha: 0.0009\t beta:0.09\n",
      "k: 0\n",
      "params initialized\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Fold 0 max acc: 0.84\n",
      "k: 1\n",
      "params initialized\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Fold 1 max acc: 0.06\n",
      "k: 2\n",
      "params initialized\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Fold 2 max acc: 0.0775\n",
      "k: 3\n",
      "params initialized\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Fold 3 max acc: 0.0675\n",
      "k: 4\n",
      "params initialized\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Fold 4 max acc: 0.06\n",
      "time elapsed: 0.029598278270827397 hrs\n",
      "acc: 0.221\n",
      "--------New Model----------\n",
      "learning rate: 0.01\t alpha: 0.0009\t beta:0.09\n",
      "k: 0\n",
      "params initialized\n",
      "Fold 0 max acc: 0.4425\n",
      "k: 1\n",
      "params initialized\n",
      "Fold 1 max acc: 0.375\n",
      "k: 2\n",
      "params initialized\n",
      "Fold 2 max acc: 0.4225\n",
      "k: 3\n",
      "params initialized\n",
      "Fold 3 max acc: 0.4125\n",
      "k: 4\n",
      "params initialized\n",
      "Fold 4 max acc: 0.495\n",
      "time elapsed: 0.029300764269298975 hrs\n",
      "acc: 0.4295\n",
      "--------New Model----------\n",
      "learning rate: 0.3\t alpha: 0.0009\t beta:0.009\n",
      "k: 0\n",
      "params initialized\n",
      "Fold 0 max acc: 0.795\n",
      "k: 1\n",
      "params initialized\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Fold 1 max acc: 0.7\n",
      "k: 2\n",
      "params initialized\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Fold 2 max acc: 0.0725\n",
      "k: 3\n",
      "params initialized\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Fold 3 max acc: 0.1025\n",
      "k: 4\n",
      "params initialized\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Fold 4 max acc: 0.0775\n",
      "time elapsed: 0.029358310765690274 hrs\n",
      "acc: 0.34950000000000003\n",
      "--------New Model----------\n",
      "learning rate: 0.09\t alpha: 0.0009\t beta:0.009\n",
      "k: 0\n",
      "params initialized\n",
      "Update params\n",
      "Update params\n",
      "Fold 0 max acc: 0.7875\n",
      "k: 1\n",
      "params initialized\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Fold 1 max acc: 0.0775\n",
      "k: 2\n",
      "params initialized\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Fold 2 max acc: 0.08\n",
      "k: 3\n",
      "params initialized\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Fold 3 max acc: 0.1675\n",
      "k: 4\n",
      "params initialized\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Fold 4 max acc: 0.05\n",
      "time elapsed: 0.029475833376248674 hrs\n",
      "acc: 0.2325\n",
      "--------New Model----------\n",
      "learning rate: 0.01\t alpha: 0.0009\t beta:0.009\n",
      "k: 0\n",
      "params initialized\n",
      "Fold 0 max acc: 0.7675\n",
      "k: 1\n",
      "params initialized\n",
      "Fold 1 max acc: 0.725\n",
      "k: 2\n",
      "params initialized\n",
      "Fold 2 max acc: 0.7275\n",
      "k: 3\n",
      "params initialized\n",
      "Fold 3 max acc: 0.6875\n",
      "k: 4\n",
      "params initialized\n",
      "Fold 4 max acc: 0.745\n",
      "time elapsed: 0.029394676884015404 hrs\n",
      "acc: 0.7304999999999999\n",
      "--------New Model----------\n",
      "learning rate: 0.3\t alpha: 0.0009\t beta:9e-05\n",
      "k: 0\n",
      "params initialized\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Fold 0 max acc: 0.7625\n",
      "k: 1\n",
      "params initialized\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Fold 1 max acc: 0.07\n",
      "k: 2\n",
      "params initialized\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Fold 2 max acc: 0.1125\n",
      "k: 3\n",
      "params initialized\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Fold 3 max acc: 0.1\n",
      "k: 4\n",
      "params initialized\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Fold 4 max acc: 0.08\n",
      "time elapsed: 0.030046490165922375 hrs\n",
      "acc: 0.22500000000000003\n",
      "--------New Model----------\n",
      "learning rate: 0.09\t alpha: 0.0009\t beta:9e-05\n",
      "k: 0\n",
      "params initialized\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Fold 0 max acc: 0.7425\n",
      "k: 1\n",
      "params initialized\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Fold 1 max acc: 0.07\n",
      "k: 2\n",
      "params initialized\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Fold 2 max acc: 0.1075\n",
      "k: 3\n",
      "params initialized\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Fold 3 max acc: 0.125\n",
      "k: 4\n",
      "params initialized\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Fold 4 max acc: 0.135\n",
      "time elapsed: 0.02971067700121138 hrs\n",
      "acc: 0.236\n",
      "--------New Model----------\n",
      "learning rate: 0.01\t alpha: 0.0009\t beta:9e-05\n",
      "k: 0\n",
      "params initialized\n",
      "Fold 0 max acc: 0.72\n",
      "k: 1\n",
      "params initialized\n",
      "Update params\n",
      "Fold 1 max acc: 0.685\n",
      "k: 2\n",
      "params initialized\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Fold 2 max acc: 0.0725\n",
      "k: 3\n",
      "params initialized\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Fold 3 max acc: 0.065\n",
      "k: 4\n",
      "params initialized\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Update params\n",
      "Fold 4 max acc: 0.1025\n",
      "time elapsed: 0.02967708170413971 hrs\n",
      "acc: 0.329\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'append'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mhyper_tuning_rsag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m2000\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m2000\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madaptive\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#, x_valid=x_train[2000:2200], y_valid=y_train[2000:2200])\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[30], line 55\u001b[0m, in \u001b[0;36mhyper_tuning_rsag\u001b[1;34m(x_train, y_train, x_valid, y_valid, adaptive, convex)\u001b[0m\n\u001b[0;32m     52\u001b[0m           \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime elapsed:\u001b[39m\u001b[38;5;124m'\u001b[39m,(end\u001b[38;5;241m-\u001b[39mstart)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m60\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m60\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhrs\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     53\u001b[0m           \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124macc:\u001b[39m\u001b[38;5;124m'\u001b[39m,avg_acc)\n\u001b[1;32m---> 55\u001b[0m data \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearningRate\u001b[39m\u001b[38;5;124m'\u001b[39m : [\u001b[38;5;241m0.3\u001b[39m]\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlen\u001b[39m(betas)\u001b[38;5;241m*\u001b[39m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43malphas\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend\u001b[49m([\u001b[38;5;241m0.09\u001b[39m]\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlen\u001b[39m(betas)\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlen\u001b[39m(alphas))\u001b[38;5;241m.\u001b[39mappend([\u001b[38;5;241m0.01\u001b[39m]\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlen\u001b[39m(betas)\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlen\u001b[39m(alphas))\u001b[38;5;241m.\u001b[39mappend([\u001b[38;5;241m0.001\u001b[39m]\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlen\u001b[39m(betas)\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlen\u001b[39m(alphas))\u001b[38;5;241m.\u001b[39mappend([\u001b[38;5;241m0.002\u001b[39m]\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlen\u001b[39m(betas)\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlen\u001b[39m(alphas))\u001b[38;5;241m.\u001b[39mappend([\u001b[38;5;241m0.004\u001b[39m]\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlen\u001b[39m(betas)\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlen\u001b[39m(alphas))\u001b[38;5;241m.\u001b[39mappend([\u001b[38;5;241m0.001\u001b[39m]\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlen\u001b[39m(betas)\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlen\u001b[39m(alphas))\u001b[38;5;241m.\u001b[39mappend([\u001b[38;5;241m0.002\u001b[39m]\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlen\u001b[39m(betas)\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlen\u001b[39m(alphas))\u001b[38;5;241m.\u001b[39mappend([\u001b[38;5;241m0.004\u001b[39m]\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlen\u001b[39m(betas)\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlen\u001b[39m(alphas)),\n\u001b[0;32m     56\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124malpha\u001b[39m\u001b[38;5;124m'\u001b[39m:[\u001b[38;5;241m.9\u001b[39m]\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlen\u001b[39m(betas)\u001b[38;5;241m.\u001b[39mappend([\u001b[38;5;241m.09\u001b[39m]\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlen\u001b[39m(betas))\u001b[38;5;241m.\u001b[39mappend([\u001b[38;5;241m.009\u001b[39m]\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlen\u001b[39m(betas))\u001b[38;5;241m.\u001b[39mappend([\u001b[38;5;241m.0009\u001b[39m]\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlen\u001b[39m(betas))\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlen\u001b[39m(learning_rate),\n\u001b[0;32m     57\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbeta\u001b[39m\u001b[38;5;124m'\u001b[39m:[\u001b[38;5;241m.001\u001b[39m, \u001b[38;5;241m.002\u001b[39m, \u001b[38;5;241m0.004\u001b[39m]\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlen\u001b[39m(alphas)\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlen\u001b[39m(learning_rate),\n\u001b[0;32m     58\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracies\u001b[39m\u001b[38;5;124m'\u001b[39m: acc_vals\n\u001b[0;32m     59\u001b[0m         }\n\u001b[0;32m     60\u001b[0m acc \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(data)\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28mprint\u001b[39m(acc)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'int' object has no attribute 'append'"
     ]
    }
   ],
   "source": [
    "hyper_tuning_rsag(x_train=x_train[:2000], y_train=y_train[:2000], adaptive=True) #, x_valid=x_train[2000:2200], y_valid=y_train[2000:2200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
