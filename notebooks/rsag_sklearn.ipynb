{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../rsag_convex.png\" alt=\"algoconvex\" />\n",
    "<img src=\"../x_update.png\" alt=\"x_update\" />\n",
    "<img src=\"../mean.png\" alt=\"mean\" />\n",
    "<img src=\"../rsag_composite.png\" alt=\"algo\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Parameters :__\n",
    "- $\\alpha$: (1-$\\alpha$) weight of aggregated x on current state, i.e. momentum\n",
    "- $\\lambda$: learning rate\n",
    "- $\\beta$: change for aggregated x\n",
    "- $p_k$ termination probability\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\camgr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import time\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import path\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from util import DataLoader, plot_accuracies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# packaging it all into a function\n",
    "def preprocess_mnist():\n",
    "  import random as rand\n",
    "\n",
    "\n",
    "  (x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data(path=\"mnist.npz\")\n",
    "  \n",
    "  \n",
    "  assert x_train.shape == (60000, 28, 28)\n",
    "  assert x_test.shape == (10000, 28, 28)\n",
    "  assert y_train.shape == (60000,)\n",
    "  assert y_test.shape == (10000,)\n",
    "\n",
    "  mean_mat = np.mean(x_train, axis=0)\n",
    "  # centering the data by removing the pixel wise mean from every pixel in every image\n",
    "  x_train_centered = x_train - mean_mat\n",
    "  x_test_centered = x_test - mean_mat\n",
    "\n",
    "  # normalizing the grayscale values to values in interval [0,1]\n",
    "  x_train_normalized = x_train_centered/255.0\n",
    "  x_test_normalized = x_test_centered/255.0\n",
    "\n",
    "  # finally, flattening the data\n",
    "  x_train = np.reshape(x_train_normalized, (60000,784))\n",
    "  x_test = np.reshape(x_test_normalized, (10000, 784))\n",
    "  \n",
    "  #converting the test data to one hot encodings\n",
    "  y_train = keras.utils.to_categorical(y_train, num_classes=10)\n",
    "  y_test = keras.utils.to_categorical(y_test, num_classes=10)\n",
    "\n",
    "  return x_train, y_train, x_test, y_test\n",
    "x_train, y_train, x_test, y_test = preprocess_mnist()\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Implementation - Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_acc(pred, truth):\n",
    "  counter =0\n",
    "\n",
    "  for i in range(len(pred)):\n",
    "    maxVal = np.where(pred[i] == np.amax(pred[i]))\n",
    "    counter += 1 if maxVal == np.where(truth[i]==1) else 0\n",
    "  return counter * 100.0 / float(len(pred))\n",
    "\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#activation functions\n",
    "softmax1D = lambda z: np.exp(z)/float(sum(np.exp(z)))\n",
    "softmax2D = lambda z: np.array([np.exp(i)/float(sum(np.exp(i))) for i in z])\n",
    "linear = lambda z: np.array([i/float(sum(i)) for i in z])\n",
    "# logistic = lambda z: 1./ (1 + np.exp(-z))\n",
    "# relu = lambda y: y[y <= 0]=0\n",
    "def relu(x):\n",
    "  alpha = 0.1\n",
    "  x=np.array(x).astype(float)\n",
    "  # x[x<=0]=0.1*x\n",
    "  np.putmask(x, x<0, alpha*x)\n",
    "  return x\n",
    "def relu_grad(x):\n",
    "  alpha = 0.1\n",
    "  x=np.array(x).astype(float)\n",
    "  x[x>0]=1\n",
    "  x[x<=0]=alpha\n",
    "  return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_2_Layer_Softmax:\n",
    "\n",
    "    def __init__(self, M = 128, num_classes = 10, rsag=False):\n",
    "        self.M = M\n",
    "        self.num_classes = num_classes\n",
    "        self.rsag = rsag\n",
    "        self.params = None\n",
    "        self.aggr_params = None\n",
    "\n",
    "    def fit(self, x, y, optimizer):\n",
    "        N,D = x.shape\n",
    "        def gradient(x, y, params):\n",
    "            w = params[0] # v.shape = (D, M), w.shape = (M)\n",
    "            z = np.dot(x, w)\n",
    "            yh = softmax2D(z)#N\n",
    "            dy = yh - y #N\n",
    "            train_acc = evaluate_acc(yh, y)\n",
    "\n",
    "            # Softmax Gradient\n",
    "            dw = np.dot(x.T, dy)/N #M\n",
    "            dparams = [dw]\n",
    "            return dparams ,train_acc\n",
    "        \n",
    "        if self.params is None:\n",
    "            initializer = keras.initializers.GlorotNormal()\n",
    "            w = initializer(shape=(D, self.num_classes))\n",
    "            self.params = [w]\n",
    "            if self.rsag:\n",
    "                self.aggr_params = [np.copy(w)]\n",
    "            print('params initialized')\n",
    "\n",
    "        if self.rsag:\n",
    "            self.params, self.aggr_params, train_accs, batch_train_acc = optimizer.mini_batch_step(gradient, x, y, self.params, self.aggr_params)\n",
    "        else:\n",
    "            self.params, train_accs, batch_train_acc = optimizer.mini_batch_step(gradient, x, y, self.params)\n",
    "\n",
    "        return self, train_accs, batch_train_acc\n",
    "\n",
    "    def predict(self, x):\n",
    "        # print('self:',self)\n",
    "        # print('self==None:',self==None)\n",
    "        w = self.params[0]\n",
    "        # print(w.shape)\n",
    "        # z = relu(np.dot(x, w)) #N x M\n",
    "        yh = softmax2D(np.dot(x, w))#N\n",
    "        return yh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_1_Layer_Linear:\n",
    "\n",
    "    def __init__(self, M = 128, num_classes = 10, rsag=False):\n",
    "        self.M = M\n",
    "        self.num_classes = num_classes\n",
    "        self.rsag = rsag\n",
    "        self.params = None\n",
    "        self.aggr_params = None\n",
    "\n",
    "    def fit(self, x, y, optimizer):\n",
    "        N,D = x.shape\n",
    "        def gradient(x, y, params):\n",
    "            w = params[0] # v.shape = (D, M), w.shape = (M)\n",
    "            z = np.dot(x, w)\n",
    "            yh = linear(z)#N\n",
    "            dw = np.sum(yh - y)/N #N\n",
    "            train_acc = evaluate_acc(yh, y)\n",
    "\n",
    "            # L2 Loss Gradient\n",
    "            # dw = np.dot(x.T, dy)/N #M\n",
    "            dparams = [dw]\n",
    "            return dparams ,train_acc\n",
    "        \n",
    "        if self.params is None:\n",
    "            initializer = keras.initializers.GlorotNormal()\n",
    "            w = initializer(shape=(D, self.num_classes))\n",
    "            self.params = [w]\n",
    "            if self.rsag:\n",
    "                self.aggr_params = [np.copy(w)]\n",
    "            print('params initialized')\n",
    "\n",
    "        if self.rsag:\n",
    "            self.params, self.aggr_params, train_accs, batch_train_acc = optimizer.mini_batch_step(gradient, x, y, self.params, self.aggr_params)\n",
    "        else:\n",
    "            self.params, train_accs, batch_train_acc = optimizer.mini_batch_step(gradient, x, y, self.params)\n",
    "\n",
    "        return self, train_accs, batch_train_acc\n",
    "\n",
    "    def predict(self, x):\n",
    "        # print('self:',self)\n",
    "        # print('self==None:',self==None)\n",
    "        w = self.params[0]\n",
    "\n",
    "        yh = linear(np.dot(x, w))#N\n",
    "        return yh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini batcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_lamda = lambda lr, t: lr/(1+t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RSAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RSAG:\n",
    "\n",
    "    def __init__(self, \n",
    "                 learning_rate=.001, \n",
    "                 alpha=0.009, \n",
    "                 beta=.000009, \n",
    "                 max_iters=200, \n",
    "                 epsilon=1e-8, \n",
    "                 batch_size=64,\n",
    "                 lr_fn = None,\n",
    "                    alpha_fn = None,\n",
    "                    beta_fn = None,\n",
    "                    start_adap = 30,\n",
    "                 ):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iters = max_iters\n",
    "        self.epsilon = epsilon\n",
    "        self.alpha = alpha  # momentum param\n",
    "        self.beta = beta \n",
    "\n",
    "        self.lr_fn = lr_fn\n",
    "        self.alpha_fn = alpha_fn\n",
    "        self.beta_fn = beta_fn\n",
    "\n",
    "        self.start_adap = start_adap\n",
    "\n",
    "        self.t = 0\n",
    "\n",
    "\n",
    "    def mini_batch_step(self, \n",
    "                       gradient_fn,\n",
    "                       x, \n",
    "                       y,\n",
    "                       params, \n",
    "                       agg_params,\n",
    "                       batch_size=32,\n",
    "                       ):\n",
    "        \n",
    "        train_acc, batch_train_acc, chunk = [], [], []\n",
    "\n",
    "\n",
    "        slices = sklearn.utils.gen_batches(x.shape[0], batch_size)\n",
    "\n",
    "\n",
    "        if self.start_adap >= self.t:\n",
    "            if self.lr_fn is not None:\n",
    "                self.learning_rate = self.lr_fn(self.learning_rate, self.t)\n",
    "                # print('New learning rate:', self.learning_rate)\n",
    "            if self.alpha_fn is not None:\n",
    "                self.alpha = self.alpha_fn(self.alpha, self.t)\n",
    "                # print('New alpha:', self.alpha)\n",
    "            if self.beta_fn is not None:\n",
    "                self.beta = self.beta_fn(self.beta, self.t)\n",
    "                # print('New beta:', self.beta)\n",
    "\n",
    "        grad = None\n",
    "        for batch in slices:\n",
    "            x_temp, y_temp = x[batch], y[batch]\n",
    "\n",
    "            proj_params = [(1-self.alpha) * a_p + self.alpha * p for p, a_p in zip(params, agg_params)]\n",
    "\n",
    "            grad, temp_acc = gradient_fn(x_temp, y_temp, proj_params)\n",
    "            # if grad == None: grad = temp_grad \n",
    "            # else:\n",
    "            # for p in range(len(params)):\n",
    "            #     grad[p] += grad[p]\n",
    "                                \n",
    "\n",
    "            # v_chunk.append(evaluate_acc())\n",
    "            chunk.append(temp_acc)\n",
    "            train_acc.append( ( self.t, temp_acc ) )\n",
    "\n",
    "            # if t%batch_size ==0:\n",
    "            # for p in range(len(params)):\n",
    "            #     agg_params[p] -= self.beta * (grad[p]/batch_size)\n",
    "            #     params[p] -= self.learning_rate * (grad[p]/batch_size)\n",
    "            for p in range(len(params)):\n",
    "                agg_params[p] -= self.beta * (grad[p])\n",
    "                params[p] -= self.learning_rate * (grad[p])\n",
    "\n",
    "            \n",
    "        self.t += 1\n",
    "            \n",
    "        return params, agg_params, train_acc, batch_train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.04339363\n",
      "Iteration 2, loss = 0.52378460\n",
      "CPU times: total: 44.5 s\n",
      "Wall time: 25 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(batch_size=64, hidden_layer_sizes=(512,), momentum=0.8,\n",
       "              solver=&#x27;sgd&#x27;, verbose=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(batch_size=64, hidden_layer_sizes=(512,), momentum=0.8,\n",
       "              solver=&#x27;sgd&#x27;, verbose=True)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MLPClassifier(batch_size=64, hidden_layer_sizes=(512,), momentum=0.8,\n",
       "              solver='sgd', verbose=True)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "clf = MLPClassifier(hidden_layer_sizes=(512,), activation='relu', solver='sgd', alpha=0.0001, momentum=0.8, batch_size=64, learning_rate='constant', learning_rate_init=0.001, nesterovs_momentum=True,verbose=True)\n",
    "%time clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8017\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_test, clf.predict(x_test)))\n",
    "# activation='relu', solver='sgd', alpha=0.0001, batch_size=64, learning_rate='constant', learning_rate_init=0.001, nesterovs_momentum=True\n",
    "# [1.8023864815965787, 1.1122309566767148, 0.9845296966527957, 0.9154617448390219, 0.8699689145736055, 0.8365705881224808, 0.8107982315381743, 0.7895384784268797, 0.771082085742921, 0.7552732548141347, 0.7415558847414158, 0.7287698625332969, 0.7172472639691361, 0.706809744230456, 0.697198112331996, 0.6880630036115248, 0.6799326487354387, 0.6720643946144156, 0.6646984452814398, 0.6576804957640667, 0.6510529769693165, 0.6446246119656461]\n",
    "# 0.8017\n",
    "# CPU times: total: 7min 10s\n",
    "# Wall time: 3min 44s\n",
    "# solver='adam', alpha=0.0001, batch_size=64, learning_rate='constant', learning_rate_init=0.001, nesterovs_momentum=True,verbose=True)\n",
    "# 84.57\n",
    "# [0.8612156609999259, 0.6249129242272599, 0.5581398511623741, 0.5108802118568585, 0.4691072568910733, 0.4353992497295652, 0.40704429576434176, 0.3805020388852907, 0.35406887324687, 0.3298601532867995, 0.30623046652949937, 0.29002717397885547, 0.2713237669144441, 0.2514935455522592, 0.23837528939129574, 0.22462548777331365, 0.2122725128906315, 0.19964016897140632, 0.18637880570153922, 0.17356588508840962, 0.1658213505209685, 0.15647343628720575, 0.14783796049234926, 0.1333524127764709, 0.14204921604363624, 0.12750238350269527, 0.11881357392933954, 0.11200988128682542, 0.1130529385547876, 0.11354326204537345, 0.09885686873471046, 0.0930096509081991, 0.09655665834549663, 0.09260154624164536, 0.08115000406704216, 0.07959780062782788, 0.08250181458817121, 0.08106108561985036, 0.07341696734054892, 0.07604154997056141, 0.06295011133958502, 0.08018669386804467, 0.0616817261430883, 0.07367609626831328, 0.06523213700827496, 0.06964252739224731, 0.05752994633281319, 0.05197989734371084, 0.07381226348438415, 0.05830003207888134, 0.059422724682382105, 0.05041107174843518, 0.05727327599541805, 0.045635822816795256, 0.07568988410558206, 0.051876567841403196, 0.046329602952653766, 0.05991002631492367, 0.053050026014173995, 0.05121909793694829, 0.05788941712186149, 0.043738605256584066, 0.062191099081914634, 0.040420605015593726, 0.04949669605103939, 0.05005099457912783, 0.046754105687687286, 0.055313401757353574, 0.031450249271437095, 0.07335947120026465, 0.03956862519001744, 0.0557280472207543, 0.04982615933858711, 0.05109769744532108, 0.04067857278228474, 0.03421287144297862, 0.05925097099087448, 0.04785653973425865, 0.03998925377281498, 0.04366581677240043]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params initialized\n",
      "Epoch 0/200\n",
      "----------\n",
      "Loss 2.3430\n",
      "Accuracy:  8.3042\n",
      "Validation Loss 2.3419\n",
      "Validation Accuracy:  8.5833\n",
      "Time : 0.0\n",
      "Epoch 10/200\n",
      "----------\n",
      "Loss 2.2712\n",
      "Accuracy:  13.1792\n",
      "Validation Loss 2.2705\n",
      "Validation Accuracy:  13.5333\n",
      "Time : 27.477996110916138\n",
      "Epoch 20/200\n",
      "----------\n",
      "Loss 2.2712\n",
      "Accuracy:  13.1792\n",
      "Validation Loss 2.2705\n",
      "Validation Accuracy:  13.5333\n",
      "Time : 56.308032274246216\n",
      "Early stopping at epoch 24\n",
      "{'loss': [2.342972937949073, 2.2927980908311643, 2.276254728701672, 2.272133238489332, 2.2713099450994045, 2.271184435984621, 2.271179960740743, 2.271179884968927, 2.2711798841405497, 2.2711798841294835, 2.2711798841294835, 2.2711798841294835, 2.2711798841294835, 2.2711798841294835, 2.2711798841294835, 2.2711798841294835, 2.2711798841294835, 2.2711798841294835, 2.2711798841294835, 2.2711798841294835, 2.2711798841294835, 2.2711798841294835, 2.2711798841294835, 2.2711798841294835, 2.2711798841294835], 'accuracy': [8.304166666666667, 11.527083333333334, 12.779166666666667, 13.110416666666667, 13.175, 13.179166666666667, 13.179166666666667, 13.179166666666667, 13.179166666666667, 13.179166666666667, 13.179166666666667, 13.179166666666667, 13.179166666666667, 13.179166666666667, 13.179166666666667, 13.179166666666667, 13.179166666666667, 13.179166666666667, 13.179166666666667, 13.179166666666667, 13.179166666666667, 13.179166666666667, 13.179166666666667, 13.179166666666667, 13.179166666666667], 'v_loss': [2.3419131529360677, 2.292019057758138, 2.2755672472361526, 2.271468471257359, 2.2706497077929306, 2.270524915412561, 2.270520472011218, 2.2705203964415785, 2.2705203956075293, 2.270520395596292, 2.270520395596292, 2.270520395596292, 2.270520395596292, 2.270520395596292, 2.270520395596292, 2.270520395596292, 2.270520395596292, 2.270520395596292, 2.270520395596292, 2.270520395596292, 2.270520395596292, 2.270520395596292, 2.270520395596292, 2.270520395596292, 2.270520395596292], 'v_accuracy': [8.583333333333334, 11.833333333333334, 13.166666666666666, 13.475, 13.533333333333333, 13.533333333333333, 13.533333333333333, 13.533333333333333, 13.533333333333333, 13.533333333333333, 13.533333333333333, 13.533333333333333, 13.533333333333333, 13.533333333333333, 13.533333333333333, 13.533333333333333, 13.533333333333333, 13.533333333333333, 13.533333333333333, 13.533333333333333, 13.533333333333333, 13.533333333333333, 13.533333333333333, 13.533333333333333, 13.533333333333333], 'time': [1703878406.6237037, 1703878409.5666995, 1703878412.278734, 1703878414.9817035, 1703878417.7177007, 1703878420.4297347, 1703878423.1487336, 1703878425.8706994, 1703878428.654733, 1703878431.3627322, 1703878434.1016998, 1703878436.878735, 1703878440.0417, 1703878442.8957012, 1703878445.769704, 1703878448.6307042, 1703878451.4767034, 1703878454.3587022, 1703878457.215737, 1703878460.0836997, 1703878462.931736, 1703878465.7967358, 1703878468.8617005, 1703878471.7197373, 1703878474.5917037, 1703878477.4327354]}\n"
     ]
    }
   ],
   "source": [
    "model = MLP(M=128, num_classes=10, rsag=True)\n",
    "optimizer = RSAG(learning_rate=.09, alpha=0.009, beta=0.00009, lr_fn=lr_lamda, alpha_fn=lr_lamda, beta_fn=lr_lamda, start_adap=49)\n",
    "\n",
    "model, log, acc_max = train_model(model, optimizer, x_train, y_train, x_valid, y_valid, print_every=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params initialized\n",
      "Epoch 0/2000\n",
      "----------\n",
      "Accuracy:  22.4042\n",
      "Validation Accuracy:  22.2000\n",
      "Time : 0.0\n",
      "Epoch 10/2000\n",
      "----------\n",
      "Accuracy:  30.2708\n",
      "Validation Accuracy:  30.2750\n",
      "Time : 25.739963054656982\n",
      "Epoch 20/2000\n",
      "----------\n",
      "Accuracy:  36.1896\n",
      "Validation Accuracy:  36.1250\n",
      "Time : 51.283992528915405\n",
      "Epoch 30/2000\n",
      "----------\n",
      "Accuracy:  40.4708\n",
      "Validation Accuracy:  40.1167\n",
      "Time : 77.01396560668945\n",
      "Epoch 40/2000\n",
      "----------\n",
      "Accuracy:  43.8854\n",
      "Validation Accuracy:  43.6583\n",
      "Time : 102.82899928092957\n",
      "Epoch 50/2000\n",
      "----------\n",
      "Accuracy:  46.9083\n",
      "Validation Accuracy:  46.7417\n",
      "Time : 128.6409993171692\n",
      "Epoch 60/2000\n",
      "----------\n",
      "Accuracy:  49.7917\n",
      "Validation Accuracy:  49.5583\n",
      "Time : 154.62896299362183\n",
      "Epoch 70/2000\n",
      "----------\n",
      "Accuracy:  52.0833\n",
      "Validation Accuracy:  52.2417\n",
      "Time : 180.45496344566345\n",
      "Epoch 80/2000\n",
      "----------\n",
      "Accuracy:  54.2354\n",
      "Validation Accuracy:  54.3583\n",
      "Time : 206.28096795082092\n",
      "Epoch 90/2000\n",
      "----------\n",
      "Accuracy:  56.0312\n",
      "Validation Accuracy:  56.0833\n",
      "Time : 232.07996726036072\n",
      "Epoch 100/2000\n",
      "----------\n",
      "Accuracy:  57.5417\n",
      "Validation Accuracy:  57.5750\n",
      "Time : 258.3939673900604\n",
      "Epoch 110/2000\n",
      "----------\n",
      "Accuracy:  58.9000\n",
      "Validation Accuracy:  58.7333\n",
      "Time : 284.44299936294556\n",
      "Epoch 120/2000\n",
      "----------\n",
      "Accuracy:  59.9229\n",
      "Validation Accuracy:  59.8667\n",
      "Time : 310.3279983997345\n",
      "Epoch 130/2000\n",
      "----------\n",
      "Accuracy:  60.8750\n",
      "Validation Accuracy:  60.6083\n",
      "Time : 336.4720003604889\n",
      "Epoch 140/2000\n",
      "----------\n",
      "Accuracy:  61.7354\n",
      "Validation Accuracy:  61.5000\n",
      "Time : 362.39496326446533\n",
      "Epoch 150/2000\n",
      "----------\n",
      "Accuracy:  62.4937\n",
      "Validation Accuracy:  62.1250\n",
      "Time : 390.0529670715332\n",
      "Epoch 160/2000\n",
      "----------\n",
      "Accuracy:  63.2000\n",
      "Validation Accuracy:  62.7250\n",
      "Time : 417.9619941711426\n",
      "Epoch 170/2000\n",
      "----------\n",
      "Accuracy:  63.7854\n",
      "Validation Accuracy:  63.2500\n",
      "Time : 444.7309670448303\n",
      "Early stopping at epoch 171\n",
      "{'loss': [], 'accuracy': [22.404166666666665, 23.16875, 24.010416666666668, 24.795833333333334, 25.679166666666667, 26.502083333333335, 27.235416666666666, 28.085416666666667, 28.916666666666668, 29.604166666666668, 30.270833333333332, 30.947916666666668, 31.56875, 32.10208333333333, 32.74375, 33.375, 33.94375, 34.63333333333333, 35.21458333333333, 35.66875, 36.18958333333333, 36.65208333333333, 37.18333333333333, 37.64791666666667, 38.06041666666667, 38.49375, 38.91458333333333, 39.28125, 39.635416666666664, 40.045833333333334, 40.47083333333333, 40.88125, 41.28125, 41.58125, 41.952083333333334, 42.24166666666667, 42.60625, 42.952083333333334, 43.24791666666667, 43.579166666666666, 43.885416666666664, 44.208333333333336, 44.50625, 44.85625, 45.15, 45.44583333333333, 45.7625, 46.06666666666667, 46.322916666666664, 46.614583333333336, 46.90833333333333, 47.20625, 47.483333333333334, 47.791666666666664, 48.10625, 48.4, 48.735416666666666, 48.975, 49.25208333333333, 49.53125, 49.791666666666664, 50.047916666666666, 50.295833333333334, 50.5375, 50.74375, 50.95, 51.19166666666667, 51.420833333333334, 51.62708333333333, 51.8375, 52.083333333333336, 52.325, 52.52708333333333, 52.764583333333334, 52.99583333333333, 53.24583333333333, 53.46666666666667, 53.6625, 53.85, 54.041666666666664, 54.235416666666666, 54.447916666666664, 54.68541666666667, 54.875, 55.0375, 55.22708333333333, 55.40208333333333, 55.53125, 55.71458333333333, 55.85, 56.03125, 56.18958333333333, 56.34375, 56.5125, 56.65833333333333, 56.77916666666667, 56.958333333333336, 57.09375, 57.25625, 57.43125, 57.541666666666664, 57.68125, 57.81458333333333, 57.96875, 58.12291666666667, 58.24375, 58.37083333333333, 58.52291666666667, 58.64791666666667, 58.775, 58.9, 59.0375, 59.1375, 59.24583333333333, 59.34791666666667, 59.45625, 59.52708333333333, 59.62916666666667, 59.73125, 59.84791666666667, 59.922916666666666, 60.03958333333333, 60.15833333333333, 60.275, 60.38125, 60.4625, 60.56041666666667, 60.641666666666666, 60.72291666666667, 60.80833333333333, 60.875, 60.99791666666667, 61.077083333333334, 61.166666666666664, 61.25625, 61.3125, 61.41458333333333, 61.5125, 61.60208333333333, 61.677083333333336, 61.735416666666666, 61.80833333333333, 61.858333333333334, 61.95, 62.020833333333336, 62.104166666666664, 62.18333333333333, 62.239583333333336, 62.32083333333333, 62.40625, 62.49375, 62.56666666666667, 62.64375, 62.7125, 62.795833333333334, 62.86875, 62.952083333333334, 63.025, 63.08541666666667, 63.139583333333334, 63.2, 63.26875, 63.333333333333336, 63.385416666666664, 63.4375, 63.49166666666667, 63.57083333333333, 63.63333333333333, 63.666666666666664, 63.72291666666667, 63.78541666666667, 63.84583333333333], 'v_loss': [], 'v_accuracy': [22.2, 22.991666666666667, 23.95, 24.8, 25.608333333333334, 26.416666666666668, 27.1, 27.991666666666667, 28.825, 29.6, 30.275, 30.975, 31.541666666666668, 32.19166666666667, 32.825, 33.38333333333333, 33.858333333333334, 34.525, 35.05, 35.575, 36.125, 36.59166666666667, 36.93333333333333, 37.44166666666667, 37.85, 38.31666666666667, 38.7, 38.93333333333333, 39.358333333333334, 39.725, 40.11666666666667, 40.55, 41.025, 41.375, 41.65, 41.9, 42.25833333333333, 42.725, 43.075, 43.43333333333333, 43.65833333333333, 44.09166666666667, 44.38333333333333, 44.7, 45.016666666666666, 45.325, 45.583333333333336, 45.9, 46.166666666666664, 46.55, 46.74166666666667, 46.99166666666667, 47.275, 47.55833333333333, 47.833333333333336, 48.2, 48.46666666666667, 48.69166666666667, 48.983333333333334, 49.208333333333336, 49.55833333333333, 49.86666666666667, 50.141666666666666, 50.333333333333336, 50.65833333333333, 51.00833333333333, 51.333333333333336, 51.55, 51.85, 52.06666666666667, 52.24166666666667, 52.43333333333333, 52.608333333333334, 52.858333333333334, 53.06666666666667, 53.31666666666667, 53.541666666666664, 53.71666666666667, 53.96666666666667, 54.19166666666667, 54.358333333333334, 54.50833333333333, 54.7, 54.88333333333333, 55.041666666666664, 55.225, 55.38333333333333, 55.56666666666667, 55.75, 55.983333333333334, 56.083333333333336, 56.275, 56.333333333333336, 56.516666666666666, 56.65833333333333, 56.775, 56.925, 57.083333333333336, 57.25, 57.416666666666664, 57.575, 57.68333333333333, 57.8, 57.925, 58.06666666666667, 58.19166666666667, 58.31666666666667, 58.425, 58.49166666666667, 58.55, 58.733333333333334, 58.86666666666667, 59.0, 59.108333333333334, 59.175, 59.25, 59.375, 59.516666666666666, 59.65833333333333, 59.8, 59.86666666666667, 59.925, 60.00833333333333, 60.075, 60.125, 60.233333333333334, 60.31666666666667, 60.44166666666667, 60.525, 60.56666666666667, 60.608333333333334, 60.675, 60.766666666666666, 60.833333333333336, 60.9, 61.041666666666664, 61.13333333333333, 61.225, 61.30833333333333, 61.40833333333333, 61.5, 61.541666666666664, 61.575, 61.65833333333333, 61.69166666666667, 61.766666666666666, 61.80833333333333, 61.85, 61.90833333333333, 61.99166666666667, 62.125, 62.19166666666667, 62.25, 62.28333333333333, 62.34166666666667, 62.40833333333333, 62.46666666666667, 62.516666666666666, 62.59166666666667, 62.675, 62.725, 62.78333333333333, 62.84166666666667, 62.875, 62.94166666666667, 62.983333333333334, 63.016666666666666, 63.108333333333334, 63.166666666666664, 63.2, 63.25, 63.34166666666667], 'time': [1703871247.9247596, 1703871250.6367228, 1703871253.1877568, 1703871255.769723, 1703871258.3367264, 1703871260.8617218, 1703871263.4207275, 1703871265.970755, 1703871268.5027227, 1703871271.0967584, 1703871273.6647227, 1703871276.2337227, 1703871278.7987237, 1703871281.350761, 1703871283.8857234, 1703871286.4157238, 1703871288.973723, 1703871291.5097268, 1703871294.048724, 1703871296.6257238, 1703871299.2087522, 1703871301.7547252, 1703871304.3727283, 1703871306.9687269, 1703871309.5527256, 1703871312.131759, 1703871314.6777227, 1703871317.2297552, 1703871319.7747583, 1703871322.368728, 1703871324.9387252, 1703871327.5197265, 1703871330.1097221, 1703871332.785758, 1703871335.352724, 1703871337.9237227, 1703871340.4947264, 1703871343.0397253, 1703871345.617757, 1703871348.1967232, 1703871350.753759, 1703871353.344727, 1703871355.9187229, 1703871358.5097227, 1703871361.0527601, 1703871363.6757596, 1703871366.2807226, 1703871368.8557262, 1703871371.4567227, 1703871373.9987228, 1703871376.565759, 1703871379.2197263, 1703871381.786727, 1703871384.3927252, 1703871386.975723, 1703871389.5547228, 1703871392.1817257, 1703871394.789723, 1703871397.398723, 1703871399.9877226, 1703871402.5537226, 1703871405.122726, 1703871407.7187228, 1703871410.265727, 1703871412.8357635, 1703871415.3997226, 1703871417.9817588, 1703871420.5517626, 1703871423.1517227, 1703871425.788723, 1703871428.379723, 1703871430.9727235, 1703871433.5337229, 1703871436.0907629, 1703871438.6777227, 1703871441.2507591, 1703871443.8267593, 1703871446.4317257, 1703871449.014755, 1703871451.577724, 1703871454.2057276, 1703871456.7817254, 1703871459.3697586, 1703871461.9337592, 1703871464.4857595, 1703871467.07976, 1703871469.6457608, 1703871472.2167606, 1703871474.8107255, 1703871477.4017227, 1703871480.004727, 1703871482.5977256, 1703871485.1867595, 1703871487.7527242, 1703871490.5737267, 1703871493.16076, 1703871495.7127275, 1703871498.3047607, 1703871501.20776, 1703871503.7577226, 1703871506.318727, 1703871508.9047267, 1703871511.5357592, 1703871514.2597609, 1703871516.8327239, 1703871519.4317226, 1703871522.0177226, 1703871524.6177232, 1703871527.1907227, 1703871529.791723, 1703871532.367759, 1703871534.9697583, 1703871537.574726, 1703871540.15776, 1703871542.7117255, 1703871545.2777593, 1703871547.8367252, 1703871550.4607594, 1703871553.0117571, 1703871555.6297226, 1703871558.252758, 1703871560.8237598, 1703871563.4477227, 1703871566.065723, 1703871568.685726, 1703871571.2757597, 1703871573.9067252, 1703871576.4997241, 1703871579.1127224, 1703871581.7997594, 1703871584.39676, 1703871587.0107224, 1703871589.6257586, 1703871592.2367265, 1703871594.82576, 1703871597.3827229, 1703871599.9617577, 1703871602.5747569, 1703871605.1397228, 1703871607.725724, 1703871610.319723, 1703871612.92076, 1703871615.536723, 1703871618.081727, 1703871620.6697228, 1703871623.4087229, 1703871626.299728, 1703871629.1487608, 1703871632.0677555, 1703871635.0777252, 1703871637.9777267, 1703871640.7447543, 1703871643.3627229, 1703871645.9447253, 1703871648.674728, 1703871651.5737524, 1703871654.3977551, 1703871657.3197258, 1703871660.291754, 1703871663.2037265, 1703871665.8867538, 1703871668.533722, 1703871671.177723, 1703871674.1867588, 1703871676.8107586, 1703871679.470723, 1703871682.103759, 1703871684.698758, 1703871687.3167229, 1703871689.9627252, 1703871692.6557267, 1703871695.3307607, 1703871697.9697602]}\n"
     ]
    }
   ],
   "source": [
    "# model = MLP(M=128, num_classes=10)\n",
    "# optimizer = GradientDescent(learning_rate=.006, max_iters=2000, batch_size=64, momentum=0.9)\n",
    "# model, log, max_acc = train_model(model, optimizer, x_train, y_train, x_valid, y_valid, patience=20, print_every=10)\n",
    "# log['label'] = 'Gradient Descent'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_accuracies_vs_time(logs):\n",
    "    for log in logs:\n",
    "        plt.plot(log['v_accuracy'], log['time'], label=log['label'])\n",
    "        plt.xlabel('Time (s)')\n",
    "        plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_accuracies(log['v_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_accuracies(log['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#earning_rate=.006, alpha=0.009, beta=0.000009 {'loss': [2.5160519958632497, 2.4973575056239055, 2.478812720935548, 2.460418367816293, 2.4421750728527756, 2.424083243209927, 2.406143218943164, 2.388355619024832, 2.370720531779371, 2.3532388507830846, 2.3359103385579245, 2.3187356583615517, 2.3017151449607045, 2.284848798003, 2.268137113025858, 2.251580131464062, 2.2351781560975392, 2.2189313277624816, 2.2028397324627114, 2.186903482608769, 2.171122576020296, 2.1554971802396707, 2.1400273974261985, 2.12471322420883, 2.1095543291442924, 2.0945509674032827, 2.0797030372028, 2.0650101785621415, 2.0504724696688656, 2.0360895385965785, 2.021861265377499, 2.007787395317621, 1.9938677452480706, 1.9801019385579555, 1.9664896459769876, 1.9530305341659135, 1.9397243989520938, 1.9265708088428968, 1.913569264293767, 1.9007191856791694, 1.8880201770698002, 1.8754717765074902, 1.8630734839007053, 1.8508246629653273, 1.8387247167204983, 1.8267731012263388, 1.8149689028182607, 1.8033118168215017, 1.7918008676058652, 1.780435443199607, 1.7692147867065802, 1.758138262094848, 1.7472050244117556, 1.736414184032042, 1.725764925254068, 1.7152563634988953, 1.704887640485752, 1.6946577872277346, 1.6845660798988868, 1.6746114205550002, 1.6647928081632135, 1.655109453942081, 1.6455602044679913, 1.63614411612935, 1.6268601594469219, 1.6177073423341959, 1.6086844168984595, 1.5997904959232623, 1.5910244241810505, 1.5823851782440828, 1.573871518177679, 1.5654824421577596, 1.557216748641881, 1.5490733628087985, 1.5410510119995289, 1.5331486888063552, 1.5253652436822294, 1.5176995752563607, 1.5101503292857068, 1.5027163181320302, 1.4953964058131015, 1.4881894832016638, 1.481094316008594, 1.4741097819182238, 1.4672346230198323, 1.4604677039683713, 1.453807736800002, 1.4472536140469547, 1.440804143388772, 1.4344579918486622, 1.4282141241856425, 1.4220714320677037, 1.4160285570580309, 1.4100845667564426, 1.4042380347574344, 1.3984878527799462, 1.392832983561538, 1.3872721741401781, 1.3818042746328631, 1.3764282049267276, 1.3711428537472001, 1.3659470475748883, 1.3608397097045988, 1.3558197165376955, 1.350885922540901, 1.3460371923663124, 1.3412725411089275, 1.336590902774661, 1.3319911511496636, 1.3274722697497163, 1.3230331517395404, 1.3186727883587195, 1.3143902311457627, 1.3101844006747139, 1.3060542439016587, 1.3019988505028786, 1.2980171772661853, 1.2941082629039047, 1.2902711788933614, 1.2865049123578411, 1.2828085680651276, 1.2791812381756231, 1.2756219102534354, 1.2721297229491526, 1.2687038625549176, 1.2653433682122233, 1.2620473589301207, 1.2588149634015926, 1.2556453335623932, 1.2525376170849052, 1.2494910262907375, 1.246504696143371, 1.243577830703593, 1.2407096924293133, 1.237899363279785, 1.2351462232563044, 1.2324494032576179, 1.229808122809259, 1.2272217424748364, 1.2246893998625108, 1.2222104309766801, 1.2197840963234217, 1.217409798994363, 1.2150867337041764, 1.2128142780481603, 1.210591738654992, 1.2084184182913702, 1.206293681621064, 1.2042168761799936, 1.2021873765389202, 1.2002045784726234, 1.1982678221929342, 1.196376580417403, 1.1945301193133262, 1.1927279806807676, 1.1909695424957498], 'accuracy': [4.3, 4.75, 5.341666666666667, 6.0125, 6.716666666666667, 7.477083333333334, 8.441666666666666, 9.40625, 10.4625, 11.641666666666667, 12.902083333333334, 14.0375, 15.26875, 16.56875, 17.841666666666665, 19.060416666666665, 20.266666666666666, 21.464583333333334, 22.55, 23.65625, 24.722916666666666, 25.739583333333332, 26.658333333333335, 27.55625, 28.470833333333335, 29.266666666666666, 30.05, 30.80625, 31.525, 32.172916666666666, 32.829166666666666, 33.43541666666667, 34.08125, 34.6875, 35.295833333333334, 35.80416666666667, 36.35208333333333, 36.87291666666667, 37.33541666666667, 37.81875, 38.385416666666664, 38.81666666666667, 39.24166666666667, 39.78333333333333, 40.22708333333333, 40.6125, 41.02291666666667, 41.4625, 41.87708333333333, 42.24375, 42.66875, 42.99375, 43.34375, 43.729166666666664, 44.1375, 44.43333333333333, 44.73125, 45.083333333333336, 45.395833333333336, 45.71041666666667, 45.989583333333336, 46.31875, 46.610416666666666, 46.895833333333336, 47.1875, 47.45625, 47.77916666666667, 48.0625, 48.31875, 48.614583333333336, 48.88125, 49.175, 49.43333333333333, 49.735416666666666, 50.07083333333333, 50.28958333333333, 50.49583333333333, 50.725, 50.96666666666667, 51.15625, 51.40625, 51.68333333333333, 51.90833333333333, 52.135416666666664, 52.3125, 52.53541666666667, 52.7375, 52.979166666666664, 53.29375, 53.49375, 53.708333333333336, 53.983333333333334, 54.22083333333333, 54.46041666666667, 54.65833333333333, 54.86875, 55.05625, 55.2125, 55.416666666666664, 55.55833333333333, 55.74166666666667, 55.927083333333336, 56.15833333333333, 56.30416666666667, 56.458333333333336, 56.58958333333333, 56.704166666666666, 56.86875, 57.05416666666667, 57.204166666666666, 57.33125, 57.43958333333333, 57.56041666666667, 57.670833333333334, 57.791666666666664, 57.90625, 58.00208333333333, 58.12708333333333, 58.24166666666667, 58.34583333333333, 58.454166666666666, 58.575, 58.7, 58.81458333333333, 58.9375, 59.06041666666667, 59.1625, 59.2375, 59.327083333333334, 59.427083333333336, 59.516666666666666, 59.577083333333334, 59.6375, 59.72291666666667, 59.78958333333333, 59.85625, 59.94166666666667, 60.03333333333333, 60.11666666666667, 60.15416666666667, 60.208333333333336, 60.260416666666664, 60.322916666666664, 60.39375, 60.43125, 60.49166666666667, 60.55833333333333, 60.63125, 60.677083333333336, 60.73125, 60.77708333333333, 60.83125, 60.90416666666667, 60.94583333333333, 60.979166666666664, 61.01875], 'v_loss': [2.5148063596832437, 2.49615115174158, 2.477645865095786, 2.459291206852179, 2.4410877930084904, 2.423036027156197, 2.4051362327818278, 2.3873890088997767, 2.369794446786681, 2.3523534307749725, 2.3350657023587504, 2.317931923993111, 2.3009524178478657, 2.2841271702453367, 2.267456656406367, 2.2509409248654566, 2.234580261467171, 2.218374792645572, 2.2023246052354963, 2.1864297853873835, 2.1706903345144157, 2.1551064139866174, 2.1396780997641, 2.1244053980517505, 2.109287957373581, 2.0943260345083266, 2.0795195103309645, 2.064868031715993, 2.050371651931761, 2.036030004501688, 2.0218429533263245, 2.007810240884165, 1.9939316872527018, 1.9802069020080229, 1.9666355495342172, 1.9532172950622861, 1.9399519304682387, 1.9268390158325168, 1.9138780509548645, 1.9010684523960792, 1.8884098183659395, 1.8759016882093311, 1.8635435585499627, 1.8513347910801574, 1.839274781648382, 1.8273629929940471, 1.8155985062359348, 1.8039810073094362, 1.7925095340134105, 1.7811834604671386, 1.7700020314668363, 1.7589646178779323, 1.7480703688756496, 1.7373183974596424, 1.7267078812669163, 1.716237935998055, 1.7059077065490273, 1.695716225615338, 1.6856627558836956, 1.6757462147164703, 1.6659655958825723, 1.6563201089086288, 1.646808603382893, 1.6374301328060965, 1.6281836675477173, 1.6190682167844326, 1.6100825409944048, 1.6012257423668843, 1.5924966624999113, 1.5838942853757854, 1.575417371423427, 1.5670649123915237, 1.5588357061659899, 1.550728687265933, 1.5427425655045626, 1.5348763544970312, 1.527128887522649, 1.519499066906007, 1.5119855398998892, 1.5045871159380524, 1.4973026538189615, 1.490131047626756, 1.4830710600306156, 1.4761215594216688, 1.4692813007763483, 1.4625491384682816, 1.4559237849669846, 1.4494041271107097, 1.4429889762008965, 1.4366769980378882, 1.4304671523642343, 1.4243583229909194, 1.4183491562713653, 1.4124387104299518, 1.4066255652717912, 1.4009086091516714, 1.3952867979658947, 1.3897588798909377, 1.3843236980094316, 1.3789801723933008, 1.3737271927204455, 1.3685635673111576, 1.3634882249170845, 1.3585000468824315, 1.3535978732176912, 1.3487805759161873, 1.3440471659343356, 1.3393965670122039, 1.334827656155502, 1.3303394145720735, 1.3259307271746612, 1.3216005858707518, 1.317348036777376, 1.3131720032899277, 1.309071424047942, 1.3050453893105254, 1.3010928511784616, 1.2972128510397276, 1.2934044490677945, 1.2896666332736608, 1.2859985116232655, 1.2823991680248932, 1.2788675915936247, 1.2754029178727024, 1.2720043281895772, 1.268670861721328, 1.265401637638159, 1.2621957819253145, 1.2590524413181323, 1.2559707630679149, 1.2529499631767145, 1.2499891689371267, 1.2470875881754835, 1.244244475245979, 1.2414589177784916, 1.2387302947582481, 1.236057727017808, 1.2334404448858396, 1.2308778046353446, 1.2283689407638467, 1.225913188960611, 1.2235098051851063, 1.2211581990139224, 1.2188575695573516, 1.2166072748931558, 1.2144066429936986, 1.2122549648512528, 1.2101516076721892, 1.2080959183405033, 1.2060872707084564, 1.2041250600952305, 1.2022086331282513, 1.200337453655215, 1.1985107994263455, 1.196728201150955, 1.1949890517948278], 'v_accuracy': [4.375, 4.766666666666667, 5.208333333333333, 5.958333333333333, 6.583333333333333, 7.383333333333334, 8.191666666666666, 9.325, 10.258333333333333, 11.541666666666666, 12.758333333333333, 13.975, 15.366666666666667, 16.716666666666665, 17.933333333333334, 19.316666666666666, 20.608333333333334, 22.0, 23.316666666666666, 24.391666666666666, 25.433333333333334, 26.55, 27.45, 28.258333333333333, 29.241666666666667, 30.175, 30.975, 31.691666666666666, 32.5, 33.21666666666667, 33.833333333333336, 34.333333333333336, 35.03333333333333, 35.45, 36.15833333333333, 36.74166666666667, 37.275, 37.74166666666667, 38.375, 38.725, 39.05833333333333, 39.55833333333333, 40.00833333333333, 40.6, 41.00833333333333, 41.50833333333333, 41.90833333333333, 42.28333333333333, 42.541666666666664, 42.96666666666667, 43.25, 43.608333333333334, 43.96666666666667, 44.4, 44.775, 45.15833333333333, 45.45, 45.75833333333333, 46.108333333333334, 46.31666666666667, 46.6, 46.958333333333336, 47.225, 47.475, 47.666666666666664, 47.94166666666667, 48.25, 48.375, 48.65, 48.916666666666664, 49.25, 49.44166666666667, 49.775, 49.96666666666667, 50.21666666666667, 50.50833333333333, 50.725, 50.94166666666667, 51.15, 51.475, 51.625, 51.833333333333336, 52.016666666666666, 52.233333333333334, 52.46666666666667, 52.641666666666666, 52.858333333333334, 53.083333333333336, 53.35, 53.575, 53.81666666666667, 53.958333333333336, 54.19166666666667, 54.40833333333333, 54.59166666666667, 54.71666666666667, 54.88333333333333, 55.016666666666666, 55.175, 55.36666666666667, 55.53333333333333, 55.666666666666664, 55.86666666666667, 56.083333333333336, 56.25833333333333, 56.458333333333336, 56.583333333333336, 56.666666666666664, 56.75, 56.84166666666667, 57.09166666666667, 57.233333333333334, 57.375, 57.55833333333333, 57.583333333333336, 57.675, 57.75833333333333, 57.925, 58.06666666666667, 58.275, 58.40833333333333, 58.38333333333333, 58.416666666666664, 58.50833333333333, 58.541666666666664, 58.641666666666666, 58.68333333333333, 58.775, 58.825, 58.88333333333333, 58.958333333333336, 59.025, 59.09166666666667, 59.13333333333333, 59.233333333333334, 59.31666666666667, 59.4, 59.458333333333336, 59.53333333333333, 59.625, 59.7, 59.74166666666667, 59.791666666666664, 59.833333333333336, 59.9, 59.925, 59.958333333333336, 59.975, 60.03333333333333, 60.11666666666667, 60.18333333333333, 60.225, 60.275, 60.275, 60.333333333333336, 60.36666666666667], 'time': [1703873184.3395016, 1703873188.1885018, 1703873191.74653, 1703873195.4125347, 1703873199.2805028, 1703873202.8574984, 1703873206.4135027, 1703873210.0285332, 1703873213.625532, 1703873217.2345355, 1703873220.821503, 1703873224.4305027, 1703873228.0215025, 1703873231.798522, 1703873235.4224987, 1703873239.0034986, 1703873242.5375032, 1703873246.0975373, 1703873249.9974995, 1703873254.0204985, 1703873258.798502, 1703873262.692498, 1703873266.701503, 1703873270.7034998, 1703873274.7695308, 1703873278.6334984, 1703873282.3705359, 1703873286.2105021, 1703873290.109498, 1703873293.7004986, 1703873297.371504, 1703873301.2295046, 1703873304.8285036, 1703873308.6995306, 1703873312.2875292, 1703873315.8975368, 1703873319.5065024, 1703873323.0665357, 1703873326.6325352, 1703873330.499538, 1703873334.1755025, 1703873337.9205039, 1703873341.6715, 1703873345.343499, 1703873348.9994996, 1703873352.5794983, 1703873356.132502, 1703873359.725535, 1703873363.306537, 1703873366.9655333, 1703873370.5275023, 1703873374.052503, 1703873377.6445022, 1703873381.2025008, 1703873384.742531, 1703873388.2855005, 1703873391.8815331, 1703873395.3805366, 1703873398.9334984, 1703873402.4845045, 1703873405.994503, 1703873409.5765352, 1703873413.1555355, 1703873416.692539, 1703873420.2695358, 1703873423.8435357, 1703873427.3725035, 1703873430.9085355, 1703873434.4765043, 1703873437.997501, 1703873441.5085018, 1703873445.0425003, 1703873448.5775356, 1703873452.145503, 1703873455.6915019, 1703873459.1914992, 1703873462.7075346, 1703873466.2524981, 1703873469.7525358, 1703873473.298503, 1703873476.8185337, 1703873480.4035356, 1703873483.9434988, 1703873487.502501, 1703873491.0235026, 1703873494.4624977, 1703873498.008499, 1703873501.5634985, 1703873505.1045027, 1703873508.6104982, 1703873512.1935358, 1703873515.6935427, 1703873519.198533, 1703873522.7155004, 1703873526.2585351, 1703873529.9525323, 1703873533.793535, 1703873537.4435356, 1703873541.0405002, 1703873544.5664992, 1703873548.0734975, 1703873551.5734987, 1703873555.0594978, 1703873558.5574987, 1703873562.0594985, 1703873565.543498, 1703873569.099536, 1703873572.6095374, 1703873576.113501, 1703873579.635535, 1703873583.1325357, 1703873586.6285028, 1703873590.1265025, 1703873593.6285346, 1703873597.2464995, 1703873600.768535, 1703873604.358533, 1703873607.8424993, 1703873611.3244975, 1703873614.7965043, 1703873618.399499, 1703873621.9274986, 1703873625.4225006, 1703873628.9004989, 1703873632.4055023, 1703873635.9894989, 1703873639.458503, 1703873642.9644985, 1703873646.4784985, 1703873649.9695032, 1703873653.4464984, 1703873656.9505029, 1703873660.4524977, 1703873663.9995322, 1703873667.4694974, 1703873670.9974983, 1703873674.495502, 1703873677.9714994, 1703873681.4595375, 1703873684.906499, 1703873688.8154988, 1703873692.9855366, 1703873696.9705014, 1703873700.941535, 1703873704.7325015, 1703873708.6955342, 1703873712.2925386, 1703873715.8814983, 1703873719.3925016, 1703873722.964502, 1703873726.4614983, 1703873730.088498, 1703873733.6855006, 1703873737.2135024, 1703873740.731503, 1703873744.236503, 1703873747.6925356]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyper_tuning(x_train, y_train, lr_fn=None):\n",
    "  from sklearn.model_selection import KFold\n",
    "  import pandas as pd\n",
    "  kf = KFold(5)\n",
    "  acc_vals = []\n",
    "  # hidden_units = [64, 128, 256, 512]\n",
    "  # activations = [relu] #,leaky_relu, tanh ]\n",
    "  learning_rate = [0.001, 0.002, 0.004]\n",
    "  batch_size = [16, 32, 64]\n",
    "  momentum = [0.9, 0.99, 0.75, 0.5]\n",
    "  for btch in batch_size:\n",
    "    print('batchsize:',btch)\n",
    "    for lr in learning_rate:\n",
    "      for mom in momentum:\n",
    "        print('--------New Model----------')\n",
    "        print(f\"learning rate: {lr}\\t Batch Size:{btch}\\t Momentum:{mom}\")\n",
    "\n",
    "        optimizer = GradientDescent(learning_rate = lr, batch_size=btch, momentum=mom, lr_lamda=lr_fn)\n",
    "        \n",
    "        avg_acc = 0;       \n",
    "        # print(f\"for M=128, nonlinearity={activ}, lr={lr}, batch size={btch}.\")\n",
    "        start = time.time()\n",
    "        for k, (train, test) in enumerate(kf.split(x_train, y_train)):\n",
    "            print('k:',k)\n",
    "            temp_model = MLP(M=128)\n",
    "            temp_model, log, temp_acc = train_model(temp_model, optimizer, x_train[train], y_train[train])\n",
    "            avg_acc += temp_acc\n",
    "        avg_acc = avg_acc/5\n",
    "        acc_vals.append(avg_acc)\n",
    "        end = time.time()\n",
    "        print('time elapsed:',(end-start)/60/60,\"hrs\")\n",
    "        print('acc:',avg_acc)\n",
    "      \n",
    "  data = {'learningRate' : [0.001, 0.002, 0.004, 0.001, 0.002, 0.004, 0.001, 0.002, 0.004], \n",
    "          'batchSize':[16, 16, 16, 32, 32, 32, 64, 64, 64],\n",
    "          'accuracies': acc_vals\n",
    "          }\n",
    "  acc = pd.DataFrame(data)\n",
    "  print(acc)\n",
    "  return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyper_tuning_gd(x_train, y_train, print_every=100, lr_fn=None):\n",
    "  from sklearn.model_selection import KFold\n",
    "  import pandas as pd\n",
    "  kf = KFold(5)\n",
    "  acc_vals = []\n",
    "  # hidden_units = [64, 128, 256, 512]\n",
    "  # activations = [relu] #,leaky_relu, tanh ]\n",
    "  learning_rate = [0.001, 0.002, 0.004]\n",
    "  batch_size = [16, 32, 64]\n",
    "  for btch in batch_size:\n",
    "    print('batchsize:',btch)\n",
    "    for lr in learning_rate:\n",
    "      \n",
    "      \n",
    "      print('--------New Model----------')\n",
    "      print(f\"learning rate: {lr}\\t Batch Size:{btch}\")\n",
    "\n",
    "      optimizer = GradientDescent(learning_rate = lr, batch_size=btch, lr_fn=lr_fn)\n",
    "      \n",
    "      avg_acc = 0;       \n",
    "      # print(f\"for M=128, nonlinearity={activ}, lr={lr}, batch size={btch}.\")\n",
    "      start = time.time()\n",
    "      for k, (train, valid) in enumerate(kf.split(x_train, y_train)):\n",
    "          print('k:',k)\n",
    "          temp_model = MLP(M=128)\n",
    "          temp_model, log, max_acc = train_model(temp_model, optimizer, x_train[train], y_train[train], x_train[valid], y_train[valid], print_every=print_every)\n",
    "          avg_acc += max_acc\n",
    "      avg_acc = avg_acc/5\n",
    "      acc_vals.append(avg_acc)\n",
    "      end = time.time()\n",
    "      print('time elapsed:',(end-start)/60/60,\"hrs\")\n",
    "      print('acc:',avg_acc)\n",
    "      \n",
    "  data = {'learningRate' : [0.001, 0.002, 0.004, 0.001, 0.002, 0.004, 0.001, 0.002, 0.004], \n",
    "          'batchSize':[16, 16, 16, 32, 32, 32, 64, 64, 64],\n",
    "          'accuracies': acc_vals\n",
    "          }\n",
    "  acc = pd.DataFrame(data)\n",
    "  print(acc)\n",
    "  return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batchsize: 16\n",
      "--------New Model----------\n",
      "learning rate: 0.001\t Batch Size:16\n",
      "k: 0\n",
      "params initialized\n",
      "acc: 9.6\n",
      "Epoch 0: 9.6%\n",
      "Epoch 100: 9.65%\n",
      "Epoch 200: 9.65%\n",
      "Epoch 300: 9.65%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[248], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mhyper_tuning_gd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#x_valid=x_train[2000:2200], y_valid=y_train[2000:2200])\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[246], line 26\u001b[0m, in \u001b[0;36mhyper_tuning_gd\u001b[1;34m(x_train, y_train)\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mk:\u001b[39m\u001b[38;5;124m'\u001b[39m,k)\n\u001b[0;32m     25\u001b[0m     temp_model \u001b[38;5;241m=\u001b[39m MLP(M\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m)\n\u001b[1;32m---> 26\u001b[0m     temp_model, temp_accs, max_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtemp_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43mvalid\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43mvalid\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m     avg_acc \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m max_acc\n\u001b[0;32m     28\u001b[0m avg_acc \u001b[38;5;241m=\u001b[39m avg_acc\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m5\u001b[39m\n",
      "Cell \u001b[1;32mIn[242], line 13\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, optimizer, x_train, y_train, x_valid, y_valid, print_every)\u001b[0m\n\u001b[0;32m     11\u001b[0m t\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m t \u001b[38;5;241m<\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mmax_iters:\n\u001b[1;32m---> 13\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;66;03m# print('here')\u001b[39;00m\n\u001b[0;32m     15\u001b[0m     y_test_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(x_valid)\n",
      "Cell \u001b[1;32mIn[189], line 36\u001b[0m, in \u001b[0;36mMLP.fit\u001b[1;34m(self, x, y, optimizer)\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maggr_params, train_accs, batch_train_acc \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mmini_batch_step(gradient, x, y, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maggr_params)\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 36\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams, train_accs, batch_train_acc \u001b[38;5;241m=\u001b[39m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_mini_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m, train_accs, batch_train_acc\n",
      "Cell \u001b[1;32mIn[244], line 33\u001b[0m, in \u001b[0;36mGradientDescent.run_mini_batch\u001b[1;34m(self, gradient_fn, x, y, params, batch_size)\u001b[0m\n\u001b[0;32m     30\u001b[0m train_acc, batch_train_acc, chunk \u001b[38;5;241m=\u001b[39m [], [], []\n\u001b[0;32m     31\u001b[0m norms \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([np\u001b[38;5;241m.\u001b[39minf])\n\u001b[1;32m---> 33\u001b[0m mini_batches \u001b[38;5;241m=\u001b[39m \u001b[43mmini_batcher\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlearning_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr_fn(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlearning_rate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mt)\n",
      "Cell \u001b[1;32mIn[10], line 3\u001b[0m, in \u001b[0;36mmini_batcher\u001b[1;34m(x, y, mini_batch_size)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmini_batcher\u001b[39m(x, y, mini_batch_size):\n\u001b[0;32m      2\u001b[0m   zipped \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mhstack( (x, y ) )\n\u001b[1;32m----> 3\u001b[0m   \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mzipped\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m   x_batches, y_batches \u001b[38;5;241m=\u001b[39m [], []\n\u001b[0;32m      5\u001b[0m   mini_batches \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hyper_tuning_gd(x_train=x_train[:10000], y_train=y_train[:10000], print_every=10) #x_valid=x_train[2000:2200], y_valid=y_train[2000:2200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyper_tuning_rsag(x_train, \n",
    "                      y_train ,\n",
    "                      x_valid=None,\n",
    "                      y_valid=None):\n",
    "  from sklearn.model_selection import KFold\n",
    "  import pandas as pd\n",
    "  kf = KFold(5)\n",
    "  acc_vals = []\n",
    "  # hidden_units = [64, 128, 256, 512]\n",
    "  # activations = [relu] #,leaky_relu, tanh ]\n",
    "  learning_rate = [0.001, 0.002, 0.004]\n",
    "  alphas = [.9, .75, .7, .5]\n",
    "  betas = [.001, .002, 0.004]\n",
    "#   batch_size = [16, 32, 64]\n",
    "  for alpha in alphas:\n",
    "    print('alpha:',alpha)\n",
    "    for beta in betas:\n",
    "        for lr in learning_rate:\n",
    "            print('--------New Model----------')\n",
    "            print(f\"learning rate: {lr}\\t alpha: {alpha}\\t beta:{beta}\")\n",
    "            optimizer = RSAG(learning_rate = lr, alpha=alpha, beta=beta, batch_size=64)\n",
    "            # for activ in activations:\n",
    "            # for hu in hidden_units:   \n",
    "            avg_acc = 0;       \n",
    "            # print(f\"for M=128, nonlinearity={activ}, lr={lr}, batch size={btch}.\")\n",
    "            start = time.time()\n",
    "            for k, (train, valid) in enumerate(kf.split(x_train, y_train)):\n",
    "                print('k:',k)\n",
    "                temp_model = MLP(M=128, rsag=True)\n",
    "                temp_model, temp_accs, max_acc = train_model(temp_model, optimizer, x_train[train], y_train[train], x_train[valid], y_train[valid])\n",
    "                avg_acc += max_acc\n",
    "            avg_acc = avg_acc/5\n",
    "            acc_vals.append(avg_acc)\n",
    "            end = time.time()\n",
    "            print('time elapsed:',(end-start)/60/60,\"hrs\")\n",
    "            print('acc:',avg_acc)\n",
    "  data = {'learningRate' : [0.001, 0.002, 0.004, 0.001, 0.002, 0.004, 0.001, 0.002, 0.004], \n",
    "          'batchSize':[16, 16, 16, 32, 32, 32, 64, 64, 64],\n",
    "          'accuracies': acc_vals\n",
    "          }\n",
    "  acc = pd.DataFrame(data)\n",
    "  print(acc)\n",
    "  return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "\n",
    "def train_model(model, \n",
    "                optimizer, \n",
    "                x_train, \n",
    "                y_train, \n",
    "                x_valid, \n",
    "                y_valid, \n",
    "                verbose=True, \n",
    "                print_every=100, \n",
    "                patience=20,\n",
    "                save_log=False):\n",
    "    # x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "    # model.fit(x_train, y_train, optimizer)\n",
    "\n",
    "    # y_test_pred = model.predict(x_valid)\n",
    "    # temp_v_acc = evaluate_acc(y_test_pred, y_valid)\n",
    "    # v_acc = [temp_v_acc]\n",
    "\n",
    "    # print('acc:',temp_v_acc)\n",
    "    log = {'loss': [], 'accuracy': [], 'v_loss': [], 'v_accuracy': [], 'time': []}\n",
    "    t=0\n",
    "    log['time'].append(time.time())\n",
    "    curr_patience = 0\n",
    "\n",
    "    while t < optimizer.max_iters:\n",
    "        \n",
    "        model.fit(x_train, y_train, optimizer)\n",
    "        log['accuracy'].append(evaluate_acc(model.predict(x_train), y_train))\n",
    "        log['loss'].append(log_loss(y_train, model.predict(x_train)))\n",
    "\n",
    "        y_valid_pred = model.predict(x_valid)\n",
    "        temp_v_acc = evaluate_acc(y_valid_pred, y_valid)\n",
    "        log['v_accuracy'].append(temp_v_acc)\n",
    "        log['v_loss'].append(log_loss(y_valid, y_valid_pred))\n",
    "        \n",
    "        if verbose:\n",
    "            if t%print_every == 0:\n",
    "                print('Epoch {}/{}'.format(t, optimizer.max_iters))\n",
    "                print('-' * 10)\n",
    "                print('Loss {:.4f}'.format(log['loss'][-1]))\n",
    "                print('Accuracy:  {:.4f}'.format(log['accuracy'][-1]))\n",
    "                print('Validation Loss {:.4f}'.format(log['v_loss'][-1]))\n",
    "                print('Validation Accuracy:  {:.4f}'.format(log['v_accuracy'][-1]))\n",
    "                print('Time :', log['time'][-1]-log['time'][0])\n",
    "        \n",
    "        log['time'].append(time.time())\n",
    "\n",
    "        if len(log['v_accuracy']) > 1 and (np.abs(log['v_accuracy'][-1]-log['v_accuracy'][-2])<0.1):\n",
    "            curr_patience += 1\n",
    "            if curr_patience > patience:\n",
    "                print('Early stopping at epoch %d'%t)\n",
    "                break\n",
    "        else:\n",
    "            curr_patience = 0\n",
    "        t+=1\n",
    "    print(log)\n",
    "    # if save_log == True:\n",
    "    #     np.save('log.csv', log)\n",
    "    return model, log, max(log['v_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha: 0.9\n",
      "--------New Model----------\n",
      "learning rate: 0.001\t alpha: 0.9\t beta:0.001\n",
      "k: 0\n",
      "params initialized\n",
      "acc: 13.0\n",
      "Epoch0: 13.0%\n",
      "Epoch100: 30.0%\n",
      "Epoch200: 36.5%\n",
      "Epoch300: 45.0%\n",
      "Epoch400: 48.5%\n",
      "Epoch500: 50.0%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_6360\\834803681.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mhyper_tuning_rsag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2000\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2000\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#, x_valid=x_train[2000:2200], y_valid=y_train[2000:2200])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_6360\\2116129435.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(x_train, y_train, x_valid, y_valid)\u001b[0m\n\u001b[0;32m     26\u001b[0m             \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'k:'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m                 \u001b[0mtemp_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMLP\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mM\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrsag\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m                 \u001b[0mtemp_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtemp_accs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemp_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mvalid\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mvalid\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m                 \u001b[0mavg_acc\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mmax_acc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m             \u001b[0mavg_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mavg_acc\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m             \u001b[0macc_vals\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mavg_acc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_6360\\1026297623.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(model, optimizer, x_train, y_train, x_valid, y_valid, print_every)\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'acc:'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtemp_v_acc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mt\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[0mt\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_iters\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m         \u001b[1;31m# print('here')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0my_test_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_valid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mtemp_v_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluate_acc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_6360\\1555615134.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, x, y, optimizer)\u001b[0m\n\u001b[0;32m     30\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maggr_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'params initialized'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrsag\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maggr_params\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_accs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_train_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmini_batch_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maggr_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_accs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_train_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_mini_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_6360\\3483094965.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, gradient_fn, x, y, params, agg_params, batch_size)\u001b[0m\n\u001b[0;32m     49\u001b[0m                 \u001b[1;31m# x_temp, y_temp = mini_batches[t %  len(mini_batches) ][0], mini_batches[t % len(mini_batches) ][1]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m                 \u001b[1;31m# print(x_temp.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[1;31m# x_val, y_val = mini_batches[t %  len(mini_batches)]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m             \u001b[0mproj_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0ma_p\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malpha\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mp\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma_p\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0magg_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m             \u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtemp_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgradient_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_temp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_temp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproj_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m             \u001b[1;31m# if grad == None: grad = temp_grad\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_6360\\3483094965.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(.0)\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m     def mini_batch_step(self, \n\u001b[0m\u001b[0;32m     54\u001b[0m                        \u001b[0mgradient_fn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m                        \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m                        \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\camgr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 155\u001b[1;33m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\camgr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(y, x)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1510\u001b[0m       \u001b[1;31m# TODO(b/178860388): Figure out why binary_op_wrapper and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1511\u001b[0m       \u001b[1;31m#   r_binary_op_wrapper use different force_same_dtype values.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1512\u001b[0m       \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmaybe_promote_tensors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mforce_same_dtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1513\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\camgr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(x, y, name)\u001b[0m\n\u001b[0;32m   1877\u001b[0m     new_vals = gen_sparse_ops.sparse_dense_cwise_mul(y.indices, y.values,\n\u001b[0;32m   1878\u001b[0m                                                      y.dense_shape, x, name)\n\u001b[0;32m   1879\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0msparse_tensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSparseTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdense_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1880\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1881\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mmultiply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\camgr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\ops\\weak_tensor_ops.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    140\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_auto_dtype_conversion_enabled\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 142\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    143\u001b[0m     \u001b[0mbound_arguments\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m     \u001b[0mbound_arguments\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_defaults\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m     \u001b[0mbound_kwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbound_arguments\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marguments\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\camgr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 155\u001b[1;33m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\camgr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1257\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1258\u001b[0m       \u001b[1;31m# Fallback dispatch system (dispatch v1):\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1259\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1260\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdispatch_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1261\u001b[1;33m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1262\u001b[0m         \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1263\u001b[0m         \u001b[1;31m# TypeError, when given unexpected types.  So we need to catch both.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1264\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop_dispatch_handler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\camgr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(x, y, name)\u001b[0m\n\u001b[0;32m    526\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    527\u001b[0m    \u001b[1;33m*\u001b[0m \u001b[0mInvalidArgumentError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mWhen\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mx\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0my\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mhave\u001b[0m \u001b[0mincompatible\u001b[0m \u001b[0mshapes\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    528\u001b[0m   \"\"\"\n\u001b[0;32m    529\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 530\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\camgr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(x, y, name)\u001b[0m\n\u001b[0;32m   6752\u001b[0m         _ctx, \"Mul\", name, x, y)\n\u001b[0;32m   6753\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6754\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6755\u001b[0m       \u001b[0m_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6756\u001b[1;33m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   6757\u001b[0m       \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6758\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6759\u001b[0m       return mul_eager_fallback(\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hyper_tuning_rsag(x_train=x_train[:2000], y_train=y_train[:2000]) #, x_valid=x_train[2000:2200], y_valid=y_train[2000:2200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
