{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../rsag_convex.png\" alt=\"algoconvex\" />\n",
    "<img src=\"../x_update.png\" alt=\"x_update\" />\n",
    "<img src=\"../mean.png\" alt=\"mean\" />\n",
    "<img src=\"../rsag_composite.png\" alt=\"algo\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Parameters :__\n",
    "- $\\alpha$: (1-$\\alpha$) weight of aggregated x on current state, i.e. momentum\n",
    "- $\\lambda$: learning rate\n",
    "- $\\beta$: change for aggregated x\n",
    "- $p_k$ termination probability\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  torch.optim import Adam, SGD, RMSprop\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch import nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as data_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch version: 2.1.2+cu121\n",
      "Using GPU, device name: NVIDIA GeForce GTX 1660 Ti\n"
     ]
    }
   ],
   "source": [
    "print('Using PyTorch version:', torch.__version__)\n",
    "if torch.cuda.is_available():\n",
    "    print('Using GPU, device name:', torch.cuda.get_device_name(0))\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    print('No GPU found, using CPU instead.') \n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import path\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from models import MLP\n",
    "from optimizers import RSAG, AccSGD\n",
    "from util import DataLoader\n",
    "from util import calc_accuracy, train_model, HPScheduler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run MLP:\n",
    "__TUNE DIFFERENT OPTIMIZERS__:\n",
    "- Nesterov w/ weight decay w/ Scheduled LR (SGD)\n",
    "- Momentum w/ weight decay w/ Scheduled LR (SGD)\n",
    "- Basic SGD\n",
    "- Adagrad?\n",
    "- Adam?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataLoader()\n",
    "loaders = data_loader.get_loaders()\n",
    "# loss_function = torch.nn.CrossEntropyLoss()\n",
    "# model = MLP().to(device)\n",
    "# print(model)\n",
    "\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=1e-4, nesterov=True, momentum=0.9)\n",
    "# optimizer = RSAG(model.parameters(), lr=1e-4, alpha=.9, beta=9e-5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RSAG Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import warnings\n",
    "import copy\n",
    "\n",
    "class RSAG(torch.optim.Optimizer):\n",
    "    r\"\"\"\n",
    "    Args:\n",
    "        params (iterable): iterable of parameters to optimize or dicts defining\n",
    "            parameter groups\n",
    "        lr (float): learning rate (lambda) (required)\n",
    "        kappa (float): lambda  (default: 1000)\n",
    "        xi (float, optional): statistical advantage parameter (default: 10)\n",
    "        smallConst (float, optional): any value <=1 (default: 0.7)\n",
    "    Example:\n",
    "        >>> from RSAG import *\n",
    "        >>> optimizer = RSAG(model.parameters(), lr=0.1, kappa = 1000.0, xi = 10.0)\n",
    "        >>> optimizer.zero_grad()\n",
    "        >>> loss_fn(model(input), target).backward()\n",
    "        >>> optimizer.step()\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 params, \n",
    "                 lr=0.01, \n",
    "                 alpha = 0.1, \n",
    "                 beta = 0.1): #, smallConst = 0.7, weight_decay=0):\n",
    "        #defaults = dict(lr=lr, kappa=kappa, xi, smallConst=smallConst,\n",
    "                        # weight_decay=weight_decay)\n",
    "        \n",
    "        if lr < 0.0:\n",
    "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
    "        if alpha < 0.0 or alpha > 1.0:\n",
    "            raise ValueError(\"Invalid alpha: {}\".format(alpha))\n",
    "        if beta < 0.0:\n",
    "            raise ValueError(\"Invalid beta: {}\".format(beta))\n",
    "        \n",
    "        defaults = dict(lr=lr, alpha=alpha, beta=beta)\n",
    "        super(RSAG, self).__init__(params, defaults)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super(RSAG, self).__setstate__(state)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        \"\"\" Performs a single optimization step.\n",
    "        Arguments:\n",
    "            closure (callable, optional): A closure that reevaluates the model\n",
    "                and returns the loss.\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            # weight_decay = group['weight_decay']\n",
    "            lr = group['lr']\n",
    "            alpha, beta = group['alpha'], group['beta']\n",
    "            alpha_bar = 1.0-alpha\n",
    "            momentum_buffer_list = []\n",
    "\n",
    "            # INITIALIZE GROUPS\n",
    "            # params_with_grad, d_p_list, momentum_buffer_list = [], [], []\n",
    "            # for p in group['params']:\n",
    "            #     if p.grad is not None:\n",
    "            #         params_with_grad.append(p)\n",
    "            #         d_p_list.append(p.grad)\n",
    "            #         # if p.grad.is_sparse:\n",
    "            #         #     has_sparse_grad = True\n",
    "\n",
    "            #         state = self.state[p]\n",
    "            #         if 'momentum_aggr' not in state:\n",
    "            #             momentum_buffer_list.append(None)\n",
    "            #         else:\n",
    "            #             momentum_buffer_list.append(state['momentum_buffer'])\n",
    "            \n",
    "            # UPDATE GROUPS\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "\n",
    "                d_w = p.grad.data\n",
    "                # w = p.data\n",
    "                param_state = self.state[p]\n",
    "\n",
    "                # if weight_decay != 0:\n",
    "                #     grad_d.add_(weight_decay, p.data)\n",
    "                \n",
    "                if 'momentum_aggr' not in param_state:\n",
    "                    param_state['momentum_aggr'] = copy.deepcopy(p.data)\n",
    "                    param_state['prev_momentum_aggr'] = copy.deepcopy(p.data)\n",
    "                buf = param_state['momentum_aggr']\n",
    "                aggr_grad = (buf-param_state['prev_momentum_aggr'])\n",
    "                aggr_grad.mul_(alpha_bar)\n",
    "                aggr_grad.add_(d_w, alpha=alpha)\n",
    "                \n",
    "                param_state['prev_momentum_aggr'] = copy.deepcopy(buf)\n",
    "                \n",
    "                # Update momentum buffer:'\n",
    "                buf.mul_(alpha_bar)\n",
    "                buf.add_(p.data, alpha=alpha)\n",
    "                buf.add_(aggr_grad, alpha=-beta)\n",
    "                \n",
    "                p.data.add_(aggr_grad, alpha=-lr)\n",
    "                # print('aggr_grad', aggr_grad)\n",
    "            \n",
    "            # UPDATE MOMENTUM BUFFER\n",
    "            # for p, momentum_buffer in zip(params_with_grad, momentum_buffer_list):\n",
    "            #     state = self.state[p]\n",
    "                \n",
    "            #     state['momentum_buffer'] = momentum_buffer\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_nesterov(alpha_values, lr_values, save_log=False):\n",
    "    loss_function = torch.nn.CrossEntropyLoss()\n",
    "    best_alpha, best_lr = 0.0, 0.0\n",
    "    best_accuracy = 0.0\n",
    "    v_accs, acc_std, v_loss, loss_std = [], [], [], []\n",
    "    acc, loss = [], []\n",
    "    \n",
    "    for alpha in alpha_values:\n",
    "        for lr in lr_values:\n",
    "            beta = lr * alpha\n",
    "            \n",
    "            print(f\"----------- Training with alpha={alpha}, lr={lr} -----------------\")\n",
    "            \n",
    "            model = MLP().to(device)\n",
    "            optimizer = SGD(model.parameters(), lr=lr, nesterov=True, momentum=0.9)\n",
    "\n",
    "\n",
    "            log, best_accuracy = train_model(model,loss_function,optimizer,loaders,print_every=5):\n",
    "            if log['v_accuracy'][-1] > best_accuracy:\n",
    "                print(f\"Found a new best accuracy: {log['v_accuracy'][-1]}\")\n",
    "                print(f\"best alpha: {alpha}, best lr: {lr}\")\n",
    "                best_accuracy = log['v_accuracy'][-1]\n",
    "                best_alpha = alpha\n",
    "                best_lr = lr\n",
    "            \n",
    "            v_accs.append(log['v_accuracy'])\n",
    "            acc_std.append(log['v_accuracy_std'])\n",
    "            v_loss.append(log['v_loss'])\n",
    "            loss_std.append(log['v_loss_std'])\n",
    "            acc.append(log['accuracy'])\n",
    "            loss.append(log['loss'])\n",
    "            \n",
    "\n",
    "    \n",
    "    return best_alpha, best_lr, v_accs, acc_std, v_loss, loss_std, acc, loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (layers): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (1): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (2): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "Starting Epoch 1\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'copy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# optimizer = torch.optim.SGD(model.parameters(), lr=1e-4, nesterov=True, momentum=0.9)\u001b[39;00m\n\u001b[0;32m      6\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m RSAG(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-4\u001b[39m, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m.9\u001b[39m, beta\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m9e-5\u001b[39m)\n\u001b[1;32m----> 8\u001b[0m log \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\camgr\\Documents\\stoch_prog\\notebooks\\..\\util\\train_pipeline.py:54\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, loss_function, optimizer, loaders, device, verbose, save_path, log_path, n_epochs)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# current_loss += loss\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# Backpropagation\u001b[39;00m\n\u001b[0;32m     53\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m---> 54\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m current_loss\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[0;32m     57\u001b[0m total_acc\u001b[38;5;241m.\u001b[39mappend(calc_accuracy(outputs, targets))\n",
      "File \u001b[1;32mc:\\Users\\camgr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\optim\\optimizer.py:373\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    368\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    369\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    370\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    371\u001b[0m             )\n\u001b[1;32m--> 373\u001b[0m out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    374\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    376\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\camgr\\Documents\\stoch_prog\\notebooks\\..\\optimizers\\rsag.py:87\u001b[0m, in \u001b[0;36mRSAG.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;66;03m# if weight_decay != 0:\u001b[39;00m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;66;03m#     grad_d.add_(weight_decay, p.data)\u001b[39;00m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmomentum_aggr\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m param_state:\n\u001b[1;32m---> 87\u001b[0m     param_state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmomentum_aggr\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mcopy\u001b[49m\u001b[38;5;241m.\u001b[39mdeepcopy(p\u001b[38;5;241m.\u001b[39mdata)\n\u001b[0;32m     88\u001b[0m     param_state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprev_momentum_aggr\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(p\u001b[38;5;241m.\u001b[39mdata)\n\u001b[0;32m     89\u001b[0m buf \u001b[38;5;241m=\u001b[39m param_state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmomentum_aggr\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'copy' is not defined"
     ]
    }
   ],
   "source": [
    "model = MLP().to(device)\n",
    "print(model)\n",
    "\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "log = train_model(model, loss_function, optimizer, loaders, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 58)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m<tokenize>:58\u001b[1;36m\u001b[0m\n\u001b[1;33m    loss_function = torch.nn.CrossEntropyLoss()\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "def train_with_hyperparameters(alpha_values, lr_values, save_log=False):\n",
    "    loss_function = torch.nn.CrossEntropyLoss()\n",
    "    best_alpha, best_lr = 0.0, 0.0\n",
    "    best_accuracy = 0.0\n",
    "    v_accs, acc_std, v_loss, loss_std = [], [], [], []\n",
    "    acc, loss = [], []\n",
    "    \n",
    "    for alpha in alpha_values:\n",
    "        for lr in lr_values:\n",
    "            beta = lr * alpha\n",
    "            \n",
    "            print(f\"----------- Training with alpha={alpha}, lr={lr} -----------------\")\n",
    "            \n",
    "            model = MLP().to(device)\n",
    "            optimizer = RSAG(model.parameters(), lr=lr, alpha=alpha, beta=beta)\n",
    "            log = train_model(model, loaders, optimizer, loss_function, device, epochs=20)\n",
    "            \n",
    "            if log['v_accuracy'][-1] > best_accuracy:\n",
    "                print(f\"Found a new best accuracy: {log['v_accuracy'][-1]}\")\n",
    "                print(f\"best alpha: {alpha}, best lr: {lr}\")\n",
    "                best_accuracy = log['v_accuracy'][-1]\n",
    "                best_alpha = alpha\n",
    "                best_lr = lr\n",
    "            \n",
    "            v_accs.append(log['v_accuracy'])\n",
    "            acc_std.append(log['v_accuracy_std'])\n",
    "            v_loss.append(log['v_loss'])\n",
    "            loss_std.append(log['v_loss_std'])\n",
    "            acc.append(log['accuracy'])\n",
    "            loss.append(log['loss'])\n",
    "            \n",
    "\n",
    "    \n",
    "    return best_alpha, best_lr, v_accs, acc_std, v_loss, loss_std, acc, loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Epoch 1\n",
      "Epoch 1 finished\n",
      "loss 4.6072\n",
      "Accuracy:  7.4900\n",
      "Starting Epoch 2\n",
      "Epoch 2 finished\n",
      "loss 4.6027\n",
      "Accuracy:  8.8633\n",
      "Starting Epoch 3\n",
      "Epoch 3 finished\n",
      "loss 4.5982\n",
      "Accuracy:  10.3017\n",
      "Starting Epoch 4\n",
      "Epoch 4 finished\n",
      "loss 4.5937\n",
      "Accuracy:  11.6917\n",
      "Starting Epoch 5\n",
      "Epoch 5 finished\n",
      "loss 4.5893\n",
      "Accuracy:  13.1383\n",
      "Training has completed\n"
     ]
    }
   ],
   "source": [
    "\n",
    "optimizer = RSAG(model.parameters(), lr=1e-4, alpha=.9, beta=9e-5)\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=1e-4, nesterov=True, momentum=0.9)\n",
    "train_model(model, loaders, optimizer, loss_function, device, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
