{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../rsag_convex.png\" alt=\"algoconvex\" />\n",
    "<img src=\"../x_update.png\" alt=\"x_update\" />\n",
    "<img src=\"../mean.png\" alt=\"mean\" />\n",
    "<img src=\"../rsag_composite.png\" alt=\"algo\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Parameters :__\n",
    "- $\\alpha$: (1-$\\alpha$) weight of aggregated x on current state, i.e. momentum\n",
    "- $\\lambda$: learning rate\n",
    "- $\\beta$: change for aggregated x\n",
    "- $p_k$ termination probability\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.optimizer import Optimizer, required\n",
    "import copy\n",
    "\n",
    "class RSAG(Optimizer):\n",
    "    r\"\"\"\n",
    "    Args:\n",
    "        params (iterable): iterable of parameters to optimize or dicts defining\n",
    "            parameter groups\n",
    "        lr (float): learning rate (lambda) (required)\n",
    "        kappa (float): lambda  (default: 1000)\n",
    "        xi (float, optional): statistical advantage parameter (default: 10)\n",
    "        smallConst (float, optional): any value <=1 (default: 0.7)\n",
    "    Example:\n",
    "        >>> from RSAG import *\n",
    "        >>> optimizer = RSAG(model.parameters(), lr=0.1, kappa = 1000.0, xi = 10.0)\n",
    "        >>> optimizer.zero_grad()\n",
    "        >>> loss_fn(model(input), target).backward()\n",
    "        >>> optimizer.step()\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, params, lr=0.01, alpha = 0.1, beta = 0.1): #, smallConst = 0.7, weight_decay=0):\n",
    "        #defaults = dict(lr=lr, kappa=kappa, xi, smallConst=smallConst,\n",
    "                        # weight_decay=weight_decay)\n",
    "        defaults = dict(lr=lr, alpha=alpha, beta=beta)\n",
    "        super(RSAG, self).__init__(params, defaults)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super(RSAG, self).__setstate__(state)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        \"\"\" Performs a single optimization step.\n",
    "        Arguments:\n",
    "            closure (callable, optional): A closure that reevaluates the model\n",
    "                and returns the loss.\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            weight_decay = group['weight_decay']\n",
    "            lr = group['lr']\n",
    "            alpha, beta = group['alpha'], group['beta']\n",
    "\n",
    "            \n",
    "            # Alpha = 1.0 - ((group['smallConst']*group['smallConst']*group['xi'])/group['kappa'])\n",
    "            # Beta = 1.0 - Alpha\n",
    "            # zeta = group['smallConst']/(group['smallConst']+Beta)\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad_d = p.grad.data\n",
    "\n",
    "                # if weight_decay != 0:\n",
    "                #     grad_d.add_(weight_decay, p.data)\n",
    "                param_state = self.state[p]\n",
    "                if 'momentum_aggr' not in param_state:\n",
    "                    param_state['momentum_aggr'] = copy.deepcopy(p.data)\n",
    "                aggr = param_state['momentum_aggr']\n",
    "                aggr.mul_((1.0/Beta)-1.0)\n",
    "                aggr.add_(-large_lr,d_p)\n",
    "                aggr.add_(p.data)\n",
    "                aggr.mul_(Beta)\n",
    "\n",
    "                p.data.add_(-group['lr'],d_p)\n",
    "                p.data.mul_(zeta)\n",
    "                p.data.add_(1.0-zeta,buf)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Optimizer and test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "optim = Adan(\n",
    "    model.parameters(),\n",
    "    lr = 1e-3,                  # learning rate (can be much higher than Adam, up to 5-10x)\n",
    "    betas = (0.02, 0.08, 0.01), # beta 1-2-3 as described in paper - author says most sensitive to beta3 tuning\n",
    "    weight_decay = 0.02         # weight decay 0.02 is optimal per author\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
