{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../rsag_convex.png\" alt=\"algoconvex\" />\n",
    "<img src=\"../x_update.png\" alt=\"x_update\" />\n",
    "<img src=\"../mean.png\" alt=\"mean\" />\n",
    "<img src=\"../rsag_composite.png\" alt=\"algo\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Parameters :__\n",
    "- $\\alpha$: (1-$\\alpha$) weight of aggregated x on current state, i.e. momentum\n",
    "- $\\lambda$: learning rate\n",
    "- $\\beta$: change for aggregated x\n",
    "- $p_k$ termination probability\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  torch.optim import Adam, SGD, RMSprop\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import warnings\n",
    "import copy\n",
    "import torch.utils.data as data_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch version: 2.1.2+cu121\n",
      "Using GPU, device name: NVIDIA GeForce GTX 1660 Ti\n"
     ]
    }
   ],
   "source": [
    "print('Using PyTorch version:', torch.__version__)\n",
    "if torch.cuda.is_available():\n",
    "    print('Using GPU, device name:', torch.cuda.get_device_name(0))\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    print('No GPU found, using CPU instead.') \n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import path\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from models import MLP\n",
    "from optimizers import RSAG, AccSGD\n",
    "from util import DataLoader\n",
    "from util import calc_accuracy, train_model, HPScheduler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run MLP:\n",
    "__TUNE DIFFERENT OPTIMIZERS__:\n",
    "- Nesterov w/ weight decay w/ Scheduled LR (SGD)\n",
    "- Momentum w/ weight decay w/ Scheduled LR (SGD)\n",
    "- Basic SGD\n",
    "- Adagrad?\n",
    "- Adam?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataLoader()\n",
    "loaders = data_loader.get_loaders()\n",
    "# loss_function = torch.nn.CrossEntropyLoss()\n",
    "# model = MLP().to(device)\n",
    "# print(model)\n",
    "\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=1e-4, nesterov=True, momentum=0.9)\n",
    "# optimizer = RSAG(model.parameters(), lr=1e-4, alpha=.9, beta=9e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module): \n",
    "    \"\"\"\n",
    "    Very simple\n",
    "    2 hidden layer MLP with 512 and 512 hidden units respectively\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=28*28, output_dim=10, h=512):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(input_dim, h),\n",
    "            nn.Linear(h, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = x.view(-1,28*28)\n",
    "        # x = F.relu(self.layers[0](x))\n",
    "        return self.layers(x)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RSAG Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RSAG(torch.optim.Optimizer):\n",
    "    r\"\"\"\n",
    "    Args:\n",
    "        params (iterable): iterable of parameters to optimize or dicts defining\n",
    "            parameter groups\n",
    "        lr (float): learning rate (lambda) (required)\n",
    "        kappa (float): lambda  (default: 1000)\n",
    "        xi (float, optional): statistical advantage parameter (default: 10)\n",
    "        smallConst (float, optional): any value <=1 (default: 0.7)\n",
    "    Example:\n",
    "        >>> from RSAG import *\n",
    "        >>> optimizer = RSAG(model.parameters(), lr=0.1, kappa = 1000.0, xi = 10.0)\n",
    "        >>> optimizer.zero_grad()\n",
    "        >>> loss_fn(model(input), target).backward()\n",
    "        >>> optimizer.step()\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 params, \n",
    "                 lr=0.01, \n",
    "                 alpha = 0.1, \n",
    "                 beta = 0.1): #, smallConst = 0.7, weight_decay=0):\n",
    "        #defaults = dict(lr=lr, kappa=kappa, xi, smallConst=smallConst,\n",
    "                        # weight_decay=weight_decay)\n",
    "        \n",
    "        if lr < 0.0:\n",
    "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
    "        if alpha < 0.0 or alpha > 1.0:\n",
    "            raise ValueError(\"Invalid alpha: {}\".format(alpha))\n",
    "        if beta < 0.0:\n",
    "            raise ValueError(\"Invalid beta: {}\".format(beta))\n",
    "        \n",
    "        defaults = dict(lr=lr, alpha=alpha, beta=beta)\n",
    "\n",
    "        # proj_params = torch.tensor(params)\n",
    "        # proj_params = params.detach().clone()\n",
    "        # proj_params.requires_grad = True\n",
    "\n",
    "        # super(RSAG, self).__init__(params, proj_params, defaults)\n",
    "        \n",
    "        # ASSUME PARAMS ARE ALREADY PROJECTED\n",
    "        # INIT REAL PARAMS\n",
    "        for p in params:\n",
    "            print('p', p)\n",
    "            p.requires_grad = False\n",
    "\n",
    "        print('params', params)\n",
    "        super(RSAG, self).__init__(params, defaults)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super(RSAG, self).__setstate__(state)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        \"\"\" Performs a single optimization step.\n",
    "        Arguments:\n",
    "            closure (callable, optional): A closure that reevaluates the model\n",
    "                and returns the loss.\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "            print('hereee')\n",
    "        # print('param_group',self.param_groups)\n",
    "        for group in self.param_groups:\n",
    "            # weight_decay = group['weight_decay']\n",
    "            lr = group['lr']\n",
    "            alpha, beta = group['alpha'], group['beta']\n",
    "            alpha_bar = 1.0-alpha\n",
    "            momentum_buffer_list = []\n",
    "            # print('group', group)\n",
    "\n",
    "            # INITIALIZE GROUPS\n",
    "            # params_with_grad, d_p_list, momentum_buffer_list = [], [], []\n",
    "            # for p in group['params']:\n",
    "            #     if p.grad is not None:\n",
    "            #         params_with_grad.append(p)\n",
    "            #         d_p_list.append(p.grad)\n",
    "            #         # if p.grad.is_sparse:\n",
    "            #         #     has_sparse_grad = True\n",
    "\n",
    "            #         state = self.state[p]\n",
    "            #         if 'momentum_aggr' not in state:\n",
    "            #             momentum_buffer_list.append(None)\n",
    "            #         else:\n",
    "            #             momentum_buffer_list.append(state['momentum_buffer'])\n",
    "            \n",
    "            # UPDATE GROUPS\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "\n",
    "                d_w = p.grad.data\n",
    "                # w = p.data\n",
    "                param_state = self.state[p]\n",
    "\n",
    "                # if weight_decay != 0:\n",
    "                #     grad_d.add_(weight_decay, p.data)\n",
    "                \n",
    "                if 'momentum_aggr' not in param_state:\n",
    "                    param_state['momentum_aggr'] = copy.deepcopy(p.data)\n",
    "                    param_state['prev_momentum_aggr'] = copy.deepcopy(p.data)\n",
    "                buf = param_state['momentum_aggr']\n",
    "                aggr_grad = (buf-param_state['prev_momentum_aggr'])\n",
    "                aggr_grad.mul_(alpha_bar)\n",
    "                aggr_grad.add_(d_w, alpha=alpha)\n",
    "                \n",
    "                param_state['prev_momentum_aggr'] = copy.deepcopy(buf)\n",
    "                \n",
    "                # Update momentum buffer:'\n",
    "                buf.mul_(alpha_bar)\n",
    "                buf.add_(p.data, alpha=alpha)\n",
    "                buf.add_(aggr_grad, alpha=-beta)\n",
    "                \n",
    "                p.data.add_(aggr_grad, alpha=-lr)\n",
    "                # print('aggr_grad', aggr_grad)\n",
    "            \n",
    "            # UPDATE MOMENTUM BUFFER\n",
    "            # for p, momentum_buffer in zip(params_with_grad, momentum_buffer_list):\n",
    "            #     state = self.state[p]\n",
    "                \n",
    "            #     state['momentum_buffer'] = momentum_buffer\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "                model,\n",
    "                loss_function,\n",
    "                optimizer,\n",
    "                loaders,\n",
    "                device='cpu',\n",
    "                verbose=True,\n",
    "                save_path=None,\n",
    "                log_path=None,\n",
    "                n_epochs=200,\n",
    "                print_every=1\n",
    "                ):\n",
    "    log = {}\n",
    "    log['loss'], log['accuracy'] = [], []\n",
    "    log['v_loss'], log['v_accuracy'] = [], []\n",
    "    \n",
    "    log['v_loss_std'], log['v_accuracy_std'] = [], []\n",
    "    log['loss_std'], log['accuracy_std'] = [], []\n",
    "\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    for epoch in range(0,n_epochs):\n",
    "        print(f'Starting Epoch {epoch+1}')\n",
    "\n",
    "        current_loss, total_acc = [], []\n",
    "        v_loss, v_acc = [], []\n",
    "\n",
    "        for data, targets in loaders['train']:\n",
    "            # inputs, targets = data\n",
    "            # inputs, targets = inputs.float(), targets.float()\n",
    "            # targets = targets.reshape((targets.shape[0], 1))\n",
    "            \n",
    "            # Copy data and targets to GPU\n",
    "            data = data.to(device)\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(data)\n",
    "\n",
    "            # Calculate the loss\n",
    "            loss = loss_function(outputs, targets)\n",
    "            # current_loss += loss\n",
    "\n",
    "            # Backpropagation\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            current_loss.append(loss.item())\n",
    "            total_acc.append(calc_accuracy(outputs, targets))\n",
    "            \n",
    "        # Validation\n",
    "        model.eval()\n",
    "\n",
    "        for data, targets in loaders['valid']:\n",
    "            data = data.to(device)\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            outputs = model(data)\n",
    "\n",
    "            loss = loss_function(outputs, targets)\n",
    "            v_loss.append(loss.item())\n",
    "            v_acc.append(calc_accuracy(outputs, targets))        \n",
    "\n",
    "            # if i%10 == 0:\n",
    "            #     print(f'Loss after mini-batch %5d: %.3f'%(i+1, current_loss/500))\n",
    "            #     current_loss = 0.0\n",
    "        print(f'Epoch {epoch+1} finished')\n",
    "        # current_loss /= len(loaders['train'])\n",
    "        # total_acc /= len(loaders['train'])\n",
    "        # print('loss {:.4f}'.format(current_loss))\n",
    "        # print('Accuracy:  {:.4f}'.format(total_acc))\n",
    "\n",
    "        update_log(log, current_loss, total_acc, v_loss, v_acc, len(loaders['train']), len(loaders['valid']))\n",
    "\n",
    "        if verbose:\n",
    "            if epoch%print_every == 0:\n",
    "                print('Epoch {}/{}'.format(epoch+1, n_epochs))\n",
    "                print('-' * 10)\n",
    "                print('Loss {:.4f}'.format(log['loss'][-1]))\n",
    "                print('Accuracy:  {:.4f}'.format(log['accuracy'][-1]))\n",
    "                print('Validation Loss {:.4f}'.format(log['v_loss'][-1]))\n",
    "                print('Validation Accuracy:  {:.4f}'.format(log['v_accuracy'][-1]))\n",
    "        \n",
    "        if len(log['v_accuracy']) > 1 and (np.abs(log['v_accuracy'][-1]-log['v_accuracy'][-2])<0.1):\n",
    "            print('Early stopping at epoch %d'%epoch)\n",
    "            break\n",
    "\n",
    "        if log['v_accuracy'][-1] > best_acc:\n",
    "            best_acc = log['v_accuracy'][-1]\n",
    "        \n",
    "    if save_path is not None:\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "        print('Model saved to %s'%save_path)\n",
    "\n",
    "    if log_path is not None:\n",
    "        df = pd.DataFrame.from_dict(log)\n",
    "        df.to_csv(log_path)\n",
    "        print('Log saved to %s'%log_path)\n",
    "\n",
    "    print(\"Training has completed\")\n",
    "    return log, best_acc\n",
    "\n",
    "def update_log(log, current_loss, total_acc, v_loss, v_acc, train_len, valid_len):\n",
    "        log['loss_std'].append(np.std(current_loss))\n",
    "        log['accuracy_std'].append(np.std(total_acc))\n",
    "        current_loss = sum(current_loss)/train_len\n",
    "        total_acc = sum(total_acc)/train_len\n",
    "        log['loss'].append(current_loss)\n",
    "        log['accuracy'].append(total_acc)\n",
    "\n",
    "        \n",
    "        log['v_loss_std'].append(np.std(v_loss))\n",
    "        log['v_accuracy_std'].append(np.std(v_acc))\n",
    "        v_loss = sum(v_loss)/valid_len\n",
    "        v_acc = sum(v_acc)/valid_len\n",
    "        log['v_loss'].append(v_loss)\n",
    "        log['v_accuracy'].append(v_acc)\n",
    "\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (layers): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (1): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (2): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "p Parameter containing:\n",
      "tensor([[ 0.0169, -0.0321, -0.0182,  ...,  0.0083,  0.0257, -0.0344],\n",
      "        [ 0.0157,  0.0356, -0.0017,  ..., -0.0302, -0.0006, -0.0148],\n",
      "        [ 0.0097, -0.0205,  0.0319,  ...,  0.0119, -0.0204,  0.0070],\n",
      "        ...,\n",
      "        [-0.0202, -0.0245, -0.0254,  ..., -0.0196, -0.0011,  0.0303],\n",
      "        [-0.0021,  0.0066,  0.0240,  ...,  0.0328,  0.0061, -0.0274],\n",
      "        [ 0.0111, -0.0354,  0.0293,  ..., -0.0203, -0.0054, -0.0056]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "p Parameter containing:\n",
      "tensor([ 3.2903e-02,  3.4208e-02, -2.6512e-02,  1.2957e-02, -1.7852e-02,\n",
      "         1.5902e-02, -2.6936e-02,  2.2904e-02,  2.9569e-02, -3.9645e-03,\n",
      "        -1.0159e-02,  2.9865e-02, -1.2674e-02,  3.4911e-02,  2.6718e-02,\n",
      "         1.8611e-03, -2.6535e-02,  3.2963e-02, -2.9120e-02, -1.1215e-02,\n",
      "        -8.4861e-03, -6.6860e-03, -2.3382e-02,  1.1584e-02, -3.1356e-02,\n",
      "        -2.7516e-02,  3.1720e-02, -2.4133e-02,  2.0943e-02, -2.3761e-02,\n",
      "         3.1130e-02,  2.0856e-02,  2.1932e-02,  1.8206e-02,  2.2755e-02,\n",
      "         2.1090e-02,  2.0040e-02, -2.7683e-02,  1.3034e-02, -1.2503e-02,\n",
      "        -1.5987e-02,  6.7767e-03,  3.2151e-02, -2.1561e-02,  3.1690e-02,\n",
      "         1.6886e-03,  1.2613e-02, -2.8443e-02,  2.4870e-03, -1.8200e-02,\n",
      "         1.2819e-02,  1.4029e-02,  6.9552e-03, -2.6527e-02,  1.2107e-02,\n",
      "         7.9003e-03, -1.3998e-03,  1.3314e-02,  1.5182e-02,  4.4453e-03,\n",
      "         2.7806e-02,  7.9980e-03,  7.1091e-03, -1.9332e-04,  6.8787e-03,\n",
      "         2.1390e-03, -1.7754e-02,  2.7674e-02, -1.1096e-02,  2.5809e-02,\n",
      "        -3.4970e-04, -1.9317e-02,  1.8883e-02,  6.1459e-03, -2.5676e-02,\n",
      "        -3.0490e-02, -8.9883e-03,  4.5137e-03,  2.6840e-02, -8.8108e-03,\n",
      "        -2.7114e-03, -1.1977e-02, -1.5945e-02, -1.1252e-02, -3.5994e-03,\n",
      "        -7.1058e-03, -1.6786e-02, -6.2825e-03,  6.7164e-03,  3.0276e-03,\n",
      "         1.8871e-02, -3.2961e-02,  1.3909e-02, -2.0774e-02,  1.7272e-02,\n",
      "        -1.0966e-02, -2.6099e-02,  6.1733e-03, -2.5746e-02,  1.9174e-02,\n",
      "         2.8432e-02, -1.8191e-02, -1.2465e-02, -2.0417e-02, -1.7505e-02,\n",
      "        -3.0801e-02, -2.8126e-02,  9.2421e-03, -3.4884e-02,  1.3968e-03,\n",
      "        -1.4287e-03,  1.1126e-02,  2.0824e-02, -2.7139e-02,  5.4094e-03,\n",
      "        -3.3171e-02,  2.4023e-02, -6.4196e-03, -6.6740e-03,  2.9218e-02,\n",
      "         1.9284e-02, -4.2427e-05, -9.6559e-03, -2.9958e-02, -5.5560e-04,\n",
      "        -2.4998e-02,  2.3991e-02, -1.3578e-02, -2.3575e-02,  2.5055e-02,\n",
      "         3.0649e-02,  9.9624e-03,  9.9702e-03,  2.7618e-02,  2.1878e-02,\n",
      "        -2.8674e-02, -6.3788e-03, -2.5312e-04, -1.0729e-02,  6.9991e-03,\n",
      "        -1.0360e-02,  1.3463e-02,  3.2523e-02, -3.4288e-02, -1.4742e-02,\n",
      "        -3.4495e-02,  2.8356e-02, -2.0814e-02,  3.2166e-02,  2.5187e-02,\n",
      "        -2.6735e-02,  3.2544e-02, -2.4796e-02, -7.3250e-03,  3.4699e-02,\n",
      "         3.1744e-02,  2.3616e-02, -4.3704e-03,  6.7186e-03, -2.3703e-02,\n",
      "        -2.0181e-02,  1.0978e-02,  3.0906e-02, -2.8512e-02,  8.1690e-03,\n",
      "        -2.9298e-03,  1.7254e-02, -3.5590e-02,  3.1134e-02, -2.8948e-02,\n",
      "         2.3473e-02, -3.3994e-02,  3.3488e-02, -3.3912e-02, -1.5747e-02,\n",
      "         3.3695e-02, -2.9697e-02,  1.8048e-02,  1.7000e-02, -2.7561e-02,\n",
      "         2.9504e-02, -9.7239e-03, -3.5626e-02, -2.8400e-02, -1.6764e-02,\n",
      "        -1.1359e-02,  2.2867e-02, -8.5604e-03,  2.5977e-02, -5.6717e-03,\n",
      "         3.1156e-02, -1.5989e-02, -4.9183e-03,  2.7166e-03,  2.7910e-02,\n",
      "         2.5436e-03, -5.4608e-03,  7.1829e-03,  3.0189e-02, -8.6177e-03,\n",
      "         1.8116e-02, -2.4175e-02,  2.1884e-02,  2.8056e-02,  5.9066e-03,\n",
      "         1.4243e-02, -3.1845e-02,  1.8832e-02, -9.3669e-03,  3.3393e-03,\n",
      "        -2.0078e-02, -1.5482e-02, -3.0613e-03,  1.0183e-03,  1.8340e-02,\n",
      "         2.1369e-02, -2.5351e-02, -2.1608e-02,  2.9698e-02,  4.9771e-03,\n",
      "         2.0701e-02,  9.5399e-03, -3.9200e-03,  2.9574e-02,  3.5416e-02,\n",
      "         2.6663e-02,  1.4306e-02,  3.2630e-02, -1.9925e-03,  2.2339e-03,\n",
      "         1.5747e-02, -2.7093e-04,  1.6562e-02,  2.9661e-02, -1.3806e-02,\n",
      "        -2.4536e-02,  1.6625e-02, -1.9629e-02, -1.7582e-02, -3.6633e-03,\n",
      "         3.5415e-03, -3.1164e-02, -3.1266e-02, -1.9631e-03,  3.1805e-02,\n",
      "        -1.2181e-02,  1.3206e-03,  1.2636e-02,  2.6577e-02, -3.2297e-02,\n",
      "         2.9262e-02, -9.9606e-03,  3.0660e-02, -1.4233e-02,  2.4343e-03,\n",
      "        -5.3405e-03, -1.4762e-02,  7.6956e-03, -1.6006e-02, -1.2950e-02,\n",
      "         6.4207e-03, -2.5022e-02,  4.8039e-03, -2.5011e-02, -2.9680e-02,\n",
      "         2.0226e-02, -2.3611e-02,  1.8679e-03, -2.9328e-02,  3.2515e-02,\n",
      "        -4.4619e-03, -1.9444e-02, -1.5315e-02,  4.6585e-03, -2.5075e-02,\n",
      "        -3.9739e-03, -2.9897e-04, -2.3268e-02,  1.3490e-02, -3.0150e-02,\n",
      "        -2.7259e-02, -2.3490e-03, -2.0613e-02,  1.5313e-02, -7.4894e-03,\n",
      "        -1.2275e-02,  3.4403e-02,  2.6203e-02,  4.0147e-04, -1.6103e-02,\n",
      "         3.4034e-02, -2.0561e-02, -3.0683e-03,  3.3504e-02, -5.1345e-03,\n",
      "         1.2599e-02, -1.9614e-03, -1.4015e-02,  2.1230e-02,  2.8132e-02,\n",
      "         2.0035e-02,  1.3169e-02,  1.6086e-02, -1.4037e-02, -6.5942e-03,\n",
      "        -5.4481e-03, -1.3341e-02, -5.1514e-03, -3.5220e-02,  2.7917e-02,\n",
      "        -1.9870e-03,  2.2153e-02,  1.5039e-02, -3.6641e-03,  3.0042e-02,\n",
      "        -2.4495e-02, -1.9856e-02,  2.5360e-02, -1.7464e-02,  3.1163e-02,\n",
      "         1.5251e-02,  1.7751e-02, -1.3456e-02, -1.5545e-02, -1.3738e-02,\n",
      "         1.2482e-02,  3.1007e-02,  2.2202e-02,  2.8930e-02, -2.5863e-02,\n",
      "        -2.1600e-02, -2.8144e-02,  1.7046e-02,  4.7255e-04,  3.1434e-02,\n",
      "         1.1017e-02, -1.3348e-03, -1.9423e-02,  1.9951e-02,  5.3764e-03,\n",
      "        -1.2561e-02, -2.7479e-02, -3.2306e-02,  6.1846e-03, -2.9312e-02,\n",
      "         7.2718e-04,  1.7103e-02, -1.8683e-02, -6.5432e-03, -1.2671e-02,\n",
      "        -1.8268e-03,  2.1130e-02, -7.9949e-03,  3.5664e-02, -7.0524e-03,\n",
      "         1.2631e-02, -2.9396e-02, -6.2401e-03,  3.2993e-02, -3.4202e-02,\n",
      "        -1.6196e-02,  8.3191e-03, -1.7317e-02,  3.2831e-03, -2.2140e-02,\n",
      "        -3.9534e-03,  2.4743e-02,  1.4598e-02, -1.3517e-02,  2.2346e-02,\n",
      "         2.6906e-03,  3.0313e-02, -1.8446e-02,  2.4496e-02,  2.0861e-02,\n",
      "        -1.4200e-02, -3.2592e-02,  2.2568e-02,  8.7444e-04, -1.2114e-02,\n",
      "        -4.6021e-03, -1.4441e-02,  2.4089e-02,  2.0612e-02, -7.6687e-03,\n",
      "         1.3762e-02,  2.4137e-02, -1.1439e-02, -2.9673e-02,  3.2375e-02,\n",
      "        -4.8078e-03, -2.3474e-02, -1.9594e-02,  3.2452e-02,  1.3872e-02,\n",
      "         1.0816e-02, -1.7244e-02,  1.6479e-02, -2.4835e-02, -1.6271e-02,\n",
      "        -1.2803e-02,  3.2753e-02, -3.3316e-03, -7.4958e-03, -1.0268e-02,\n",
      "         1.5056e-02, -2.0489e-03, -2.7180e-02, -7.7834e-03,  4.5673e-03,\n",
      "         2.1085e-02, -6.2162e-03, -8.5728e-03,  7.5179e-03, -1.6871e-02,\n",
      "        -3.3196e-02, -1.3027e-02,  1.9915e-02, -5.9074e-03, -1.1883e-02,\n",
      "        -9.6868e-03, -6.8775e-03,  1.3214e-02, -6.8088e-03, -3.3583e-02,\n",
      "        -2.3487e-02, -3.1673e-02, -2.0060e-02, -3.4508e-02,  2.0551e-02,\n",
      "        -2.0055e-02,  1.2987e-03,  8.0015e-03, -2.2462e-02, -1.9915e-02,\n",
      "        -6.3314e-03,  1.7491e-02,  3.0040e-02, -1.5238e-04,  1.1683e-02,\n",
      "        -2.3392e-03, -2.7345e-02, -1.3911e-02,  2.2408e-02, -3.4056e-02,\n",
      "        -3.4230e-02, -3.5368e-02,  1.4769e-02, -8.2561e-03,  1.8250e-02,\n",
      "        -1.5511e-02,  3.3690e-03, -3.2453e-02,  2.9584e-02, -1.9784e-02,\n",
      "         3.1551e-02,  3.1114e-02,  1.5617e-02, -9.9878e-03,  3.0083e-02,\n",
      "         1.6119e-02, -2.5981e-02, -2.9163e-02, -1.4921e-02, -9.1848e-03,\n",
      "        -2.7463e-02, -2.5421e-02,  1.3855e-02,  2.0294e-02,  2.0163e-02,\n",
      "         3.0503e-02,  2.6294e-02, -1.5649e-02,  2.7443e-02,  6.2805e-03,\n",
      "        -3.3456e-02,  7.7451e-03, -2.9488e-02, -7.6843e-03,  8.9979e-03,\n",
      "        -1.0030e-02, -1.4900e-02, -2.3478e-02, -3.4906e-02,  1.4518e-02,\n",
      "         3.4806e-02, -1.8588e-02,  1.3006e-02, -4.2764e-03, -6.2975e-03,\n",
      "        -2.9168e-02, -1.0370e-02, -1.3431e-02,  2.0891e-03, -1.2403e-02,\n",
      "         8.6092e-04,  3.2888e-03,  2.8625e-02,  1.5108e-02, -2.5140e-02,\n",
      "        -2.2247e-02,  1.7766e-02, -7.6451e-03,  2.2186e-02,  2.6012e-02,\n",
      "        -6.6303e-03, -2.0235e-02, -2.2666e-02,  2.3988e-03,  1.9562e-02,\n",
      "         2.6742e-02, -1.8259e-02], device='cuda:0', requires_grad=True)\n",
      "p Parameter containing:\n",
      "tensor([[ 0.0268, -0.0038,  0.0203,  ..., -0.0071, -0.0067,  0.0211],\n",
      "        [ 0.0295,  0.0120, -0.0031,  ...,  0.0275,  0.0110, -0.0066],\n",
      "        [-0.0193,  0.0404,  0.0373,  ..., -0.0056, -0.0143,  0.0169],\n",
      "        ...,\n",
      "        [ 0.0321, -0.0050,  0.0171,  ..., -0.0365,  0.0139, -0.0217],\n",
      "        [-0.0271, -0.0314,  0.0212,  ...,  0.0162,  0.0285, -0.0318],\n",
      "        [ 0.0006,  0.0318,  0.0407,  ..., -0.0128,  0.0407, -0.0229]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "p Parameter containing:\n",
      "tensor([-0.0337,  0.0002, -0.0391, -0.0155,  0.0130,  0.0138, -0.0008,  0.0247,\n",
      "         0.0054, -0.0135], device='cuda:0', requires_grad=True)\n",
      "params <generator object Module.parameters at 0x0000018887C4C6D0>\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "optimizer got an empty parameter list",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 6\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(model)\n\u001b[0;32m      4\u001b[0m loss_function \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[1;32m----> 6\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m \u001b[43mRSAG\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m.9\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m9e-5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m log \u001b[38;5;241m=\u001b[39m train_model(model, loss_function, optimizer, loaders, device)\n",
      "Cell \u001b[1;32mIn[22], line 43\u001b[0m, in \u001b[0;36mRSAG.__init__\u001b[1;34m(self, params, lr, alpha, beta)\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mp\u001b[39m\u001b[38;5;124m'\u001b[39m, p)\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m, params)\n\u001b[1;32m---> 43\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mRSAG\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefaults\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\camgr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\optim\\optimizer.py:261\u001b[0m, in \u001b[0;36mOptimizer.__init__\u001b[1;34m(self, params, defaults)\u001b[0m\n\u001b[0;32m    259\u001b[0m param_groups \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(params)\n\u001b[0;32m    260\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(param_groups) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 261\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptimizer got an empty parameter list\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    262\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(param_groups[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m    263\u001b[0m     param_groups \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m: param_groups}]\n",
      "\u001b[1;31mValueError\u001b[0m: optimizer got an empty parameter list"
     ]
    }
   ],
   "source": [
    "model = MLP().to(device)\n",
    "print(model)\n",
    "\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = RSAG(model.parameters(),  lr=1e-4, alpha=.9, beta=9e-5)\n",
    "log = train_model(model, loss_function, optimizer, loaders, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 58)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m<tokenize>:58\u001b[1;36m\u001b[0m\n\u001b[1;33m    loss_function = torch.nn.CrossEntropyLoss()\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "def train_with_hyperparameters(alpha_values, lr_values, save_log=False):\n",
    "    loss_function = torch.nn.CrossEntropyLoss()\n",
    "    best_alpha, best_lr = 0.0, 0.0\n",
    "    best_accuracy = 0.0\n",
    "    v_accs, acc_std, v_loss, loss_std = [], [], [], []\n",
    "    acc, loss = [], []\n",
    "    \n",
    "    for alpha in alpha_values:\n",
    "        for lr in lr_values:\n",
    "            beta = lr * alpha\n",
    "            \n",
    "            print(f\"----------- Training with alpha={alpha}, lr={lr} -----------------\")\n",
    "            \n",
    "            model = MLP().to(device)\n",
    "            optimizer = RSAG(model.parameters(), lr=lr, alpha=alpha, beta=beta)\n",
    "            log = train_model(model, loaders, optimizer, loss_function, device, epochs=20)\n",
    "            \n",
    "            if log['v_accuracy'][-1] > best_accuracy:\n",
    "                print(f\"Found a new best accuracy: {log['v_accuracy'][-1]}\")\n",
    "                print(f\"best alpha: {alpha}, best lr: {lr}\")\n",
    "                best_accuracy = log['v_accuracy'][-1]\n",
    "                best_alpha = alpha\n",
    "                best_lr = lr\n",
    "            \n",
    "            v_accs.append(log['v_accuracy'])\n",
    "            acc_std.append(log['v_accuracy_std'])\n",
    "            v_loss.append(log['v_loss'])\n",
    "            loss_std.append(log['v_loss_std'])\n",
    "            acc.append(log['accuracy'])\n",
    "            loss.append(log['loss'])\n",
    "            \n",
    "\n",
    "    \n",
    "    return best_alpha, best_lr, v_accs, acc_std, v_loss, loss_std, acc, loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Epoch 1\n",
      "Epoch 1 finished\n",
      "loss 4.6072\n",
      "Accuracy:  7.4900\n",
      "Starting Epoch 2\n",
      "Epoch 2 finished\n",
      "loss 4.6027\n",
      "Accuracy:  8.8633\n",
      "Starting Epoch 3\n",
      "Epoch 3 finished\n",
      "loss 4.5982\n",
      "Accuracy:  10.3017\n",
      "Starting Epoch 4\n",
      "Epoch 4 finished\n",
      "loss 4.5937\n",
      "Accuracy:  11.6917\n",
      "Starting Epoch 5\n",
      "Epoch 5 finished\n",
      "loss 4.5893\n",
      "Accuracy:  13.1383\n",
      "Training has completed\n"
     ]
    }
   ],
   "source": [
    "\n",
    "optimizer = RSAG(model.parameters(), lr=1e-4, alpha=.9, beta=9e-5)\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=1e-4, nesterov=True, momentum=0.9)\n",
    "train_model(model, loaders, optimizer, loss_function, device, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
